@article{abboodCommunityDetectionModel2023,
  title = {Community Detection Model for Dynamic Networks Based on Hidden {{Markov}} Model and Evolutionary Algorithm},
  author = {Abbood, Amenah D. and Attea, Bara’a A. and Hasan, Ammar A. and Everson, Richard M. and Pizzuti, Clara},
  date = {2023-02-09},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10383-2},
  url = {https://doi.org/10.1007/s10462-022-10383-2},
  urldate = {2023-03-06},
  abstract = {Finding communities of connected individuals in complex networks is challenging, yet crucial for understanding different real-world societies and their interactions. Recently attention has turned to discover the dynamics of such communities. However, detecting accurate community structures that evolve over time adds additional challenges. Almost all the state-of-the-art algorithms are designed based on seemingly the same principle while treating the problem as a coupled optimization model to simultaneously identify community structures and their evolution over time. Unlike all these studies, the current work aims to individually consider this three measures, i.e. intra-community score, inter-community score, and evolution of community over time. Here, we adopt a new perspective towards detecting the evolution of community structures. The proposed method realizes the decomposition of the problem into three essential components; searching in: intra-community connections, inter-community connections, and community evolution. A multi-objective optimization problem is defined to account for the different intra and inter community structures. Further, we formulate the community evolution problem as a Hidden Markov Model in an attempt to dexterously track the most likely sequence of communities. Then the new model, called Hidden Markov Model-based Multi-Objective evolutionary algorithm for Dynamic Community Detection (HMM-MODCD), uses a multi-objective evolutionary algorithm and Viterbi algorithm for formulating objective functions and providing temporal smoothness over time for clustering dynamic networks. The performance of the proposed algorithm is evaluated on synthetic and real-world dynamic networks and compared against several state-of-the-art algorithms. The results clearly demonstrate the effectiveness of the proposed algorithm to outperform other algorithms.},
  langid = {english}
}

@online{abdelkhalikDemystifyingNvidiaAmpere2022,
  title = {Demystifying the {{Nvidia Ampere Architecture}} through {{Microbenchmarking}} and {{Instruction-level Analysis}}},
  author = {Abdelkhalik, Hamdy and Arafa, Yehia and Santhi, Nandakishore and Badawy, Abdel-Hameed},
  date = {2022-08-23},
  eprint = {2208.11174},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2208.11174},
  urldate = {2022-12-18},
  abstract = {Graphics processing units (GPUs) are now considered the leading hardware to accelerate general-purpose workloads such as AI, data analytics, and HPC. Over the last decade, researchers have focused on demystifying and evaluating the microarchitecture features of various GPU architectures beyond what vendors reveal. This line of work is necessary to understand the hardware better and build more efficient workloads and applications. Many works have studied the recent Nvidia architectures, such as Volta and Turing, comparing them to their successor, Ampere. However, some microarchitecture features, such as the clock cycles for the different instructions, have not been extensively studied for the Ampere architecture. In this paper, we study the clock cycles per instructions with various data types found in the instruction-set architecture (ISA) of Nvidia GPUs. Using microbenchmarks, we measure the clock cycles for PTX ISA instructions and their SASS ISA instructions counterpart. we further calculate the clock cycle needed to access each memory unit. We also demystify the new version of the tensor core unit found in the Ampere architecture by using the WMMA API and measuring its clock cycles per instruction and throughput for the different data types and input shapes. The results found in this work should guide software developers and hardware architects. Furthermore, the clock cycles per instructions are widely used by performance modeling simulators and tools to model and predict the performance of the hardware.},
  pubstate = {preprint},
  keywords = {Computer Science - Hardware Architecture},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/84ASDFQA/Abdelkhalik и др. - 2022 - Demystifying the Nvidia Ampere Architecture throug.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/P98KKYPX/2208.html}
}

@article{abelAutomaticGenerationModels,
  title = {Automatic Generation of Models of Microarchitectures},
  author = {Abel, Andreas},
  langid = {ngerman},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/XZS28PKG/Abel - Automatic Generation of Models of Microarchitectur.pdf}
}

@article{abelAutomaticGenerationModelsa,
  title = {Automatic Generation of Models of Microarchitectures},
  author = {Abel, Andreas},
  langid = {ngerman},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/7VKVK3ZP/Abel - Automatic Generation of Models of Microarchitectur.pdf}
}

@inproceedings{abelNanoBenchLowOverheadTool2020,
  title = {{{nanoBench}}: {{A Low-Overhead Tool}} for {{Running Microbenchmarks}} on X86 {{Systems}}},
  shorttitle = {{{nanoBench}}},
  booktitle = {2020 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}} ({{ISPASS}})},
  author = {Abel, Andreas and Reineke, Jan},
  date = {2020-08},
  eprint = {1911.03282},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {34--46},
  doi = {10.1109/ISPASS48437.2020.00014},
  url = {http://arxiv.org/abs/1911.03282},
  urldate = {2023-03-06},
  abstract = {We present nanoBench, a tool for evaluating small microbenchmarks using hardware performance counters on Intel and AMD x86 systems. Most existing tools and libraries are intended to either benchmark entire programs, or program segments in the context of their execution within a larger program. In contrast, nanoBench is specifically designed to evaluate small, isolated pieces of code. Such code is common in microbenchmark-based hardware analysis techniques. Unlike previous tools, nanoBench can execute microbenchmarks directly in kernel space. This allows to benchmark privileged instructions, and it enables more accurate measurements. The reading of the performance counters is implemented with minimal overhead avoiding functions calls and branches. As a consequence, nanoBench is precise enough to measure individual memory accesses. We illustrate the utility of nanoBench at the hand of two case studies. First, we briefly discuss how nanoBench has been used to determine the latency, throughput, and port usage of more than 13,000 instruction variants on recent x86 processors. Second, we show how to generate microbenchmarks to precisely characterize the cache architectures of eleven Intel Core microarchitectures. This includes the most comprehensive analysis of the employed cache replacement policies to date.},
  keywords = {Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/BSHGHKR8/Abel и Reineke - 2020 - nanoBench A Low-Overhead Tool for Running Microbe.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/L3R4SKXE/abel2020.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/RKGYBL5P/1911.html}
}

@inproceedings{abelUiCAAccurateThroughput2022,
  title = {{{uiCA}}: {{Accurate Throughput Prediction}} of {{Basic Blocks}} on {{Recent Intel Microarchitectures}}},
  shorttitle = {{{uiCA}}},
  booktitle = {Proceedings of the 36th {{ACM International Conference}} on {{Supercomputing}}},
  author = {Abel, Andreas and Reineke, Jan},
  date = {2022-06-28},
  eprint = {2107.14210},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--14},
  doi = {10.1145/3524059.3532396},
  url = {http://arxiv.org/abs/2107.14210},
  urldate = {2022-12-17},
  abstract = {Performance models that statically predict the steady-state throughput of basic blocks on particular microarchitectures, such as IACA, Ithemal, llvm-mca, OSACA, or CQA, can guide optimizing compilers and aid manual software optimization. However, their utility heavily depends on the accuracy of their predictions. The average error of existing models compared to measurements on the actual hardware has been shown to lie between 9\% and 36\%. But how good is this? To answer this question, we propose an extremely simple analytical throughput model that may serve as a baseline. Surprisingly, this model is already competitive with the state of the art, indicating that there is significant potential for improvement. To explore this potential, we develop a simulation-based throughput predictor. To this end, we propose a detailed parametric pipeline model that supports all Intel Core microarchitecture generations released between 2011 and 2021. We evaluate our predictor on an improved version of the BHive benchmark suite and show that its predictions are usually within 1\% of measurement results, improving upon prior models by roughly an order of magnitude. The experimental evaluation also demonstrates that several microarchitectural details considered to be rather insignificant in previous work, are in fact essential for accurate prediction. Our throughput predictor is available as open source at https://github.com/andreas-abel/uiCA.},
  keywords = {Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/PKCF5Z9U/Abel и Reineke - 2022 - uiCA Accurate Throughput Prediction of Basic Bloc.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/8D23LTTE/2107.html}
}

@software{abelUiCAUopsInfo2022,
  title = {{{uiCA}} (Uops.Info {{Code Analyzer}})},
  author = {Abel, Andreas},
  date = {2022-12-13T22:22:58Z},
  origdate = {2020-04-18T23:23:09Z},
  url = {https://github.com/andreas-abel/uiCA},
  urldate = {2022-12-18},
  abstract = {uops.info Code Analyzer}
}

@inproceedings{abelUopsInfoCharacterizing2019,
  title = {Uops.Info: {{Characterizing Latency}}, {{Throughput}}, and {{Port Usage}} of {{Instructions}} on {{Intel Microarchitectures}}},
  shorttitle = {Uops.Info},
  booktitle = {Proceedings of the {{Twenty-Fourth International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Abel, Andreas and Reineke, Jan},
  date = {2019-04-04},
  series = {{{ASPLOS}} '19},
  pages = {673--686},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3297858.3304062},
  url = {https://doi.org/10.1145/3297858.3304062},
  urldate = {2022-12-17},
  abstract = {Modern microarchitectures are some of the world's most complex man-made systems. As a consequence, it is increasingly difficult to predict, explain, let alone optimize the performance of software running on such microarchitectures. As a basis for performance predictions and optimizations, we would need faithful models of their behavior, which are, unfortunately, seldom available. In this paper, we present the design and implementation of a tool to construct faithful models of the latency, throughput, and port usage of x86 instructions. To this end, we first discuss common notions of instruction throughput and port usage, and introduce a more precise definition of latency that, in contrast to previous definitions, considers dependencies between different pairs of input and output operands. We then develop novel algorithms to infer the latency, throughput, and port usage based on automatically-generated microbenchmarks that are more accurate and precise than existing work. To facilitate the rapid construction of optimizing compilers and tools for performance prediction, the output of our tool is provided in a machine-readable format. We provide experimental results for processors of all generations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake, and discuss various cases where the output of our tool differs considerably from prior work.},
  isbn = {978-1-4503-6240-5},
  keywords = {compilers,cpu,hardware performance counters,instructions,intel,latency,micro-ops,microarchitecture,microbenchmarks,microops,optimization,out-of-order execution,performance,performance analysis,performance prediction,pipeline,port usage,simulation,throughput,uops,uops.info,x86},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/2NDCKWQ4/Abel и Reineke - 2019 - uops.info Characterizing Latency, Throughput, and.pdf}
}

@article{aditPerformanceLeftTable2022,
  title = {Performance {{Left}} on the {{Table}}: {{An Evaluation}} of {{Compiler Autovectorization}} for {{RISC-V}}},
  shorttitle = {Performance {{Left}} on the {{Table}}},
  author = {Adit, Neil and Sampson, Adrian},
  date = {2022-09-01},
  journaltitle = {IEEE Micro},
  shortjournal = {IEEE Micro},
  volume = {42},
  number = {5},
  pages = {41--48},
  issn = {0272-1732, 1937-4143},
  doi = {10.1109/MM.2022.3184867},
  url = {https://ieeexplore.ieee.org/document/9802745/},
  urldate = {2022-12-17},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/LPXSFB2A/Adit и Sampson - 2022 - Performance Left on the Table An Evaluation of Co.pdf}
}

@online{AdvancedProfilingTopics,
  title = {Advanced Profiling Topics. {{PEBS}} and {{LBR}}. | {{Easyperf}}},
  url = {https://easyperf.net/blog/2018/06/08/Advanced-profiling-topics-PEBS-and-LBR},
  urldate = {2023-01-03},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/23G5GL2R/Advanced-profiling-topics-PEBS-and-LBR.html}
}

@incollection{afzalAnalyticModelingIdle2021,
  title = {Analytic {{Modeling}} of {{Idle Waves}} in {{Parallel Programs}}: {{Communication}}, {{Cluster Topology}}, and {{Noise Impact}}},
  shorttitle = {Analytic {{Modeling}} of {{Idle Waves}} in {{Parallel Programs}}},
  author = {Afzal, Ayesha and Hager, Georg and Wellein, Gerhard},
  date = {2021},
  volume = {12728},
  eprint = {2103.03175},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {351--371},
  doi = {10.1007/978-3-030-78713-4_19},
  url = {http://arxiv.org/abs/2103.03175},
  urldate = {2022-12-31},
  abstract = {Most distributed-memory bulk-synchronous parallel programs in HPC assume that compute resources are available continuously and homogeneously across the allocated set of compute nodes. However, long one-off delays on individual processes can cause global disturbances, so-called idle waves, by rippling through the system. This process is mainly governed by the communication topology of the underlying parallel code. This paper makes significant contributions to the understanding of idle wave dynamics. We study the propagation mechanisms of idle waves across the ranks of MPI-parallel programs. We present a validated analytic model for their propagation velocity with respect to communication parameters and topology, with a special emphasis on sparse communication patterns. We study the interaction of idle waves with MPI collectives and show that, depending on the implementation, a collective may be transparent to the wave. Finally we analyze two mechanisms of idle wave decay: topological decay, which is rooted in differences in communication characteristics among parts of the system, and noise-induced decay, which is caused by system or application noise. We show that noise-induced decay is largely independent of noise characteristics but depends only on the overall noise power. An analytic expression for idle wave decay rate with respect to noise power is derived. For model validation we use microbenchmarks and stencil algorithms on three different supercomputing platforms.},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/54EMDBGX/Afzal и др. - 2021 - Analytic Modeling of Idle Waves in Parallel Progra.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/3IX5CAHL/2103.html}
}

@online{afzalAnalyticPerformanceModel2020,
  title = {An Analytic Performance Model for Overlapping Execution of Memory-Bound Loop Kernels on Multicore {{CPUs}}},
  author = {Afzal, Ayesha and Hager, Georg and Wellein, Gerhard},
  date = {2020-10-31},
  eprint = {2011.00243},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2011.00243},
  url = {http://arxiv.org/abs/2011.00243},
  urldate = {2022-12-31},
  abstract = {Complex applications running on multicore processors show a rich performance phenomenology. The growing number of cores per ccNUMA domain complicates performance analysis of memory-bound code since system noise, load imbalance, or task-based programming models can lead to thread desynchronization. Hence, the simplifying assumption that all cores execute the same loop can not be upheld. Motivated by observations on plain and modified versions of the HPCG benchmark, we construct a performance model of execution of memory-bound loop kernels. It can predict the memory bandwidth share per kernel on a memory contention domain depending on the number of active cores and which other workload the kernel is paired with. The only code features required are the single-thread cache line access frequency per kernel, which is directly related to the single-thread memory bandwidth, and its saturated bandwidth. It can either be measured directly or predicted using the Execution-Cache-Memory (ECM) performance model. The computational intensity of the kernels and the detailed structure of the code is of no significance. We validate our model on Intel Broadwell, Intel Cascade Lake, and AMD Rome processors pairing various streaming and stencil kernels. The error in predicting the bandwidth share per kernel is less than 8\%.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/85B2FG23/Afzal и др. - 2020 - An analytic performance model for overlapping exec.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/VQKKGBYI/2011.html}
}

@article{agafonovPodhodGeneraciiTestovyh2022,
  title = {Подход к генерации тестовых программ для верификации когерентности памяти микропроцессоров «Эльбрус»},
  author = {Агафонов, Владимир Андреевич and Фролов, Павел Викторович and Мешков, Алексей Николаевич},
  date = {2022-07-13},
  journaltitle = {Труды Института системного программирования РАН},
  volume = {34},
  number = {2},
  pages = {7--16},
  issn = {2220-6426},
  url = {https://ispranproceedings.elpub.ru/jour/article/view/1511},
  urldate = {2023-03-01},
  abstract = {Научный рецензируемый журнал, список ВАК},
  issue = {2},
  langid = {russian},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/7KJR96CX/Агафонов и др. - 2022 - Подход к генерации тестовых программ для верификац.pdf}
}

@article{ainsworthSoftwarePrefetchingIndirect,
  title = {Software {{Prefetching}} for {{Indirect Memory Accesses}}},
  author = {Ainsworth, Sam and Jones, Timothy M},
  abstract = {Many modern data processing and HPC workloads are heavily memory-latency bound. A tempting proposition to solve this is software prefetching, where special non-blocking loads are used to bring data into the cache hierarchy just before being required. However, these are difficult to insert to effectively improve performance, and techniques for automatic insertion are currently limited.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/MFRUTVLJ/Ainsworth и Jones - Software Prefetching for Indirect Memory Accesses.pdf}
}

@online{aktarBBMLBasicBlock2022,
  title = {{{BB-ML}}: {{Basic Block Performance Prediction}} Using {{Machine Learning Techniques}}},
  shorttitle = {{{BB-ML}}},
  author = {Aktar, Shamminuj and Abdelkhalik, Hamdy and Turja, Nazmul Haque and Arafa, Yehia and Barai, Atanu and Panda, Nishant and Chennupati, Gopinath and Santhi, Nandakishore and Eidenbenz, Stephan and Badawy, Abdel-Hameed},
  date = {2022-02-17},
  eprint = {2202.07798},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.07798},
  urldate = {2023-03-06},
  abstract = {Recent years have seen the adoption of Machine Learning (ML) techniques to predict the performance of large-scale applications, mostly at a coarse level. In contrast, we propose to use ML techniques for performance prediction at much finer granularity, namely at the levels of Basic Block (BB), which are the single entry-single exit code blocks that are used as analysis tools by all compilers to break down a large code into manageable pieces. Utilizing ML and BB analysis together can enable scalable hardware-software co-design beyond the current state of the art. In this work, we extrapolate the basic block execution counts of GPU applications for large inputs sizes from the counts of smaller input sizes of the same application. We employ two ML models, a Poisson Neural Network (PNN) and a Bayesian Regularization Backpropagation Neural Network (BR-BPNN). We train both models using the lowest input values of the application and random input values to predict basic block counts. Results show that our models accurately predict the basic block execution counts of 16 benchmark applications. For PNN and BR-BPNN models, we achieve an average accuracy of 93.5\% and 95.6\%, respectively, while extrapolating the basic block counts for large input sets when the model is trained using smaller input sets. Additionally, the models show an average accuracy of 97.7\% and 98.1\%, respectively, while predicting basic block counts on random instances.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/3V84M6Z9/Aktar и др. - 2022 - BB-ML Basic Block Performance Prediction using Ma.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/W4YI346F/2202.html}
}

@inproceedings{alappatPerformanceModelingStreaming2020,
  title = {Performance {{Modeling}} of {{Streaming Kernels}} and {{Sparse Matrix-Vector Multiplication}} on {{A64FX}}},
  booktitle = {2020 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  author = {Alappat, Christie L. and Laukemann, Jan and Gruber, Thomas and Hager, Georg and Wellein, Gerhard and Meyer, Nils and Wettig, Tilo},
  date = {2020-11},
  eprint = {2009.13903},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--7},
  doi = {10.1109/PMBS51919.2020.00006},
  url = {http://arxiv.org/abs/2009.13903},
  urldate = {2022-12-31},
  abstract = {The A64FX CPU powers the current number one supercomputer on the Top500 list. Although it is a traditional cache-based multicore processor, its peak performance and memory bandwidth rival accelerator devices. Generating efficient code for such a new architecture requires a good understanding of its performance features. Using these features, we construct the Execution-Cache-Memory (ECM) performance model for the A64FX processor in the FX700 supercomputer and validate it using streaming loops. We also identify architectural peculiarities and derive optimization hints. Applying the ECM model to sparse matrix-vector multiplication (SpMV), we motivate why the CRS matrix storage format is inappropriate and how the SELL-C-sigma format with suitable code optimizations can achieve bandwidth saturation for SpMV.},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Hardware Architecture,Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/RZT29RT3/Alappat и др. - 2020 - Performance Modeling of Streaming Kernels and Spar.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/S6VNWQ98/alappat2020.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/58XS2JYU/2009.html}
}

@inproceedings{alappatUnderstandingHPCBenchmark2020,
  title = {Understanding {{HPC Benchmark Performance}} on {{Intel Broadwell}} and~{{Cascade Lake Processors}}},
  booktitle = {High {{Performance Computing}}},
  author = {Alappat, Christie L. and Hofmann, Johannes and Hager, Georg and Fehske, Holger and Bishop, Alan R. and Wellein, Gerhard},
  editor = {Sadayappan, Ponnuswamy and Chamberlain, Bradford L. and Juckeland, Guido and Ltaief, Hatem},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {412--433},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-50743-5_21},
  abstract = {Hardware platforms in high performance computing are constantly getting more complex to handle even when considering multicore CPUs alone. Numerous features and configuration options in the hardware and the software environment that are relevant for performance are not even known to most application users or developers. Microbenchmarks, i.e., simple codes that fathom a particular aspect of the hardware, can help to shed light on such issues, but only if they are well understood and if the results can be reconciled with known facts or performance models. The insight gained from microbenchmarks may then be applied to real applications for performance analysis or optimization. In this paper we investigate two modern Intel x86 server CPU architectures in depth: Broadwell EP and Cascade Lake SP. We highlight relevant hardware configuration settings that can have a decisive impact on code performance and show how to properly measure on-chip and off-chip data transfer bandwidths. The new victim L3 cache of Cascade Lake and its advanced replacement policy receive due attention. Finally we use DGEMM, sparse matrix-vector multiplication, and the HPCG benchmark to make a connection to relevant application scenarios.},
  isbn = {978-3-030-50743-5},
  langid = {english},
  keywords = {Benchmarking,Intel,Microbenchmarking,x86},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/SFW3SHEY/Alappat и др. - 2020 - Understanding HPC Benchmark Performance on Intel B.pdf}
}

@article{arhipovGeneraciyaKodovDlya2020,
  title = {Генерация кодов для вещественной арифметики в архитектуре MIPS},
  author = {Архипов, Иван Сергеевич},
  date = {2020-08-01},
  journaltitle = {Труды Института системного программирования РАН},
  volume = {32},
  number = {3},
  pages = {49--56},
  issn = {2220-6426},
  url = {https://ispranproceedings.elpub.ru/jour/article/view/1300},
  urldate = {2023-03-01},
  abstract = {Научный рецензируемый журнал, список ВАК},
  issue = {3},
  langid = {russian},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/BP9IEHXD/Архипов - 2020 - Генерация кодов для вещественной арифметики в архи.pdf}
}

@article{ArmArchitectureReference,
  title = {Arm {{Architecture Reference Manual}} for {{A-profile}} Architecture},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ET7IN9MG/Arm Architecture Reference Manual for A-profile ar.pdf}
}

@article{Armv8MArchitectureReference2015,
  title = {Armv8-{{M Architecture Reference Manual}}},
  date = {2015},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/5XHX8FGN/2015 - Armv8-M Architecture Reference Manual.pdf}
}

@article{austinSimpleScalarInfrastructureComputer2002,
  title = {{{SimpleScalar}}: An Infrastructure for Computer System Modeling},
  shorttitle = {{{SimpleScalar}}},
  author = {Austin, T. and Larson, E. and Ernst, D.},
  date = {2002-02},
  journaltitle = {Computer},
  volume = {35},
  number = {2},
  pages = {59--67},
  issn = {1558-0814},
  doi = {10.1109/2.982917},
  abstract = {Designers can execute programs on software models to validate a proposed hardware design's performance and correctness, while programmers can use these models to develop and test software before the real hardware becomes available. Three critical requirements drive the implementation of a software model: performance, flexibility, and detail. Performance determines the amount of workload the model can exercise given the machine resources available for simulation. Flexibility indicates how well the model is structured to simplify modification, permitting design variants or even completely different designs to be modeled with ease. Detail defines the level of abstraction used to implement the model's components. The SimpleScalar tool set provides an infrastructure for simulation and architectural modeling. It can model a variety of platforms ranging from simple unpipelined processors to detailed dynamically scheduled microarchitectures with multiple-level memory hierarchies. SimpleScalar simulators reproduce computing device operations by executing all program instructions using an interpreter. The tool set's instruction interpreters also support several popular instruction sets, including Alpha, PPC, x86, and ARM.},
  eventtitle = {Computer},
  keywords = {Computational modeling,Computer aided instruction,Dynamic scheduling,Hardware,Instruction sets,Microarchitecture,Processor scheduling,Programming profession,Software performance,Software testing},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/EWD2KR5F/982917.html}
}

@inreference{AVX5122022,
  title = {{{AVX-512}}},
  booktitle = {Wikipedia},
  date = {2022-12-15T00:26:04Z},
  url = {https://en.wikipedia.org/w/index.php?title=AVX-512&oldid=1127485861},
  urldate = {2022-12-18},
  abstract = {AVX-512 are 512-bit extensions to the 256-bit Advanced Vector Extensions SIMD instructions for x86 instruction set architecture (ISA) proposed by Intel in July 2013, and implemented in Intel's Xeon Phi x200 (Knights Landing) and Skylake-X CPUs; this includes the Core-X series (excluding the Core i5-7640X and Core i7-7740X), as well as the new Xeon Scalable Processor Family and Xeon D-2100 Embedded Series. AVX-512 consists of multiple extensions that may be implemented independently. This policy is a departure from the historical requirement of implementing the entire instruction block. Only the core extension AVX-512F (AVX-512 Foundation) is required by all AVX-512 implementations. Besides widening most 256-bit instructions, the extensions introduce various new operations, such as new data conversions, scatter operations, and permutations. The number of AVX registers is increased from 16 to 32, and eight new "mask registers" are added, which allow for variable selection and blending of the results of instructions. In CPUs with the vector length (VL) extension—included in most AVX-512-capable processors (see § CPUs with AVX-512)—these instructions may also be used on the 128-bit and 256-bit vector sizes. AVX-512 is not the first 512-bit SIMD instruction set that Intel has introduced in processors: the earlier 512-bit SIMD instructions used in the first generation Xeon Phi coprocessors, derived from Intel's Larrabee project, are similar but not binary compatible and only partially source compatible.},
  langid = {english},
  annotation = {Page Version ID: 1127485861},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/CKCIM2ND/AVX-512.html}
}

@online{baghdadiTiramisuPolyhedralCompiler2018,
  title = {Tiramisu: {{A Polyhedral Compiler}} for {{Expressing Fast}} and {{Portable Code}}},
  shorttitle = {Tiramisu},
  author = {Baghdadi, Riyadh and Ray, Jessica and Romdhane, Malek Ben and Del Sozzo, Emanuele and Akkas, Abdurrahman and Zhang, Yunming and Suriana, Patricia and Kamil, Shoaib and Amarasinghe, Saman},
  date = {2018-12-20},
  eprint = {1804.10694},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1804.10694},
  urldate = {2022-12-17},
  abstract = {This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Mathematical Software,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,Computer Science - Programming Languages},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/QYR6A73S/Baghdadi и др. - 2018 - Tiramisu A Polyhedral Compiler for Expressing Fas.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/T3LGR96K/1804.html}
}

@online{banchelliPortableCodingStrategy2022,
  title = {A Portable Coding Strategy to Exploit Vectorization on Combustion Simulations},
  author = {Banchelli, Fabio and Oyarzun, Guillermo and Garcia-Gasulla, Marta and Mantovani, Filippo and Both, Ambrus and Houzeaux, Guillaume and Mira, Daniel},
  date = {2022-10-21},
  eprint = {2210.11917},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/2210.11917},
  urldate = {2022-12-17},
  abstract = {The complexity of combustion simulations demands the latest high-performance computing tools to accelerate its time-to-solution results. A current trend on HPC systems is the utilization of CPUs with SIMD or vector extensions to exploit data parallelism. Our work proposes a strategy to improve the automatic vectorization of finite element-based scientific codes. The approach applies a parametric configuration to the data structures to help the compiler detect the block of codes that can take advantage of vector computation while maintaining the code portable. A detailed analysis of the computational impact of this methodology on the different stages of a CFD solver is studied on the PRECCINSTA burner simulation. Our parametric implementation has proven to help the compiler generate more vector instructions in the assembly operation: this results in a reduction of up to 9.3 times of the total executed instruction maintaining constant the Instructions Per Cycle and the CPU frequency. The proposed strategy improves the performance of the CFD case under study up to 4.67 times on the MareNostrum 4 supercomputer.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance,Physics - Applied Physics},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/T4G7KKCV/Banchelli и др. - 2022 - A portable coding strategy to exploit vectorizatio.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/UF3P48HY/2210.html}
}

@inproceedings{bordesTranslatingEmbeddingsModeling2013,
  title = {Translating Embeddings for Modeling Multi-Relational Data},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{Neural Information Processing Systems}} - {{Volume}} 2},
  author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Durán, Alberto and Weston, Jason and Yakhnenko, Oksana},
  date = {2013-12-05},
  series = {{{NIPS}}'13},
  pages = {2787--2795},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.}
}

@book{bouguilaHiddenMarkovModels2022,
  title = {Hidden {{Markov Models}} and {{Applications}}},
  editor = {Bouguila, Nizar and Fan, Wentao and Amayri, Manar},
  date = {2022},
  series = {Unsupervised and {{Semi-Supervised Learning}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-99142-5},
  url = {https://link.springer.com/10.1007/978-3-030-99142-5},
  urldate = {2023-03-06},
  isbn = {978-3-030-99141-8 978-3-030-99142-5},
  langid = {english},
  keywords = {Bayesian learning,Hidden Markov models,infinite models,{machine learning, inference},nonparametric Bayesian,semi-supervised learning,supervised learning,unsupervised learning},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/9WMMABWR/Bouguila и др. - 2022 - Hidden Markov Models and Applications.pdf}
}

@inproceedings{brueningInfrastructureAdaptiveDynamic2003,
  title = {An Infrastructure for Adaptive Dynamic Optimization},
  booktitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}, 2003. {{CGO}} 2003.},
  author = {Bruening, D. and Garnett, T. and Amarasinghe, S.},
  date = {2003},
  pages = {265--275},
  publisher = {{IEEE Comput. Soc}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1109/CGO.2003.1191551},
  url = {http://ieeexplore.ieee.org/document/1191551/},
  urldate = {2023-08-12},
  abstract = {Dynamic optimization is emerging as a promising approach to overcome many of the obstacles of traditional static compilation. But while there are a number of compiler infrastructures for developing static optimizations, there are very few for developing dynamic optimizations. We present a framework for implementing dynamic analyses and optimizations. We provide an interface for building external modules, or clients, for the DynamoRIO dynamic code modification system. This interface abstracts away many low-level details of the DynamoRIO runtime system while exposing a simple and powerful, yet efficient and lightweight, API. This is achieved by restricting optimization units to linear streams of code and using adaptive levels of detail for representing instructions. The interface is not restricted to optimization and can be used for instrumentation, profiling, dynamic translation, etc.},
  eventtitle = {International {{Symposium}} on {{Code Generation}} and {{Optimization}}. {{CGO}} 2003},
  isbn = {978-0-7695-1913-5},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/4MEP5G4X/Bruening и др. - 2003 - An infrastructure for adaptive dynamic optimization.pdf}
}

@article{chaiImplementationOptimizationData2021,
  title = {Implementation and {{Optimization}} of {{Data Prefetching Algorithm Based}} on {{LLVM Compilation System}}},
  author = {Chai, Yunda and Chen, Mengyao and Li, Jianan and Han, Lin},
  date = {2021-03-01},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {1827},
  number = {1},
  pages = {012136},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1827/1/012136},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/1827/1/012136},
  urldate = {2022-12-18},
  abstract = {In order to reduce the problem of mismatch between high-performance processors and DRAM speeds, current processors have added a cache structure, but the low cache hit rate also seriously affects the actual performance of the program. Data prefetching technology can alleviate the problems of memory access latency and low hit rate caused by the speed difference between high-performance processors and DRAM. Based on the LLVM open source compiler, this article first implements the data prefetch module on the Shenwei platform. This paper improves the prefetch distance algorithm, proposes a new prefetch scheduling algorithm, introduces a cost model to evaluate the prefetch revenue, and accurately determines the insertion timing of the prefetch instruction to improve the cache hit rate.SPEC2006 performance test results show that after optimization, Shenwei 1621 processor single-core can achieve a maximum performance improvement of 50\%, and an average performance improvement of 11\%.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/LTXDN2XV/Chai и др. - 2021 - Implementation and Optimization of Data Prefetchin.pdf}
}

@inproceedings{chenAllYouNeed2022,
  title = {All You Need Is Superword-Level Parallelism: Systematic Control-Flow Vectorization with {{SLP}}},
  shorttitle = {All You Need Is Superword-Level Parallelism},
  booktitle = {Proceedings of the 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Chen, Yishen and Mendis, Charith and Amarasinghe, Saman},
  date = {2022-06-09},
  pages = {301--315},
  publisher = {{ACM}},
  location = {{San Diego CA USA}},
  doi = {10.1145/3519939.3523701},
  url = {https://dl.acm.org/doi/10.1145/3519939.3523701},
  urldate = {2023-05-01},
  eventtitle = {{{PLDI}} '22: 43rd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  isbn = {978-1-4503-9265-5},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/GN8VIKBZ/Chen и др. - 2022 - All you need is superword-level parallelism syste.pdf}
}

@article{chenAutomaticSuperwordVectorization,
  title = {An {{Automatic Superword Vectorization}} in {{LLVM}}},
  author = {Chen, Kuan-Hsu and Shen, Bor-Yeh and Yang, Wuu},
  abstract = {More and more modern processors support SIMD instructions for improving performance in media applications. Programmers usually need detailed targetspecific knowledge to use SIMD instructions directly. Thus, an auto-vectorization compiler that automatically generates efficient SIMD instructions is in urgent need. We implement an automatic superword vectorization based on the LLVM compiler infrastructure, to which an autovectorization and an alignment analysis passes have been added.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/AIDP6RN2/Chen и др. - An Automatic Superword Vectorization in LLVM.pdf}
}

@inproceedings{chenBHiveBenchmarkSuite2019,
  title = {{{BHive}}: {{A Benchmark Suite}} and {{Measurement Framework}} for {{Validating}} X86-64 {{Basic Block Performance Models}}},
  shorttitle = {{{BHive}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}})},
  author = {Chen, Yishen and Brahmakshatriya, Ajay and Mendis, Charith and Renda, Alex and Atkinson, Eric and Sýkora, Ondřej and Amarasinghe, Saman and Carbin, Michael},
  date = {2019-11},
  pages = {167--177},
  doi = {10.1109/IISWC47752.2019.9042166},
  abstract = {Compilers and performance engineers use hardware performance models to simplify program optimizations. Performance models provide a necessary abstraction over complex modern processors. However, constructing and maintaining a performance model can be onerous, given the numerous microarchi-tectural optimizations employed by modern processors. Despite their complexity and reported inaccuracy (e.g., deviating from native measurement by more than 30\%), existing performance models-such as IACA and llvm-mca-have not been systematically validated, because there is no scalable machine code profiler that can automatically obtain throughput of arbitrary basic blocks while conforming to common modeling assumptions. In this paper, we present a novel profiler that can profile arbitrary memory-accessing basic blocks without any user intervention. We used this profiler to build BHive, a benchmark for systematic validation of performance models of x86-64 basic blocks. We used BHive to evaluate four existing performance models: IACA, llvm-mca, Ithemal, and OSACA. We automatically cluster basic blocks in the benchmark suite based on their utilization of CPU resources. Using this clustering, our benchmark can give a detailed analysis of a performance model's strengths and weaknesses on different workloads (e.g., vectorized vs. scalar basic blocks). We additionally demonstrate that our dataset well captures basic properties of two Google applications: Spanner and Dremel.},
  eventtitle = {2019 {{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}})},
  keywords = {Benchmarking,Cost/performance,Measurement techniques,Modeling techniques},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/653ZVE96/Chen и др. - 2019 - BHive A Benchmark Suite and Measurement Framework.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/HIJ82V55/chen2019.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/DVG594UP/9042166.html}
}

@inproceedings{chenVeGenVectorizerGenerator2021,
  title = {{{VeGen}}: A Vectorizer Generator for {{SIMD}} and Beyond},
  shorttitle = {{{VeGen}}},
  booktitle = {Proceedings of the 26th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Chen, Yishen and Mendis, Charith and Carbin, Michael and Amarasinghe, Saman},
  date = {2021-04-19},
  pages = {902--914},
  publisher = {{ACM}},
  location = {{Virtual USA}},
  doi = {10.1145/3445814.3446692},
  url = {https://dl.acm.org/doi/10.1145/3445814.3446692},
  urldate = {2022-12-17},
  abstract = {Vector instructions are ubiquitous in modern processors. Traditional compiler auto-vectorization techniques have focused on targeting single instruction multiple data (SIMD) instructions. However, these auto-vectorization techniques are not sufficiently powerful to model non-SIMD vector instructions, which can accelerate applications in domains such as image processing, digital signal processing, and machine learning. To target non-SIMD instruction, compiler developers have resorted to complicated, ad hoc peephole optimizations, expending significant development time while still coming up short. As vector instruction sets continue to rapidly evolve, compilers cannot keep up with these new hardware capabilities.},
  eventtitle = {{{ASPLOS}} '21: 26th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  isbn = {978-1-4503-8317-2},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/CYARKFCV/Chen и др. - 2021 - VeGen a vectorizer generator for SIMD and beyond.pdf}
}

@online{CORAL2Benchmarks,
  title = {{{CORAL-2 Benchmarks}}},
  url = {https://asc.llnl.gov/coral-2-benchmarks},
  urldate = {2022-12-18},
  abstract = {Introduction The CORAL-2 benchmarks contained within this site represent their state as of early December 2017. Over the next few months the list of applications will be changing. While we expect most of the changes will be additions it is possible that we will remove applications or change their category.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/AQ9N4U2K/coral-2-benchmarks.html}
}

@article{dahoudaDeepLearnedEmbeddingTechnique2021,
  title = {A {{Deep-Learned Embedding Technique}} for {{Categorical Features Encoding}}},
  author = {Dahouda, Mwamba Kasongo and Joe, Inwhee},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {114381--114391},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3104357},
  abstract = {Many machine learning algorithms and almost all deep learning architectures are incapable of processing plain texts in their raw form. This means that their input to the algorithms must be numerical in order to solve classification or regression problems. Hence, it is necessary to encode these categorical variables into numerical values using encoding techniques. Categorical features are common and often of high cardinality. One-hot encoding in such circumstances leads to very high dimensional vector representations, raising memory and computability concerns for machine learning models. This paper proposes a deep-learned embedding technique for categorical features encoding on categorical datasets. Our technique is a distributed representation for categorical features where each category is mapped to a distinct vector, and the properties of the vector are learned while training a neural network. First, we create a data vocabulary that includes only categorical data, and then we use word tokenization to make each categorical data a single word. After that, feature learning is introduced to map all of the categorical data from the vocabulary to word vectors. Three different datasets provided by the University of California Irvine (UCI) are used for training. The experimental results show that the proposed deep-learned embedding technique for categorical data provides a higher F1 score of 89\% than 71\% of one-hot encoding, in the case of the Long short-term memory (LSTM) model. Moreover, the deep-learned embedding technique uses less memory and generates fewer features than one-hot encoding.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biological neural networks,categorical variables,Computational modeling,Data models,Data preprocessing,Encoding,machine learning,Machine learning,natural language processing,Numerical models,Training},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/4SWLQWXX/Dahouda и Joe - 2021 - A Deep-Learned Embedding Technique for Categorical.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/XG5H8GNT/dahouda2021.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/Y6WYV874/stamp.html}
}

@online{dasMLdrivenHardwareCost2023,
  title = {{{ML-driven Hardware Cost Model}} for {{MLIR}}},
  author = {Das, Dibyendu and Mannarswamy, Sandya},
  date = {2023-02-14},
  eprint = {2302.11405},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2302.11405},
  urldate = {2023-04-16},
  abstract = {During early optimization passes, compilers must make predictions for machine-dependent characteristics such as execution unit utilization, number of register spills, latency, throughput etc. to generate better code. Often a hand-written static/analytical hardware cost model is built into the compiler. However, the need for more sophisticated and varied predictions has become more pronounced with the development of deep learning compilers which need to optimize dataflow graphs. Such compilers usually employ a much higher level MLIR form as an IR representation before lowering to traditional LLVM-IR. A static/analytical cost model in such a scenario is cumbersome and error prone as the opcodes represent very high level algebraic/arithmetic operations. Hence, we develop a machine learning-based cost model for high-level MLIR which can predict different target variables of interest such as CPU/GPU/xPU utilization, instructions executed, register usage etc. By considering the incoming MLIR as a text input a la NLP models we can apply well-known techniques from modern NLP research to help predict hardware characteristics more accurately. We expect such precise ML-driven hardware cost models to guide our deep learning compiler in graph level optimizations around operator fusion, local memory allocation, kernel scheduling etc. as well as in many kernel-level optimizations such as loop interchange, LICM and unroll. We report early work-in -progress results of developing such models on high-level MLIR representing dataflow graphs emitted by Pytorch/Tensorflow-like frameworks as well as lower-level dialects like affine. We show that these models can provide reasonably good estimates with low error bounds for various hardware characteristics of interest and can be a go-to mechanism for hardware cost modelling in the future.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/AXUD9GB7/Das и Mannarswamy - 2023 - ML-driven Hardware Cost Model for MLIR.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/53RDENFM/2302.html}
}

@online{derumignyPALMEDThroughputCharacterization2022,
  title = {{{PALMED}}: {{Throughput Characterization}} for {{Superscalar Architectures}} -- {{Extended Version}}},
  shorttitle = {{{PALMED}}},
  author = {Derumigny, Nicolas and Gruber, Fabian and Bastian, Théophile and Iooss, Guillaume and Guillon, Christophe and Pouchet, Louis-Noël and Rastello, Fabrice},
  date = {2022-01-18},
  eprint = {2012.11473},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.11473},
  urldate = {2023-03-06},
  abstract = {In a super-scalar architecture, the scheduler dynamically assigns micro-operations (\$\textbackslash mu\$OPs) to execution ports. The port mapping of an architecture describes how an instruction decomposes into \$\textbackslash mu\$OPs and lists for each \$\textbackslash mu\$OP the set of ports it can be mapped to. It is used by compilers and performance debugging tools to characterize the performance throughput of a sequence of instructions repeatedly executed as the core component of a loop. This paper introduces a dual equivalent representation: The resource mapping of an architecture is an abstract model where, to be executed, an instruction must use a set of abstract resources, themselves representing combinations of execution ports. For a given architecture, finding a port mapping is an important but difficult problem. Building a resource mapping is a more tractable problem and provides a simpler and equivalent model. This paper describes Palmed, a tool that automatically builds a resource mapping for pipelined, super-scalar, out-of-order CPU architectures. Palmed does not require hardware performance counters, and relies solely on runtime measurements. We evaluate the pertinence of our dual representation for throughput modeling by extracting a representative set of basic-blocks from the compiled binaries of the SPEC CPU 2017 benchmarks. We compared the throughput predicted by existing machine models to that produced by Palmed, and found comparable accuracy to state-of-the art tools, achieving sub-10 \% mean square error rate on this workload on Intel's Skylake microarchitecture.},
  pubstate = {preprint},
  keywords = {Computer Science - Hardware Architecture,Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/7NPK634K/Derumigny и др. - 2022 - PALMED Throughput Characterization for Superscala.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/7RIWAHPN/2012.html}
}

@article{devkotaCcNavUnderstandingCompiler2021,
  title = {{{CcNav}}: {{Understanding Compiler Optimizations}} in {{Binary Code}}},
  shorttitle = {{{CcNav}}},
  author = {Devkota, Sabin and Aschwanden, Pascal and Kunen, Adam and Legendre, Matthew and Isaacs, Katherine E.},
  date = {2021-02},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  volume = {27},
  number = {2},
  eprint = {2009.00956},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {667--677},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2020.3030357},
  url = {http://arxiv.org/abs/2009.00956},
  urldate = {2022-12-17},
  abstract = {Program developers spend significant time on optimizing and tuning programs. During this iterative process, they apply optimizations, analyze the resulting code, and modify the compilation until they are satisfied. Understanding what the compiler did with the code is crucial to this process but is very time-consuming and labor-intensive. Users need to navigate through thousands of lines of binary code and correlate it to source code concepts to understand the results of the compilation and to identify optimizations. We present a design study in collaboration with program developers and performance analysts. Our collaborators work with various artifacts related to the program such as binary code, source code, control flow graphs, and call graphs. Through interviews, feedback, and pair-analytics sessions, we analyzed their tasks and workflow. Based on this task analysis and through a human-centric design process, we designed a visual analytics system Compilation Navigator (CcNav) to aid exploration of the effects of compiler optimizations on the program. CcNav provides a streamlined workflow and a unified context that integrates disparate artifacts. CcNav supports consistent interactions across all the artifacts making it easy to correlate binary code with source code concepts. CcNav enables users to navigate and filter large binary code to identify and summarize optimizations such as inlining, vectorization, loop unrolling, and code hoisting. We evaluate CcNav through guided sessions and semi-structured interviews. We reflect on our design process, particularly the immersive elements, and on the transferability of design studies through our experience with a previous design study on program analysis.},
  keywords = {Computer Science - Human-Computer Interaction,D.2.5,H.5.2},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/8AFHAGHA/Devkota и др. - 2021 - CcNav Understanding Compiler Optimizations in Bin.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/VAXJMF8U/2009.html}
}

@inproceedings{engelkeInstrewLeveragingLLVM2020,
  title = {Instrew: Leveraging {{LLVM}} for High Performance Dynamic Binary Instrumentation},
  shorttitle = {Instrew},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN}}/{{SIGOPS International Conference}} on {{Virtual Execution Environments}}},
  author = {Engelke, Alexis and Schulz, Martin},
  date = {2020-03-17},
  pages = {172--184},
  publisher = {{ACM}},
  location = {{Lausanne Switzerland}},
  doi = {10.1145/3381052.3381319},
  url = {https://dl.acm.org/doi/10.1145/3381052.3381319},
  urldate = {2022-12-31},
  abstract = {Dynamic binary instrumentation frameworks are popular tools to enhance programs with additional analysis, debugging, or profiling facilities or to add optimizations or translations without requiring recompilation or access to source code. They analyze the binary code, translate into a—typically low-level—intermediate representation, add the needed instrumentation or transformation and then generate new code on-demand and at run-time. Most tools thereby focus on a fast code rewriting process at the cost of lower quality code, leading to a significant slowdown in the instrumented code. Further, most tools run in the application’s address space, making their development cumbersome.},
  eventtitle = {{{VEE}} '20: 16th {{ACM SIGPLAN}}/{{SIGOPS International Conference}} on {{Virtual Execution Environments}}},
  isbn = {978-1-4503-7554-2},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/KH6A9I27/engelke2020.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/T3UK435H/Engelke и Schulz - 2020 - Instrew leveraging LLVM for high performance dyna.pdf}
}

@online{EnhancePerformanceAnalysis,
  title = {Enhance Performance Analysis with {{Intel Processor Trace}}. | {{Easyperf}}},
  url = {https://easyperf.net/blog/2019/08/23/Intel-Processor-Trace},
  urldate = {2023-01-03},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/NFJPLLJ2/Intel-Processor-Trace.html}
}

@article{erezSoftwarePrefetchingMemorylevel,
  title = {Software {{Prefetching}} for {{Memory-level Parallelism}}},
  author = {Erez, Mattan},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/BNSBLEUB/Erez - Software Prefetching for Memory-level Parallelism.pdf}
}

@online{ernstAnalyticalPerformanceEstimation2022,
  title = {Analytical {{Performance Estimation}} during {{Code Generation}} on {{Modern GPUs}}},
  author = {Ernst, Dominik and Holzer, Markus and Hager, Georg and Knorr, Matthias and Wellein, Gerhard},
  date = {2022-04-29},
  eprint = {2204.14242},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.14242},
  url = {http://arxiv.org/abs/2204.14242},
  urldate = {2022-12-31},
  abstract = {Automatic code generation is frequently used to create implementations of algorithms specifically tuned to particular hardware and application parameters. The code generation process involves the selection of adequate code transformations, tuning parameters, and parallelization strategies. We propose an alternative to time-intensive autotuning, scenario-specific performance models, or black-box machine learning to select the best-performing configuration. This paper identifies the relevant performance-defining mechanisms for memory-intensive GPU applications through a performance model coupled with an analytic hardware metric estimator. This enables a quick exploration of large configuration spaces to identify highly efficient code candidates with high accuracy. We examine the changes of the A100 GPU architecture compared to the predecessor V100 and address the challenges of how to model the data transfer volumes through the new memory hierarchy. We show how our method can be coupled to the pystencils stencil code generator, which is used to generate kernels for a range-four 3D-25pt stencil and a complex two-phase fluid solver based on the Lattice Boltzmann Method. For both, it delivers a ranking that can be used to select the best-performing candidate. The method is not limited to stencil kernels but can be integrated into any code generator that can generate the required address expressions.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/B58V2A9X/Ernst и др. - 2022 - Analytical Performance Estimation during Code Gene.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/PJT7D2AG/2204.html}
}

@article{ernstAnalyticalPerformanceEstimation2023,
  title = {Analytical Performance Estimation during Code Generation on Modern {{GPUs}}},
  author = {Ernst, Dominik and Holzer, Markus and Hager, Georg and Knorr, Matthias and Wellein, Gerhard},
  date = {2023-03-01},
  journaltitle = {Journal of Parallel and Distributed Computing},
  shortjournal = {Journal of Parallel and Distributed Computing},
  volume = {173},
  pages = {152--167},
  issn = {0743-7315},
  doi = {10.1016/j.jpdc.2022.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0743731522002313},
  urldate = {2022-12-31},
  abstract = {Automatic code generation is frequently used to create implementations of algorithms specifically tuned to particular hardware and application parameters. The code generation process involves the selection of adequate code transformations, tuning parameters, and parallelization strategies. We propose an alternative to time-intensive autotuning, scenario-specific performance models, or black-box machine learning to select the best-performing configuration. This paper identifies the relevant performance-defining mechanisms for memory-intensive GPU applications through a performance model coupled with an analytic hardware metric estimator. This enables a quick exploration of large configuration spaces to identify highly efficient code candidates with high accuracy. We examine the changes of the A100 GPU architecture compared to the predecessor V100 and address the challenges of how to model the data transfer volumes through the new memory hierarchy. We show how our method can be coupled to the “pystencils” stencil code generator, which is used to generate kernels for a range-four 3D-25pt stencil and a complex two-phase fluid solver based on the Lattice Boltzmann Method. For both, it delivers a ranking that can be used to select the best-performing candidate. The method is not limited to stencil kernels but can be integrated into any code generator that can generate the required address expressions.},
  langid = {english},
  keywords = {Analytical performance modeling,GPU,GPU performance model,Layer condition,Stencil codes},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/IK6LM2P5/Ernst и др. - 2023 - Analytical performance estimation during code gene.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/VANP2U8X/Ernst и др. - 2023 - Analytical performance estimation during code gene.pdf}
}

@online{ernstOpeningBlackBox2021,
  title = {Opening the {{Black Box}}: {{Performance Estimation}} during {{Code Generation}} for {{GPUs}}},
  shorttitle = {Opening the {{Black Box}}},
  author = {Ernst, Dominik and Hager, Georg and Holzer, Markus and Knorr, Matthias and Wellein, Gerhard},
  date = {2021-07-02},
  eprint = {2107.01143},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2107.01143},
  url = {http://arxiv.org/abs/2107.01143},
  urldate = {2022-12-31},
  abstract = {Automatic code generation is frequently used to create implementations of algorithms specifically tuned to particular hardware and application parameters. The code generation process involves the selection of adequate code transformations, tuning parameters, and parallelization strategies. To cover the huge search space, code generation frameworks may apply time-intensive autotuning, exploit scenario-specific performance models, or treat performance as an intangible black box that must be described via machine learning. This paper addresses the selection problem by identifying the relevant performance-defining mechanisms through a performance model coupled with an analytic hardware metric estimator. This enables a quick exploration of large configuration spaces to identify highly efficient candidates with high accuracy. Our current approach targets memory-intensive GPGPU applications and focuses on the correct modeling of data transfer volumes to all levels of the memory hierarchy. We show how our method can be coupled to the pystencils stencil code generator, which is used to generate kernels for a range four 3D25pt stencil and a complex two phase fluid solver based on the Lattice Boltzmann Method. For both, it delivers a ranking that can be used to select the best performing candidate. The method is not limited to stencil kernels, but can be integrated into any code generator that can generate the required address expressions.},
  pubstate = {preprint},
  keywords = {C.4,Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/MKB9VCS6/Ernst и др. - 2021 - Opening the Black Box Performance Estimation duri.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/7GHNNXBR/2107.html}
}

@online{fengTensorIRAbstractionAutomatic2022,
  title = {{{TensorIR}}: {{An Abstraction}} for {{Automatic Tensorized Program Optimization}}},
  shorttitle = {{{TensorIR}}},
  author = {Feng, Siyuan and Hou, Bohan and Jin, Hongyi and Lin, Wuwei and Shao, Junru and Lai, Ruihang and Ye, Zihao and Zheng, Lianmin and Yu, Cody Hao and Yu, Yong and Chen, Tianqi},
  date = {2022-10-27},
  eprint = {2207.04296},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.04296},
  urldate = {2022-12-17},
  abstract = {Deploying deep learning models on various devices has become an important topic. The wave of hardware specialization brings a diverse set of acceleration primitives for multi-dimensional tensor computations. These new acceleration primitives, along with the emerging machine learning models, bring tremendous engineering challenges. In this paper, we present TensorIR, a compiler abstraction for optimizing programs with these tensor computation primitives. TensorIR generalizes the loop nest representation used in existing machine learning compilers to bring tensor computation as the first-class citizen. Finally, we build an end-to-end framework on top of our abstraction to automatically optimize deep learning models for given tensor computation primitives. Experimental results show that TensorIR compilation automatically uses the tensor computation primitives for given hardware backends and delivers performance that is competitive to state-of-art hand-optimized systems across platforms.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/PINH82GC/Feng и др. - 2022 - TensorIR An Abstraction for Automatic Tensorized .pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/VZXV84WD/2207.html}
}

@inproceedings{firemanNewAlgorithmsSIMD2007,
  title = {New {{Algorithms}} for {{SIMD Alignment}}},
  author = {Fireman, Liza and Petrank, Erez and Zaks, Ayal},
  date = {2007-03-26},
  pages = {1--15},
  doi = {10.1007/978-3-540-71229-9_1},
  abstract = {Optimizing programs for modern multiprocessor or vector platforms is a major important challenge for compilers today. In this work, we focus on one challenging aspect: the SIMD ALIGNMENT problem. Previously, only heuristics were used to solve this problem, without guarantees on the number of shifts in the obtained solution. We study two interesting and realistic special cases of the SIMD ALIGNMENT problem and present two novel and efficient algorithms that provide optimal solutions for these two cases. The new algorithms employ dynamic pro- gramming and a MIN-CUT/MAX-FLOW algorithm as subroutines. We also discuss the relation between the SIMD ALIGNMENT problem and the MULTIWAY CUT and NODE MULTIWAY CUT problems; and we show how to derive an approximated solution to the SIMD ALIGNMENT problem based on approximation algorithms to these two known problems. Designing effective optimizations for modern architectures is an important goal for compiler designers today. This general task is composed of many non-trivial problems, the solution to which is not always known. In this paper we study one such problem — the SIMD ALIGNMENT problem, which emerges when optimizing for multimedia extensions. Previously only heuristics were studied for this problem (25, 30, 14). In this paper we present two novel algorithms that obtain optimal solutions for two special cases. These special cases are actually broad enough to cover many practical instances of the SIMD ALIGNMENT problem.},
  isbn = {978-3-540-71228-2},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/K2IUBLR6/Fireman и др. - 2007 - New Algorithms for SIMD Alignment.pdf}
}

@inproceedings{flynnExploringSourcetosourceCompiler2022,
  title = {Exploring Source-to-Source Compiler Transformation of {{OpenMP SIMD}} Constructs for {{Intel AVX}} and {{Arm SVE}} Vector Architectures},
  booktitle = {Proceedings of the {{Thirteenth International Workshop}} on {{Programming Models}} and {{Applications}} for {{Multicores}} and {{Manycores}}},
  author = {Flynn, Patrick and Yi, Xinyao and Yan, Yonghong},
  date = {2022-04-02},
  pages = {11--20},
  publisher = {{ACM}},
  location = {{Seoul Republic of Korea}},
  doi = {10.1145/3528425.3529100},
  url = {https://dl.acm.org/doi/10.1145/3528425.3529100},
  urldate = {2022-12-17},
  abstract = {Over the past decade, SIMD (single instruction multiple data) or vector architectures have made significant advances, now existing across a wide range of devices from commodity CPUs to high performance computing (HPC) cores. Intel’s AVX (Advanced Vector Extensions) architecture has been one of the most popular SIMD extensions to commodity and HPC CPUs from Intel. Over the past few years, Arm has made significant inroads with its new SVE (Scalable Vector Extension), used in the supercomputer of the top place in the Top500 list. As SIMD has become more advanced and more important, it has become equally important the compilers support these architecture extensions. In this paper, we present our approach of source-to-source compiler transformation of explicit vectorization constructs using the OpenMP SIMD directive. We present the design of a unified IR that is easily translated to AVX and SVE vector architectures. Finally, we conduct performance evaluations on Intel AVX and Arm SVE to demonstrate how this method of vectorization can bridge the gap between auto- and manual- vectorization.},
  eventtitle = {{{PPoPP}} '22: 27th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  isbn = {978-1-4503-9339-3},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/K3AE4XBQ/Flynn и др. - 2022 - Exploring source-to-source compiler transformation.pdf}
}

@article{frolovIssledovanieTehnologiiRISCV2020,
  title = {Исследование технологии RISC-V},
  author = {Фролов, Владимир Александрович and Галактионов, Владимир Александрович and Санжаров, Вадим Владимирович},
  date = {2020-06-15},
  journaltitle = {Труды Института системного программирования РАН},
  volume = {32},
  number = {2},
  pages = {81--98},
  issn = {2220-6426},
  url = {https://ispranproceedings.elpub.ru/jour/article/view/1285},
  urldate = {2023-03-01},
  abstract = {Научный рецензируемый журнал, список ВАК},
  issue = {2},
  langid = {russian},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/IDGI57PY/Фролов и др. - 2020 - Исследование технологии RISC-V.pdf}
}

@article{gBLISFrameworkRapidly2015,
  title = {{{BLIS}}: {{A Framework}} for {{Rapidly Instantiating BLAS Functionality}}},
  shorttitle = {{{BLIS}}},
  author = {G, Van ZeeField and A, van de GeijnRobert},
  date = {2015-06-01},
  journaltitle = {ACM Transactions on Mathematical Software (TOMS)},
  publisher = {{ACM}},
  doi = {10.1145/2764454},
  url = {https://dl.acm.org/doi/10.1145/2764454},
  urldate = {2022-12-18},
  abstract = {The BLAS-like Library Instantiation Software (BLIS) framework is a new infrastructure for rapidly instantiating Basic Linear Algebra Subprograms (BLAS) functionality. Its fundamental innovation is that virtually all computation within level-2 (matrix-...},
  langid = {english},
  annotation = {PUB27 		New York, NY, USA},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/FCAZUGAZ/2764454.html}
}

@article{gebraadSeamlessGPUAcceleration2022,
  title = {Seamless {{GPU}} Acceleration for {{C}}++ Based Physics with the {{Metal Shading Language}} on {{Apple}}'s {{M}} Series Unified Chips},
  author = {Gebraad, Lars and Fichtner, Andreas},
  date = {2022-06-01},
  journaltitle = {arXiv e-prints},
  url = {https://ui.adsabs.harvard.edu/abs/2022arXiv220601791G},
  urldate = {2022-12-18},
  abstract = {The M series of chips produced by Apple have proven a capable and power-efficient alternative to mainstream Intel and AMD x86 processors for everyday tasks. Additionally, the unified design integrating the central processing and graphics processing unit, have allowed these M series chips to excel at many tasks with heavy graphical requirements without the need for a discrete graphical processing unit (GPU), and in some cases even outperforming discrete GPUs. In this work, we show how the M series chips can be leveraged using the Metal Shading Language (MSL) to accelerate typical array operations in C++. More importantly, we show how the usage of MSL avoids the typical complexity of CUDA or OpenACC memory management, by allowing the central processing unit (CPU) and GPU to work in unified memory. We demonstrate how performant the M series chips are on standard one-dimensional and two-dimensional array operations such as array addition, SAXPY and finite difference stencils, with respect to serial and OpenMP accelerated CPU code. The reduced complexity of implementing MSL also allows us to accelerate an existing elastic wave equation solver (originally based on OpenMP accelerated C++) using MSL, with minimal effort, while retaining all CPU and OpenMP functionality. The resulting performance gain of simulating the wave equation is near an order of magnitude for specific settings. This gain attained from using MSL is similar to other GPU-accelerated wave-propagation codes with respect to their CPU variants, but does not come at much increased programming complexity that prohibits the typical scientific programmer to leverage these accelerators. This result shows how unified processing units can be a valuable tool to seismologists and computational scientists in general, lowering the bar to writing performant codes that leverage modern GPUs.},
  keywords = {Physics - Computational Physics,Physics - Geophysics},
  annotation = {ADS Bibcode: 2022arXiv220601791G},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/P5U8LJWF/Gebraad и Fichtner - 2022 - Seamless GPU acceleration for C++ based physics wi.pdf}
}

@article{georgiouLostTranslationExposing2022,
  title = {Lost in Translation: {{Exposing}} Hidden Compiler Optimization Opportunities},
  shorttitle = {Lost in Translation},
  author = {Georgiou, Kyriakos and Chamski, Zbigniew and Garcia, Andres Amaya and May, David and Eder, Kerstin},
  date = {2022-03-14},
  journaltitle = {The Computer Journal},
  volume = {65},
  number = {3},
  eprint = {1903.11397},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {718--735},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/bxaa103},
  url = {http://arxiv.org/abs/1903.11397},
  urldate = {2022-12-17},
  abstract = {Existing iterative compilation and machine-learning-based optimization techniques have been proven very successful in achieving better optimizations than the standard optimization levels of a compiler. However, they were not engineered to support the tuning of a compiler's optimizer as part of the compiler's daily development cycle. In this paper, we first establish the required properties which a technique must exhibit to enable such tuning. We then introduce an enhancement to the classic nightly routine testing of compilers which exhibits all the required properties, and thus, is capable of driving the improvement and tuning of the compiler's common optimizer. This is achieved by leveraging resource usage and compilation information collected while systematically exploiting prefixes of the transformations applied at standard optimization levels. Experimental evaluation using the LLVM v6.0.1 compiler demonstrated that the new approach was able to reveal hidden cross-architecture and architecture-dependent potential optimizations on two popular processors: the Intel i5-6300U and the Arm Cortex-A53-based Broadcom BCM2837 used in the Raspberry Pi 3B+. As a case study, we demonstrate how the insights from our approach enabled us to identify and remove a significant shortcoming of the CFG simplification pass of the LLVM v6.0.1 compiler.},
  keywords = {Computer Science - Programming Languages},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/NM6GGT4J/Georgiou и др. - 2022 - Lost in translation Exposing hidden compiler opti.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/KJCQ7NRZ/1903.html}
}

@inproceedings{ghiglioImprovingPerformanceSYCL2022,
  title = {Improving Performance of {{SYCL}} Applications on {{CPU}} Architectures Using {{LLVM-directed}} Compilation Flow},
  booktitle = {Proceedings of the {{Thirteenth International Workshop}} on {{Programming Models}} and {{Applications}} for {{Multicores}} and {{Manycores}}},
  author = {Ghiglio, Pietro and Dolinsky, Uwe and Goli, Mehdi and Narasimhan, Kumudha},
  date = {2022-04-02},
  pages = {1--10},
  publisher = {{ACM}},
  location = {{Seoul Republic of Korea}},
  doi = {10.1145/3528425.3529099},
  url = {https://dl.acm.org/doi/10.1145/3528425.3529099},
  urldate = {2022-12-17},
  eventtitle = {{{PPoPP}} '22: 27th {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  isbn = {978-1-4503-9339-3},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/3HK3LVLD/Ghiglio и др. - 2022 - Improving performance of SYCL applications on CPU .pdf}
}

@online{gibsonTransferTuningReusingAutoSchedules2022,
  title = {Transfer-{{Tuning}}: {{Reusing Auto-Schedules}} for {{Efficient Tensor Program Code Generation}}},
  shorttitle = {Transfer-{{Tuning}}},
  author = {Gibson, Perry and Cano, José},
  date = {2022-09-07},
  eprint = {2201.05587},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.05587},
  urldate = {2022-12-17},
  abstract = {Auto-scheduling for tensor programs is a process where a search algorithm automatically explores candidate schedules (program transformations) for a given program on a target hardware platform to improve its performance. However this can be a very time consuming process depending on the complexity of the tensor program and the capacity of the target device, with often many thousands of program variants being explored. To address this, in this paper we introduce the idea of transfer-tuning, a novel approach to identify and reuse auto-schedules between tensor programs. We demonstrate this concept using Deep Neural Networks (DNNs), taking sets of auto-schedules from pre-tuned DNNs and using them to reduce the inference time of a new DNN. We compare transfer-tuning against the state-of-the-art Ansor auto-scheduler, defining the maximum possible speedup for a given DNN model as what Ansor achieves using its recommended full tuning time. On a server-class CPU and across 11 widely used DNN models, we observe that transfer-tuning achieves up to \$88.41\textbackslash\%\$ (\$49.13\textbackslash\%\$ on average) of this maximum speedup, while Ansor requires \$6.5\textbackslash times\$ more search time on average to match it. We also evaluate transfer-tuning on a constrained edge CPU and observe that the differences in search time are exacerbated, with Ansor requiring \$10.8\textbackslash times\$ more time on average to match transfer-tuning's speedup, which further demonstrates its value. Our code is available at https://www.github.com/gicLAB/transfer-tuning},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,Computer Science - Programming Languages},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/9UK5Y4RK/Gibson и Cano - 2022 - Transfer-Tuning Reusing Auto-Schedules for Effici.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/Z2AUBG5U/2201.html}
}

@article{gilmanDemystifyingPlacementPolicies2021,
  title = {Demystifying the {{Placement Policies}} of the {{NVIDIA GPU Thread Block Scheduler}} for {{Concurrent Kernels}}},
  author = {Gilman, Guin and Ogden, Samuel S. and Guo, Tian and Walls, Robert J.},
  date = {2021-03-05},
  journaltitle = {ACM SIGMETRICS Performance Evaluation Review},
  shortjournal = {SIGMETRICS Perform. Eval. Rev.},
  volume = {48},
  number = {3},
  pages = {81--88},
  issn = {0163-5999},
  doi = {10.1145/3453953.3453972},
  url = {https://dl.acm.org/doi/10.1145/3453953.3453972},
  urldate = {2022-12-18},
  abstract = {In this work, we empirically derive the scheduler’s behavior under concurrent workloads for NVIDIA’s Pascal, Volta, and Turing microarchitectures. In contrast to past studies that suggest the scheduler uses a round-robin policy to assign thread blocks to streaming multiprocessors (SMs), we instead find that the scheduler chooses the next SM based on the SM’s local resource availability. We show how this scheduling policy can lead to significant, and seemingly counter-intuitive, performance degradation; for example, a decrease of one thread per block resulted in a 3.58X increase in execution time for one kernel in our experiments. We hope that our work will be useful for improving the accuracy of GPU simulators and aid in the development of novel scheduling algorithms.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/5P88H3BT/Gilman и др. - 2021 - Demystifying the Placement Policies of the NVIDIA .pdf}
}

@thesis{gocFastFourierTransform,
  title = {Fast {{Fourier Transform}} Using Compiler Auto-Vectorization},
  author = {Göc, Dundar},
  url = {https://odr.chalmers.se/server/api/core/bitstreams/33ebecda-0e40-4b5d-9f2d-488a0447bad1/content},
  urldate = {2022-12-17},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/YMITSS63/content.pdf}
}

@inproceedings{gongTwinKernelsExecutionModel2017,
  title = {{{TwinKernels}}: {{An}} Execution Model to Improve {{GPU}} Hardware Scheduling at Compile Time},
  shorttitle = {{{TwinKernels}}},
  booktitle = {2017 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  author = {Gong, Xiang and Chen, Zhongliang and Ziabari, Amir Kavyan and Ubal, Rafael and Kaeli, David},
  date = {2017-02},
  pages = {39--49},
  doi = {10.1109/CGO.2017.7863727},
  abstract = {As throughput-oriented accelerators, GPUs provide tremendous processing power by running a massive number of threads in parallel. However, exploiting high degrees of thread-level parallelism (TLP) does not always translate to the peak performance that GPUs can offer, leaving the GPU's resources often under-utilized. Compared to compute resources, memory resources can tolerate considerably lower levels of TLP due to hardware bottlenecks. Unfortunately, this tolerance is not effectively exploited by the Single Instruction Multiple Thread (SIMT) execution model employed by current GPU compute frameworks. Assuming an SIMT execution model, GPU applications tend to send bursts of memory requests that compete for GPU memory resources. Traditionally, hardware units, such as the wavefront scheduler, are used to manage such requests. However, the scheduler struggles when the number of computational operations are too low to effectively hide the long latency of memory operations. In this paper, we propose a Twin Kernel Multiple Thread (TKMT) execution model, a compiler-centric solution that improves hardware scheduling at compile time. TKMT better distributes the burst of memory requests in some of the wavefronts through static instruction scheduling. Our results show that TKMT can offer a 12\% average improvement over the baseline SIMT implementation on a variety of benchmarks on AMD Radeon systems.},
  eventtitle = {2017 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  keywords = {Computational modeling,Graphics processing units,Hardware,Instruction sets,Kernel,Processor scheduling,Scheduling},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/H3XS38M4/gong2017.pdf.pdf}
}

@online{GPUVersionsOther,
  title = {{{GPU Versions}} and {{Other Supplementary Material}}},
  url = {https://asc.llnl.gov/coral-2-benchmarks/gpu-versions-and-other-supplementary-material},
  urldate = {2022-12-18},
  abstract = {GPU Versions and Other Supplementary Material This page collects information about GPU ports and other supplementary material in a single place for the Tier-1 benchmarks. The information here is meant to help vendors better understand how various applications scale and how they have been ported to GPUs in the past. All information and code on this site is provided as is and there is no warranty that it will work as advertised. CORAL 2 Memory Requirement Estimates (.xlsx)},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/AR7KUKFQ/gpu-versions-and-other-supplementary-material.html}
}

@inproceedings{grechStaticAnalysisEnergy2015,
  title = {Static Analysis of Energy Consumption for {{LLVM IR}} Programs},
  booktitle = {Proceedings of the 18th {{International Workshop}} on {{Software}} and {{Compilers}} for {{Embedded Systems}}},
  author = {Grech, Neville and Georgiou, Kyriakos and Pallister, James and Kerrison, Steve and Morse, Jeremy and Eder, Kerstin},
  date = {2015-06},
  eprint = {1405.4565},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {12--21},
  doi = {10.1145/2764967.2764974},
  url = {http://arxiv.org/abs/1405.4565},
  urldate = {2022-12-18},
  abstract = {Energy models can be constructed by characterizing the energy consumed by executing each instruction in a processor's instruction set. This can be used to determine how much energy is required to execute a sequence of assembly instructions, without the need to instrument or measure hardware. However, statically analyzing low-level program structures is hard, and the gap between the high-level program structure and the low-level energy models needs to be bridged. We have developed techniques for performing a static analysis on the intermediate compiler representations of a program. Specifically, we target LLVM IR, a representation used by modern compilers, including Clang. Using these techniques we can automatically infer an estimate of the energy consumed when running a function under different platforms, using different compilers. One of the challenges in doing so is that of determining an energy cost of executing LLVM IR program segments, for which we have developed two different approaches. When this information is used in conjunction with our analysis, we are able to infer energy formulae that characterize the energy consumption for a particular program. This approach can be applied to any languages targeting the LLVM toolchain, including C and XC or architectures such as ARM Cortex-M or XMOS xCORE, with a focus towards embedded platforms. Our techniques are validated on these platforms by comparing the static analysis results to the physical measurements taken from the hardware. Static energy consumption estimation enables energy-aware software development, without requiring hardware knowledge.},
  keywords = {Computer Science - Programming Languages},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/9P3URSAM/Grech и др. - 2015 - Static analysis of energy consumption for LLVM IR .pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/H8V8K5JH/1405.html}
}

@article{hagerNodeLevelPerformanceEngineering,
  title = {Node-{{Level Performance Engineering}}},
  author = {Hager, Georg and Gruber, Thomas and Wellein, Gerhard},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/Y72VZB9B/Hager и др. - Node-Level Performance Engineering.pdf}
}

@inproceedings{haidlPACXXv2RVLLVMbased2017,
  title = {{{PACXXv2}} + {{RV}}: {{An LLVM-based Portable High-Performance Programming Model}}},
  shorttitle = {{{PACXXv2}} + {{RV}}},
  booktitle = {Proceedings of the {{Fourth Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Haidl, Michael and Moll, Simon and Klein, Lars and Sun, Huihui and Hack, Sebastian and Gorlatch, Sergei},
  date = {2017-11-12},
  series = {{{LLVM-HPC}}'17},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3148173.3148185},
  url = {https://doi.org/10.1145/3148173.3148185},
  urldate = {2022-12-18},
  abstract = {To achieve high performance on today's high-performance computing (HPC) systems multiple programming models have to be used. An example for this burden to the developer is OpenCL: the OpenCL's SPMD programming model must be used together with a host programming model, commonly C or C++. Different programming models require different compilers for code generation, which introduce challenges for the software developer, e. g., different compilers must be convinced to agree on basic properties like type layouts to avoid subtle bugs. Moreover, the resulting performance highly depends on the features of the used compilers and may vary unpredictably. We present PACXXv2 -- an LLVM based, single-source, single-compiler programming model which integrates explicitly parallel SPMD programming into C++. Our novel CPU back-end provides portable and predictable performance on various state-of-the-art CPU architectures comprising Intel x86 architectures, IBM Power8 and ARM Cortex CPUs. We efficiently integrate the Region Vectorizer (RV) into our back-end and exploit its whole function vectorization capabilities for our kernels. PACXXv2 utilizes C++ generalized attributes to transparently propagate information about memory allocations to the PACXX back-ends to enable additional optimizations. We demonstrate the high-performance capabilities of PACXXv2 together with RV on benchmarks from well-known benchmark suites and compare the performance of the generated code to Intel's OpenCL driver and POCL -- the portable OpenCL project based on LLVM.},
  isbn = {978-1-4503-5565-0},
  keywords = {C++14,Compilers,LLVM,Performance,Vectorization},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/LLZQZYCK/haidl2017.pdf.pdf}
}

@article{haj-aliLearningVectorizeUsing,
  title = {Learning to {{Vectorize Using Deep Reinforcement Learning}}},
  author = {Haj-Ali, Ameer and Ahmed, Nesreen K and Willke, Ted and Shao, Sophia and Asanovic, Krste and Stoica, Ion},
  abstract = {We explore a novel approach for handling loop vectorization automatically and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and access patterns to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors when compared against the currently used fixed-cost models that rely on heuristics. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. The learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops.We evaluate our approach against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29 × −4.73× performance speedup compared to baseline and only 3\% worse than the brute-force search on a wide range of benchmarks.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/3CYGC76P/Haj-Ali и др. - Learning to Vectorize Using Deep Reinforcement Lea.pdf}
}

@inproceedings{haj-aliNeuroVectorizerEndtoendVectorization2020,
  title = {{{NeuroVectorizer}}: End-to-End Vectorization with Deep Reinforcement Learning},
  shorttitle = {{{NeuroVectorizer}}},
  booktitle = {Proceedings of the 18th {{ACM}}/{{IEEE International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {Haj-Ali, Ameer and Ahmed, Nesreen K. and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion},
  date = {2020-02-22},
  pages = {242--255},
  publisher = {{ACM}},
  location = {{San Diego CA USA}},
  doi = {10.1145/3368826.3377928},
  url = {https://dl.acm.org/doi/10.1145/3368826.3377928},
  urldate = {2022-12-17},
  abstract = {One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time. In this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which ∗Part of this work was done while Ameer Haj-Ali was in a summer internship at Intel Labs.},
  eventtitle = {{{CGO}} '20: 18th {{ACM}}/{{IEEE International Symposium}} on {{Code Generation}} and {{Optimization}}},
  isbn = {978-1-4503-7047-9},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/TVD9GYDW/Haj-Ali и др. - 2020 - NeuroVectorizer end-to-end vectorization with dee.pdf}
}

@incollection{hammerKerncraftToolAnalytic2017,
  title = {Kerncraft: {{A Tool}} for {{Analytic Performance Modeling}} of {{Loop Kernels}}},
  shorttitle = {Kerncraft},
  author = {Hammer, Julian and Eitzinger, Jan and Hager, Georg and Wellein, Gerhard},
  date = {2017},
  eprint = {1702.04653},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--22},
  doi = {10.1007/978-3-319-56702-0_1},
  url = {http://arxiv.org/abs/1702.04653},
  urldate = {2022-12-31},
  abstract = {Achieving optimal program performance requires deep insight into the interaction between hardware and software. For software developers without an in-depth background in computer architecture, understanding and fully utilizing modern architectures is close to impossible. Analytic loop performance modeling is a useful way to understand the relevant bottlenecks of code execution based on simple machine models. The Roofline Model and the Execution-Cache-Memory (ECM) model are proven approaches to performance modeling of loop nests. In comparison to the Roofline model, the ECM model can also describes the single-core performance and saturation behavior on a multicore chip. We give an introduction to the Roofline and ECM models, and to stencil performance modeling using layer conditions (LC). We then present Kerncraft, a tool that can automatically construct Roofline and ECM models for loop nests by performing the required code, data transfer, and LC analysis. The layer condition analysis allows to predict optimal spatial blocking factors for loop nests. Together with the models it enables an ab-initio estimate of the potential benefits of loop blocking optimizations and of useful block sizes. In cases where LC analysis is not easily possible, Kerncraft supports a cache simulator as a fallback option. Using a 25-point long-range stencil we demonstrate the usefulness and predictive power of the Kerncraft tool.},
  keywords = {Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/Y8TWPJ53/Hammer и др. - 2017 - Kerncraft A Tool for Analytic Performance Modelin.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/24IEU79R/1702.html}
}

@article{hofmannBridgingArchitectureGap2020,
  title = {Bridging the {{Architecture Gap}}: {{Abstracting Performance-Relevant Properties}} of {{Modern Server Processors}}},
  shorttitle = {Bridging the {{Architecture Gap}}},
  author = {Hofmann, Johannes and Alappat, Christie L. and Hager, Georg and Fey, Dietmar and Wellein, Gerhard},
  date = {2020-07-11},
  journaltitle = {Supercomputing Frontiers and Innovations},
  volume = {7},
  number = {2},
  pages = {54--78},
  issn = {2313-8734},
  doi = {10.14529/jsfi200204},
  url = {https://superfri.org/index.php/superfri/article/view/310},
  urldate = {2022-12-31},
  abstract = {We propose several improvements to the execution-cache-memory (ECM) model, an analytic performance model for predicting single- and multicore runtime of steady-state loops on server processors. The model is made more general by strictly differentiating between application and machine models: an application model comprises the loop code, problem sizes, and other runtime parameters, while a machine model is an abstraction of all performance-relevant properties of a processor. Moreover, new first principles underlying the model’s estimates are derived from common microarchitectural features implemented by today’s server processors to make the model more architecture independent, thereby extending its applicability beyond Intel processors. We introduce a generic method for determining machine models, and present results for relevant server-processor architectures by Intel, AMD, IBM, and Marvell/Cavium. Considering this wide range of architectures, the set of features required for adequate performance modeling is surprisingly small. To validate our approach, we compare performance predictions to empirical data for an OpenMP-parallel preconditioned CG algorithm, which includes compute- and memory-bound kernels. Both single- and multicore analysis shows that the model exhibits average and maximum relative errors of 5 \% and 10 \%. Deviations from the model and insights gained are discussed in detail.},
  issue = {2},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/R8WTTIYE/Hofmann и др. - 2020 - Bridging the Architecture Gap Abstracting Perform.pdf}
}

@online{Ieeemicro22Pdf,
  title = {Ieeemicro22.Pdf},
  url = {https://drive.google.com/file/d/1LdNGJfEJ1AZN8-th-iZ-9NHh1M3G2eUT/view?usp=embed_facebook},
  urldate = {2022-12-17},
  organization = {{Google Docs}},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/TJXF4YDN/view.html}
}

@article{Intel64IA32,
  title = {{{Intel}}® 64 and {{IA-32 Architectures Software Developer}}’s {{Manual}}, {{Volume 3A}}: {{System Programming Guide}}, {{Part}} 1},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/QYB2E2QR/Intel® 64 and IA-32 Architectures Software Develop.pdf}
}

@article{Intel64IA322022,
  title = {{{Intel}}® 64 and {{IA-32 Architectures Software Developer}}’s {{Manual}}, {{Combined Volumes}}: 1, {{2A}}, {{2B}}, {{2C}}, {{2D}}, {{3A}}, {{3B}}, {{3C}}, {{3D}}, and 4},
  date = {2022},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/6TDBTAY2/2022 - Intel® 64 and IA-32 Architectures Software Develop.pdf}
}

@article{Intel64IA32a,
  title = {{{Intel}}® 64 and {{IA-32 Architectures Software Developer}}’s {{Manual}}, {{Volume}} 3 ({{3A}}, {{3B}}, {{3C}} \& {{3D}}): {{System Programming Guide}}},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/HWTPU9AN/Intel® 64 and IA-32 Architectures Software Develop.pdf}
}

@online{IntroductionSVE,
  title = {Introduction to {{SVE}}},
  url = {https://developer.arm.com/documentation/102476/0100},
  urldate = {2022-12-18},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/SCGZUCPT/0100.html}
}

@online{islamRVCoreP32IMEffectiveArchitecture2020,
  title = {{{RVCoreP-32IM}}: {{An}} Effective Architecture to Implement Mul/Div Instructions for Five Stage {{RISC-V}} Soft Processors},
  shorttitle = {{{RVCoreP-32IM}}},
  author = {Islam, Md Ashraful and Miyazaki, Hiromu and Kise, Kenji},
  date = {2020-10-30},
  eprint = {2010.16171},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.16171},
  urldate = {2023-03-02},
  abstract = {RISC-V, an open instruction set architecture, is getting the attention of soft processor developers. Implementing only a basic 32-bit integer instruction set of RISC-V, which is defined as RV32I, might be satisfactory for embedded systems. However, multiplication and division instructions are not present in RV32I, rather than defined as M-extension. Several research projects have proposed both RV32I and RV32IM processor. However, there is no indication of how much performance can be improved by adding M-extension to RV32I. In other words, when we should consider adding M-extension into the soft processor and how much hardware resource requirements will increase. In this paper, we propose an extension of the RVCoreP soft processor (which implements RV32I instruction set only) to support RISC-V M-extension instructions. A simple fork-join method is used to expand the execution capability to support M-extension instructions as well as a possible future enhancement. We then perform the benchmark using Dhrystone, Coremark, and Embench programs. We found that RV32IM is 1.87 and 3.13 times better in performance for radix-4 and DSP multiplier, respectively. In addition to that, our RV32IM implementation is 13\textbackslash\% better than the equivalent RISC-V processor.},
  pubstate = {preprint},
  keywords = {bpu,Computer Science - Hardware Architecture,hw,riscv},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/EUQHTW2K/Islam и др. - 2020 - RVCoreP-32IM An effective architecture to impleme.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/QNIYQQ3R/2010.html}
}

@online{jiaDissectingNVidiaTuring2019,
  title = {Dissecting the {{NVidia Turing T4 GPU}} via {{Microbenchmarking}}},
  author = {Jia, Zhe and Maggioni, Marco and Smith, Jeffrey and Scarpazza, Daniele Paolo},
  date = {2019-03-18},
  eprint = {1903.07486},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1903.07486},
  urldate = {2022-12-18},
  abstract = {In 2019, the rapid rate at which GPU manufacturers refresh their designs, coupled with their reluctance to disclose microarchitectural details, is still a hurdle for those software designers who want to extract the highest possible performance. Last year, these very reasons motivated us to dissect the Volta GPU architecture using microbenchmarks. The introduction in August 2018 of Turing, NVidia's latest architecture, pressed us to update our study. In this report, we examine Turing and compare it quantitatively against previous NVidia GPU generations. Specifically, we study the T4 GPU: a low-power board aiming at inference applications. We describe its improvements against its inference-oriented predecessor: the P4 GPU based on the Pascal architecture. Both T4 and P4 GPUs achieve significantly higher frequency-per-Watt figures than their full-size counterparts. We study the performance of the T4's TensorCores, finding a much higher throughput on low-precision operands than on the P4 GPU. We reveal that Turing introduces new instructions that express matrix math more succinctly. We map Turing's instruction space, finding the same encoding as Volta, and additional instructions. We reveal that the Turing TU104 chip has the same memory hierarchy depth as the Volta GV100; cache levels sizes on the TU104 are frequently twice as large as those found on the Pascal GP104. We benchmark each constituent of the T4 memory hierarchy and find substantial overall performance improvements over its P4 predecessor. We studied how clock throttling affects compute-intensive workloads that hit power or thermal limits. Many of our findings are novel, published here for the first time. All of them can guide high-performance software developers get closer to the GPU's peak performance.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/KCHJ2ZFR/Jia и др. - 2019 - Dissecting the NVidia Turing T4 GPU via Microbench.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/S94Q39UH/1903.html}
}

@online{jiaDissectingNVIDIAVolta2018,
  title = {Dissecting the {{NVIDIA Volta GPU Architecture}} via {{Microbenchmarking}}},
  author = {Jia, Zhe and Maggioni, Marco and Staiger, Benjamin and Scarpazza, Daniele P.},
  date = {2018-04-18},
  eprint = {1804.06826},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1804.06826},
  urldate = {2022-12-18},
  abstract = {Every year, novel NVIDIA GPU designs are introduced. This rapid architectural and technological progression, coupled with a reluctance by manufacturers to disclose low-level details, makes it difficult for even the most proficient GPU software designers to remain up-to-date with the technological advances at a microarchitectural level. To address this dearth of public, microarchitectural-level information on the novel NVIDIA GPUs, independent researchers have resorted to microbenchmarks-based dissection and discovery. This has led to a prolific line of publications that shed light on instruction encoding, and memory hierarchy's geometry and features at each level. Namely, research that describes the performance and behavior of the Kepler, Maxwell and Pascal architectures. In this technical report, we continue this line of research by presenting the microarchitectural details of the NVIDIA Volta architecture, discovered through microbenchmarks and instruction set disassembly. Additionally, we compare quantitatively our Volta findings against its predecessors, Kepler, Maxwell and Pascal.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/FIHXT2YX/Jia и др. - 2018 - Dissecting the NVIDIA Volta GPU Architecture via M.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/I7SWKIKD/1804.html}
}

@online{johnsenTemporalVectorizationCompiler2022,
  title = {Temporal {{Vectorization}}: {{A Compiler Approach}} to {{Automatic Multi-Pumping}}},
  shorttitle = {Temporal {{Vectorization}}},
  author = {Johnsen, Carl-Johannes and De Matteis, Tiziano and Ben-Nun, Tal and Licht, Johannes de Fine and Hoefler, Torsten},
  date = {2022-09-19},
  eprint = {2210.04598},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.04598},
  urldate = {2022-12-17},
  abstract = {The multi-pumping resource sharing technique can overcome the limitations commonly found in single-clocked FPGA designs by allowing hardware components to operate at a higher clock frequency than the surrounding system. However, this optimization cannot be expressed in high levels of abstraction, such as HLS, requiring the use of hand-optimized RTL. In this paper we show how to leverage multiple clock domains for computational subdomains on reconfigurable devices through data movement analysis on high-level programs. We offer a novel view on multi-pumping as a compiler optimization - a superclass of traditional vectorization. As multiple data elements are fed and consumed, the computations are packed temporally rather than spatially. The optimization is applied automatically using an intermediate representation that maps high-level code to HLS. Internally, the optimization injects modules into the generated designs, incorporating RTL for fine-grained control over the clock domains. We obtain a reduction of resource consumption by up to 50\% on critical components and 23\% on average. For scalable designs, this can enable further parallelism, increasing overall performance.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ENICATCB/Johnsen и др. - 2022 - Temporal Vectorization A Compiler Approach to Aut.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/7TFVDQTV/2210.html}
}

@book{jordansHighlevelSoftwarepipeliningLLVM2015,
  title = {High-Level Software-Pipelining in {{LLVM}}},
  author = {Jordans, Roel and Corporaal, Henk},
  date = {2015-06-01},
  doi = {10.1145/2764967.2771935},
  abstract = {Software-pipelining is an important technique for increasing the instruction level parallelism of loops during compilation. Currently, the LLVM compiler infrastructure does not offer this optimization although some target specific implementations do exist. We have implemented a high-level method for software-pipelining within the LLVM framework. By implementing this within LLVM's optimization layer we have taken the first steps towards a target independent software-pipelining method.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/IRKIFTRM/Jordans и Corporaal - 2015 - High-level software-pipelining in LLVM.pdf}
}

@inproceedings{juanOptimizingSIMDParallel2012,
  title = {Optimizing {{SIMD Parallel Computation}} with {{Non-Consecutive Array Access}} in {{Inline SSE Assembly Language}}},
  booktitle = {2012 {{Fifth International Conference}} on {{Intelligent Computation Technology}} and {{Automation}}},
  author = {Juan, Chen and Canqun, Yang},
  date = {2012-01},
  pages = {254--257},
  doi = {10.1109/ICICTA.2012.70},
  abstract = {Many processors, such as Intel Xeon processor 5100 series, AMD Athlon 64, support SIMD computation model with the Streaming SIMD Extensions (SSE), SSE2 and SSE3. Using double-precision SSE/SSE2/SSE3 instructions simultaneously can handle two packed double-precision floating-point data elements with 128-bit XMM vector registers, which greatly improves floating-point performance. Sometimes non-consecutive data instead of consecutive ones appear in SIMD computation, which prevents SIMD optimization. That is because two non-consecutive double precision floating-point data elements cannot be loaded into 128-bit vector registers simultaneously and they have to be loaded for twice. How to implement SIMD optimization for non-consecutive data is our concern. Loop unrolling exposes the rule and characteristics of such non-consecutive data. Register rotation can help transform non-consecutive data to vector data. Based on a representative kernel program, we illustrate our SIMD optimization combining loop unrolling with register rotation. Through vectorizing non-consecutive data, the performance of "KERNEL" code is improved by 42.4\% and PQMRCGSTAB application is improved by 15.3\%.},
  eventtitle = {2012 {{Fifth International Conference}} on {{Intelligent Computation Technology}} and {{Automation}}},
  keywords = {Arrays,Assembly,inline assembly,Kernel,loop unrolling,nonconsecutive data,Optimization,Program processors,register rotation,Registers,SIMD,SSE/SSE2/SSE3,Vectors},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/K6Q6HMFD/juan2012.pdf.pdf}
}

@online{kanamoriRVCoreP32ICHighperformanceRISCV2020,
  title = {{{RVCoreP-32IC}}: {{A}} High-Performance {{RISC-V}} Soft Processor with an Efficient Fetch Unit Supporting the Compressed Instructions},
  shorttitle = {{{RVCoreP-32IC}}},
  author = {Kanamori, Takuto and Miyazaki, Hiromu and Kise, Kenji},
  date = {2020-11-23},
  eprint = {2011.11246},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.11246},
  urldate = {2023-03-02},
  abstract = {In this paper, we propose a high-performance RISC-V soft processor with an efficient fetch unit supporting the compressed instructions targeting on FPGA. The compressed instruction extension in RISC-V can reduce the program size by about 25\%. But it needs a complicated logic for the instruction fetch unit and has a significant impact on performance. We propose an instruction fetch unit that supports the compressed instructions while exhibiting high performance. Furthermore, we propose a RISC-V soft processor using this unit. We implement this proposed processor in Verilog HDL and verify the behavior using Verilog simulation and an actual Xilinx Atrix-7 FPGA board. We compare the results of some benchmarks and the amount of hardware with related works. DMIPS, CoreMark value, and Embench value of the proposed processor achieved 42.5\%, 41.1\% and 21.3\% higher performance than the related work, respectively.},
  pubstate = {preprint},
  keywords = {bpu,Computer Science - Hardware Architecture,hw,riscv,softcore},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/B7833BYT/Kanamori и др. - 2020 - RVCoreP-32IC A high-performance RISC-V soft proce.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/B2Q9ULIL/2011.html}
}

@article{kaufmanLearnedTPUCost,
  title = {Learned {{TPU Cost Model}} for {{XLA Tensor Programs}}},
  author = {Kaufman, Samuel J and Phothilimthana, Phitchaya Mangpo and Burrows, Mike},
  abstract = {At Google, we would like to develop a cost model that can accurately estimate the execution time of a machine learning model running on a Tensor Processing Unit (TPU). This cost model can be used by a compiler to make heuristic decisions, by an autotuner to find an optimal configuration of a specific program, and by Neural Architecture Search to co-optimize accuracy and inference time. However, building an accurate analytical cost model is challenging because of the complexity of modern processors.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/9CXGMZ9F/Kaufman и др. - Learned TPU Cost Model for XLA Tensor Programs.pdf}
}

@incollection{kayrakliogluLocalityBasedOptimizationsChapel2022,
  title = {Locality-{{Based Optimizations}} in the {{Chapel Compiler}}},
  booktitle = {Languages and {{Compilers}} for {{Parallel Computing}}},
  author = {Kayraklioglu, Engin and Ronaghan, Elliot and Ferguson, Michael P. and Chamberlain, Bradford L.},
  editor = {Li, Xiaoming and Chandrasekaran, Sunita},
  date = {2022},
  volume = {13181},
  pages = {3--17},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-99372-6_1},
  url = {https://link.springer.com/10.1007/978-3-030-99372-6_1},
  urldate = {2022-12-17},
  abstract = {One of the main challenges of distributed memory programming is achieving efficient access to data. Low-level programming paradigms such as MPI and SHMEM require programmers to explicitly move data between compute nodes, which typically results in good execution performance at the expense of programmer productivity. Highlevel paradigms such as the Chapel programming language aim to reduce programming difficulty by supporting a global memory view. However, implicit communication afforded by the global memory view can make it easier for the programmers to overlook performance considerations.},
  isbn = {978-3-030-99371-9 978-3-030-99372-6},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/8YNUXMXH/Kayraklioglu и др. - 2022 - Locality-Based Optimizations in the Chapel Compile.pdf}
}

@article{killianPerformanceImprovementKernels,
  title = {Performance {{Improvement}} in {{Kernels}} by {{Guiding Compiler Auto-Vectorization Heuristics}}},
  author = {Killian, William and Miceli, Renato and Park, EunJung and Vega, Marco Alvarez and Cavazos, John},
  abstract = {Vectorization support in hardware continues to expand and grow as we still continue on superscalar architectures. Unfortunately, compilers are not always able to generate optimal code for the hardware; detecting and generating vectorized code is extremely complex. Programmers can use a number of tools to aid in development and tuning, but most of these tools require expert or domain-specific knowledge to use. In this work we aim to provide techniques for determining the best way to optimize certain codes, with an end goal of guiding the compiler into generating optimized code without requiring expert knowledge from the developer. Initially, we study how to combine vectorization reports with iterative compilation and code generation and summarize our insights and patterns on how the compiler vectorizes code. Our utilities for iterative compilation and code generation can be further used by non-experts in the generation and analysis of programs. Finally, we leverage the obtained knowledge to design a Support Vector Machine classifier to predict the speedup of a program given a sequence of optimization. We show that our classifier is able to predict the speedup of 56\% of the inputs within 15\% overprediction and 50\% underprediction, with 82\% of these accurate within 15\% both ways.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/VUMF675C/Killian и др. - Performance Improvement in Kernels by Guiding Comp.pdf}
}

@incollection{klausCompilingLinearAlgebra2022,
  title = {Compiling {{Linear Algebra Expressions}} into {{Efficient Code}}},
  booktitle = {Computational {{Science}} – {{ICCS}} 2022},
  author = {Klaus, Julien and Blacher, Mark and Giesen, Joachim and Rump, Paul Gerhardt and Wiedom, Konstantin},
  editor = {Groen, Derek and family=Mulatier, given=Clélia, prefix=de, useprefix=true and Paszynski, Maciej and Krzhizhanovskaya, Valeria V. and Dongarra, Jack J. and Sloot, Peter M. A.},
  date = {2022},
  volume = {13351},
  pages = {11--17},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-08754-7_2},
  url = {https://link.springer.com/10.1007/978-3-031-08754-7_2},
  urldate = {2022-12-17},
  abstract = {In textbooks, linear algebra expressions often use indices to specify the elements of variables. This index form expressions cannot be directly translated into efficient code, since optimized linear algebra libraries and frameworks require expressions in index-free form. To address this problem, we developed Lina, a tool that automatically converts linear algebra expressions with indices into index-free linear algebra expressions that we map efficiently to NumPy and Eigen code.},
  isbn = {978-3-031-08753-0 978-3-031-08754-7},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/TT94LVGG/Klaus и др. - 2022 - Compiling Linear Algebra Expressions into Efficien.pdf}
}

@inproceedings{klemmerRVVRadarFrameworkSupporting2022,
  title = {{{RVVRadar}}: {{A Framework}} for {{Supporting}} the {{Programmer}} in {{Vectorization}} for {{RISC-V}}},
  shorttitle = {{{RVVRadar}}},
  booktitle = {Proceedings of the {{Great Lakes Symposium}} on {{VLSI}} 2022},
  author = {Klemmer, Lucas and Schlägl, Manfred and Große, Daniel},
  date = {2022-06-06},
  pages = {183--187},
  publisher = {{ACM}},
  location = {{Irvine CA USA}},
  doi = {10.1145/3526241.3530388},
  url = {https://dl.acm.org/doi/10.1145/3526241.3530388},
  urldate = {2022-12-17},
  abstract = {In this paper, we present RVVRadar, a framework to support the programmer over the four major steps of development, verification, measurement, and evaluation during the vectorization process of an algorithm. We demonstrate the advantages of RVVRadar for vectorization on several practical relevant algorithms. This includes in particular the widely-used libpng library where we vectorized all filter computations resulting in speedups of up to 5.43. We made RVVRadar as well as all benchmarks (including the RVV-based libpng) open source.},
  eventtitle = {{{GLSVLSI}} '22: {{Great Lakes Symposium}} on {{VLSI}} 2022},
  isbn = {978-1-4503-9322-5},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/INNTPHS4/Klemmer и др. - 2022 - RVVRadar A Framework for Supporting the Programme.pdf}
}

@inproceedings{kosmalaOnlineHandwrittenFormula1999,
  title = {On-Line Handwritten Formula Recognition Using Hidden {{Markov}} Models and Context Dependent Graph Grammars},
  booktitle = {Proceedings of the {{Fifth International Conference}} on {{Document Analysis}} and {{Recognition}}. {{ICDAR}} '99 ({{Cat}}. {{No}}.{{PR00318}})},
  author = {Kosmala, A. and Rigoll, G. and Lavirotte, S. and Pottier, L.},
  date = {1999},
  pages = {107--110},
  publisher = {{IEEE}},
  location = {{Bangalore, India}},
  doi = {10.1109/ICDAR.1999.791736},
  url = {http://ieeexplore.ieee.org/document/791736/},
  urldate = {2023-03-06},
  abstract = {This paper presents an approach for the recognition of on-line handwritten mathematical expressions. The Hidden Markov Model (HMM) based system makes use of simultaneous segmentation and recognition capabilities, avoiding a crucial segmentation during pre-processing. With the segmentation and recognition results, obtained from the HMMrecognizer, it is possible to analyze and interpret the spatial two-dimensional arrangement of the symbols. We use a graph grammar approach for the structure recognition, also used in off-line recognition process, resulting in a general tree-structure of the underlying input-expression. The resulting constructed tree can be translated to any desired syntax (for example: Lisp, LATEX, OpenMath . . . ).},
  eventtitle = {Proceedings of the {{Fifth International Conference}} on {{Document Analysis}} and {{Recognition}}. {{ICDAR}} '99 ({{Cat}}. {{No}}.{{PR00318}})},
  isbn = {978-0-7695-0318-9},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/HWR7RKGB/Kosmala и др. - 1999 - On-line handwritten formula recognition using hidd.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/PV2AHTYZ/kosmala1999.pdf.pdf}
}

@book{kovacIntelligentCompilerOptimization2022,
  title = {Towards Intelligent Compiler Optimization},
  author = {Kovač, Mihael and Brcic, Mario and Krajna, Agneza and Krleza, Dalibor},
  date = {2022-05-26},
  doi = {10.23919/MIPRO55190.2022.9803630},
  abstract = {The future of computation is massively parallel and heterogeneous with specialized accelerator devices and instruction sets in both edge-and cluster-computing. However, software development is bound to become the bottleneck. To extract the potential of hardware wonders, the software would have to solve the following problems: heterogeneous device mapping, capability discovery, parallelization, adaptation to new ISAs, and many others. This systematic complexity will be impossible to manually tame for human developers. These problems need to be offloaded to intelligent compilers. In this paper, we present the current research that utilizes deep learning, polyhedral optimization, reinforcement learning, etc. We envision the future of compilers as consisting of empirical testing, automatic statistics collection, continual learning, device capability discovery, multiphase compiling-precompiling and JIT tuning, and classification of workloads. We devise a simple classification experiment to demonstrate the power of simple graph neural networks (GNNs) paired with program graphs. The test performance demonstrates the effectiveness and representational appro-priateness of GNNs for compiler optimizations in heterogeneous systems. The benefits of intelligent compilers are time savings for the economy, energy savings for the environment, and greater democratization of software development.}
}

@inproceedings{laukemannAutomatedInstructionStream2018,
  title = {Automated {{Instruction Stream Throughput Prediction}} for {{Intel}} and {{AMD Microarchitectures}}},
  booktitle = {2018 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  author = {Laukemann, Jan and Hammer, Julian and Hofmann, Johannes and Hager, Georg and Wellein, Gerhard},
  date = {2018-11},
  eprint = {1809.00912},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {121--131},
  doi = {10.1109/PMBS.2018.8641578},
  url = {http://arxiv.org/abs/1809.00912},
  urldate = {2022-12-31},
  abstract = {An accurate prediction of scheduling and execution of instruction streams is a necessary prerequisite for predicting the in-core performance behavior of throughput-bound loop kernels on out-of-order processor architectures. Such predictions are an indispensable component of analytical performance models, such as the Roofline and the Execution-Cache-Memory (ECM) model, and allow a deep understanding of the performance-relevant interactions between hardware architecture and loop code. We present the Open Source Architecture Code Analyzer (OSACA), a static analysis tool for predicting the execution time of sequential loops comprising x86 instructions under the assumption of an infinite first-level cache and perfect out-of-order scheduling. We show the process of building a machine model from available documentation and semi-automatic benchmarking, and carry it out for the latest Intel Skylake and AMD Zen micro-architectures. To validate the constructed models, we apply them to several assembly kernels and compare runtime predictions with actual measurements. Finally we give an outlook on how the method may be generalized to new architectures.},
  keywords = {Computer Science - Performance,Computer Science - Software Engineering},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/CIB38SJH/Laukemann и др. - 2018 - Automated Instruction Stream Throughput Prediction.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/ZJ39SQCY/laukemann2018.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/NN29G4JE/1809.html}
}

@inproceedings{laukemannAutomaticThroughputCritical2019,
  title = {Automatic {{Throughput}} and {{Critical Path Analysis}} of X86 and {{ARM Assembly Kernels}}},
  booktitle = {2019 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  author = {Laukemann, Jan and Hammer, Julian and Hager, Georg and Wellein, Gerhard},
  date = {2019-11},
  pages = {1--6},
  doi = {10.1109/PMBS49563.2019.00006},
  abstract = {Useful models of loop kernel runtimes on out-of-order architectures require an analysis of the in-core performance behavior of instructions and their dependencies. While an instruction throughput prediction sets a lower bound to the kernel runtime, the critical path defines an upper bound. Such predictions are an essential part of analytic (i.e., white-box) performance models like the Roofline and Execution-Cache-Memory (ECM) models. They enable a better understanding of the performance-relevant interactions between hardware architecture and loop code. The Open Source Architecture Code Analyzer (OSACA) is a static analysis tool for predicting the execution time of sequential loops. It previously supported only x86 (Intel and AMD) architectures and simple, optimistic full-throughput execution. We have heavily extended OSACA to support ARM instructions and critical path prediction including the detection of loop-carried dependencies, which turns it into a versatile cross-architecture modeling tool. We show runtime predictions for code on Intel Cascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on machine models from available documentation and semi-automatic benchmarking. The predictions are compared with actual measurements.},
  eventtitle = {2019 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  keywords = {Analytical models,architecture analysis,benchmarking,Computer architecture,Computer Science - Performance,Kernel,Load modeling,performance engineering,performance modeling,Predictive models,static analysis,Throughput,Tools},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/UVCUYJQB/Laukemann и др. - 2019 - Automatic Throughput and Critical Path Analysis of.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/VYK7DU8I/laukemann2019.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/99RHU7ZY/1910.html}
}

@inproceedings{laukemannAutomaticThroughputCritical2019a,
  title = {Automatic {{Throughput}} and {{Critical Path Analysis}} of X86 and {{ARM Assembly Kernels}}},
  booktitle = {2019 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  author = {Laukemann, Jan and Hammer, Julian and Hager, Georg and Wellein, Gerhard},
  date = {2019-11},
  pages = {1--6},
  doi = {10.1109/PMBS49563.2019.00006},
  abstract = {Useful models of loop kernel runtimes on out-of-order architectures require an analysis of the in-core performance behavior of instructions and their dependencies. While an instruction throughput prediction sets a lower bound to the kernel runtime, the critical path defines an upper bound. Such predictions are an essential part of analytic (i.e., white-box) performance models like the Roofline and Execution-Cache-Memory (ECM) models. They enable a better understanding of the performance-relevant interactions between hardware architecture and loop code. The Open Source Architecture Code Analyzer (OSACA) is a static analysis tool for predicting the execution time of sequential loops. It previously supported only x86 (Intel and AMD) architectures and simple, optimistic full-throughput execution. We have heavily extended OSACA to support ARM instructions and critical path prediction including the detection of loop-carried dependencies, which turns it into a versatile cross-architecture modeling tool. We show runtime predictions for code on Intel Cascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on machine models from available documentation and semi-automatic benchmarking. The predictions are compared with actual measurements.},
  eventtitle = {2019 {{IEEE}}/{{ACM Performance Modeling}}, {{Benchmarking}} and {{Simulation}} of {{High Performance Computer Systems}} ({{PMBS}})},
  keywords = {Analytical models,architecture analysis,benchmarking,Computer architecture,Kernel,Load modeling,performance engineering,performance modeling,Predictive models,static analysis,Throughput,Tools},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ENG9AV42/Laukemann и др. - 2019 - Automatic Throughput and Critical Path Analysis of.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/SLVJI5K5/laukemann2019.pdf.pdf}
}

@article{LearnArchitectureA642022,
  title = {Learn the Architecture - {{A64 Instruction Set Architecture}}},
  date = {2022},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ISCNHUQ8/2022 - Learn the architecture - A64 Instruction Set Archi.pdf}
}

@online{LearnArchitectureIntroducing,
  title = {Learn the Architecture - {{Introducing Neon}}},
  url = {https://developer.arm.com/documentation/102474/latest},
  urldate = {2022-12-18}
}

@article{LearnArchitectureIntroducing2020,
  title = {Learn the Architecture - {{Introducing Neon}}},
  date = {2020},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/RYEC62K2/2020 - Learn the architecture - Introducing Neon.pdf}
}

@article{LearnArchitectureUnderstanding2019,
  title = {Learn the Architecture - {{Understanding}} the {{Armv8}}.x and {{Armv9}}.x Extensions},
  date = {2019},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/L4SJD32F/2019 - Learn the architecture - Understanding the Armv8.x.pdf}
}

@article{leeWhenPrefetchingWorks2012,
  title = {When {{Prefetching Works}}, {{When It Doesn}}’t, and {{Why}}},
  author = {Lee, Jaekyu and Kim, Hyesoon and Vuduc, Richard},
  date = {2012-03},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  volume = {9},
  number = {1},
  pages = {1--29},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/2133382.2133384},
  url = {https://dl.acm.org/doi/10.1145/2133382.2133384},
  urldate = {2022-12-17},
  abstract = {In emerging and future high-end processor systems, tolerating increasing cache miss latency and properly managing memory bandwidth will be critical to achieving high performance. Prefetching, in both hardware and software, is among our most important available techniques for doing so; yet, we claim that prefetching is perhaps also the least well-understood.             Thus, the goal of this study is to develop a novel, foundational understanding of both the benefits and limitations of hardware and software prefetching. Our study includes: source code-level analysis, to help in understanding the practical strengths and weaknesses of compiler- and software-based prefetching; a study of the synergistic and antagonistic effects between software and hardware prefetching; and an evaluation of hardware prefetching training policies in the presence of software prefetching requests. We use both simulation and measurement on real systems. We find, for instance, that although there are many opportunities for compilers to prefetch much more aggressively than they currently do, there is also a tangible risk of interference with training existing hardware prefetching mechanisms. Taken together, our observations suggest new research directions for cooperative hardware/software prefetching.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ZDT9H2IE/Lee и др. - 2012 - When Prefetching Works, When It Doesn’t, and Why.pdf}
}

@online{liEfficientVectorizationScheme2021,
  title = {An {{Efficient Vectorization Scheme}} for {{Stencil Computation}}},
  author = {Li, Kun and Yuan, Liang and Zhang, Yunquan and Yue, Yue and Cao, Hang and Lu, Pengqi},
  date = {2021-03-17},
  eprint = {2103.08825},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.08825},
  urldate = {2022-12-17},
  abstract = {Stencil computation is one of the most important kernels in various scientific and engineering applications. A variety of work has focused on vectorization and tiling techniques, aiming at exploiting the in-core data parallelism and data locality respectively. In this paper, the downsides of existing vectorization schemes are analyzed. Briefly, they either incur data alignment conflicts or hurt the data locality when integrated with tiling. Then we propose a novel transpose layout to preserve the data locality for tiling and reduce the data reorganization overhead for vectorization simultaneously. To further improve the data reuse at the register level, a time loop unroll-and-jam strategy is designed to perform multistep stencil computation along the time dimension. Experimental results on the AVX-2 and AVX-512 CPUs show that our approach obtains a competitive performance.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/A4TMBG4U/Li и др. - 2021 - An Efficient Vectorization Scheme for Stencil Comp.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/V7XW58EY/2103.html}
}

@article{liuCombiningRuntimeChecks,
  title = {Combining {{Run-time Checks}} and {{Compile-time Analysis}} to {{Improve Control Flow Auto-Vectorization}}},
  author = {Liu, Bangtian and Laird, Avery and Tsang, Wai Hung and Mahjour, Bardia and Dehnavi, Maryam Mehri},
  abstract = {SIMD (Single Instruction Multiple Data) instructions apply the same operation to multiple elements simultaneously. Compilers transform codes to exploit SIMD instructions through auto-vectorization. Control flow can lead to challenges for auto-vectorization tools because compilers conservatively assume branches are divergent. However, it is common that all SIMD lanes follow the same controlpath at run-time, a property we call dynamic uniformity. In this paper, we present VecRC (an auto-vectorizer with run-time checks), a novel compile-time technique that uses run-time checks to test for dynamically uniform control flows. Under the assumption of dynamic uniformity, we perform several compile-time analyses that improve control flow auto-vectorization vs state-of-the-art approaches. VecRC leverages dynamic uniformity to vectorize loops with control-dependent loop-carried dependences. Existing strategies use speculation to optimistically execute vector code, and must correct any incorrect computation due to violated run-time assumptions. VecRC performs compile-time analysis based on uniformity to support such dependences without the overhead of speculation. We propose a probability-based cost model to predict the profitability of run-time checks to avoid the specialized profiling or expensive auto-tuning required in existing methods. VecRC is evaluated in LLVM on a diverse range of benchmarks including SPEC2017, NPB, Parboil, TSVC, and Rodinia on Intel Skylake and IBM Power 9 architectures. On the Skylake architecture, geometric mean speedups of 1.31x, 1.20x, 1.19x, and 1.06x over Region Vectorizer, GCC, Clang, and ICC are obtained with VecRC on real benchmark code.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/H85JZTLR/Liu и др. - Combining Run-time Checks and Compile-time Analysi.pdf}
}

@inproceedings{lozanoConstraintBasedRegisterAllocation2012,
  title = {Constraint-{{Based Register Allocation}} and {{Instruction Scheduling}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}}},
  author = {Lozano, Roberto Castañeda and Carlsson, Mats and Drejhammar, Frej and Schulte, Christian},
  editor = {Milano, Michela},
  date = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {750--766},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-33558-7_54},
  abstract = {This paper introduces a constraint model and solving techniques for code generation in a compiler back-end. It contributes a new model for global register allocation that combines several advanced aspects: multiple register banks (subsuming spilling to memory), coalescing, and packing. The model is extended to include instruction scheduling and bundling. The paper introduces a decomposition scheme exploiting the underlying program structure and exhibiting robust behavior for functions with thousands of instructions. Evaluation shows that code quality is on par with LLVM, a state-of-the-art compiler infrastructure.},
  isbn = {978-3-642-33558-7},
  langid = {english},
  keywords = {Code Quality,Constraint Model,Constraint Programming,Instruction Schedule,Register Allocation},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/FLZKC62S/Lozano и др. - 2012 - Constraint-Based Register Allocation and Instructi.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/RM3PHUTI/lozano2012.pdf.pdf}
}

@online{mannarswamyLearningCombineInstructions2022,
  title = {Learning to {{Combine Instructions}} in {{LLVM Compiler}}},
  author = {Mannarswamy, Sandya and Das, Dibyendu},
  date = {2022-02-22},
  eprint = {2202.12379},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.12379},
  urldate = {2022-12-17},
  abstract = {Instruction combiner (IC) is a critical compiler optimization pass, which replaces a sequence of instructions with an equivalent and optimized instruction sequence at basic block level. There can be thousands of instruction-combining patterns which need to be frequently updated as new coding idioms/applications and novel hardware evolve over time. This results in frequent updates to the IC optimization pass thereby incurring considerable human effort and high software maintenance costs. To mitigate these challenges associated with the traditional IC, we design and implement a Neural Instruction Combiner (NIC) and demonstrate its feasibility by integrating it into the standard LLVM compiler optimization pipeline. NIC leverages neural sequence-to-sequence (Seq2Seq) models for generating optimized encoded IR sequence from the unoptimized encoded IR sequence. To the best of our knowledge, ours is the first work demonstrating the feasibility of a neural instruction combiner built into a full-fledged compiler pipeline. Given the novelty of this task, we built a new dataset for training our NIC neural model. We show that NIC achieves exact match results percentage of 72\% for optimized sequences as compared to traditional IC and neural machine translation metric Bleu precision score of 0.94, demonstrating its feasibility in a production compiler pipeline.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ZZJJBRVS/Mannarswamy и Das - 2022 - Learning to Combine Instructions in LLVM Compiler.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/3FN3ZCQH/2202.html}
}

@inproceedings{markidisNVIDIATensorCore2018,
  title = {{{NVIDIA Tensor Core Programmability}}, {{Performance}} \& {{Precision}}},
  booktitle = {2018 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Markidis, Stefano and Der Chien, Steven Wei and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
  date = {2018-05},
  eprint = {1803.04014},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {522--531},
  doi = {10.1109/IPDPSW.2018.00091},
  url = {http://arxiv.org/abs/1803.04014},
  urldate = {2022-12-18},
  abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called "Tensor Core" that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/VWQ8EN4H/Markidis и др. - 2018 - NVIDIA Tensor Core Programmability, Performance & .pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/8QUEAPRN/1803.html}
}

@inproceedings{mastenFunctionKernelVectorization2018,
  title = {Function/{{Kernel Vectorization}} via {{Loop Vectorizer}}},
  booktitle = {2018 {{IEEE}}/{{ACM}} 5th {{Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}} ({{LLVM-HPC}})},
  author = {Masten, Matt and Tyurin, Evgeniy and Mitropoulou, Konstantina and Garcia, Eric and Saito, Hideki},
  date = {2018-11},
  pages = {39--48},
  publisher = {{IEEE}},
  location = {{Dallas, TX, USA}},
  doi = {10.1109/LLVM-HPC.2018.8639483},
  url = {https://ieeexplore.ieee.org/document/8639483/},
  urldate = {2022-12-17},
  abstract = {Currently, there are three vectorizers in the LLVM trunk: Loop Vectorizer, SLP Vectorizer, and Load-Store Vectorizer. There is a need for vectorizing functions/kernels: 1) Function calls are an integral part of programming real world application code and we cannot always rely on fully inlining them. When a function call is made from a vectorized context such as vectorized loop or vectorized function, if there are no vectorized callees available, the call has to be made to a scalar callee, one vector element at a time. At the programming model level, OpenMP declare simd is a standardized syntax to address this problem. LLVM needs a vectorizer to properly vectorize OpenMP declare simd functions. 2) Also, in the GPGPU programming model, such as OpenCL, work-item (thread) parallelism is not expressed with a loop; it is implicit in the execution of the kernels. In order to exploit SIMD parallelism at this top-level (thread-level), we need to start from vectorizing the kernel.},
  eventtitle = {2018 {{IEEE}}/{{ACM}} 5th {{Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}} ({{LLVM-HPC}})},
  isbn = {978-1-72810-188-0},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/LFXYG32H/Masten и др. - 2018 - FunctionKernel Vectorization via Loop Vectorizer.pdf}
}

@online{mccalpinCommentsTimingShort,
  title = {Comments on Timing Short Code Sections on {{Intel}} Processors},
  author = {McCalpin, John},
  url = {https://sites.utexas.edu/jdm4372/2018/07/23/comments-on-timing-short-code-sections-on-intel-processors/},
  organization = {{John McCalpin's blog}}
}

@inproceedings{mcgovernSchedulingStraightLineCode1998,
  title = {Scheduling {{Straight-Line Code Using Reinforcement Learning}} and {{Rollouts}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {McGovern, Amy and Moss, J.},
  date = {1998},
  volume = {11},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/1998/hash/596f713f9a7376fe90a62abaaedecc2d-Abstract.html},
  urldate = {2023-03-06},
  abstract = {In  1986, Tanner and Mead [1] implemented an interesting constraint sat(cid:173) isfaction  circuit  for  global  motion  sensing  in  a VLSI.  We  report  here  a  new  and  improved a VLSI implementation that provides smooth optical  flow as well as global motion in a two dimensional visual field.  The com(cid:173) putation of optical flow  is  an ill-posed problem, which expresses itself as  the aperture problem.  However, the optical flow  can be estimated by the  use of regularization methods, in  which additional constraints are intro(cid:173) duced in  terms of a global energy functional that must be minimized . We  show how the algorithmic constraints of Hom and Schunck [2]  on com(cid:173) puting smooth optical flow can be mapped onto the physical constraints  of an equivalent electronic network.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/Z35VKUS2/McGovern и Moss - 1998 - Scheduling Straight-Line Code Using Reinforcement .pdf}
}

@online{meiDissectingGPUMemory2016,
  title = {Dissecting {{GPU Memory Hierarchy}} through {{Microbenchmarking}}},
  author = {Mei, Xinxin and Chu, Xiaowen},
  date = {2016-03-14},
  eprint = {1509.02308},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1509.02308},
  urldate = {2022-12-31},
  abstract = {Memory access efficiency is a key factor in fully utilizing the computational power of graphics processing units (GPUs). However, many details of the GPU memory hierarchy are not released by GPU vendors. In this paper, we propose a novel fine-grained microbenchmarking approach and apply it to three generations of NVIDIA GPUs, namely Fermi, Kepler and Maxwell, to expose the previously unknown characteristics of their memory hierarchies. Specifically, we investigate the structures of different GPU cache systems, such as the data cache, the texture cache and the translation look-aside buffer (TLB). We also investigate the throughput and access latency of GPU global memory and shared memory. Our microbenchmark results offer a better understanding of the mysterious GPU memory hierarchy, which will facilitate the software optimization and modelling of GPU architectures. To the best of our knowledge, this is the first study to reveal the cache properties of Kepler and Maxwell GPUs, and the superiority of Maxwell in shared memory performance under bank conflict.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Hardware Architecture},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/IGDC2TSE/Mei и Chu - 2016 - Dissecting GPU Memory Hierarchy through Microbench.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/FB7WLKGL/1509.html}
}

@inproceedings{mendisCompilerAutoVectorizationImitation2019,
  title = {Compiler {{Auto-Vectorization}} with {{Imitation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mendis, Charith and Yang, Cambridge and Pu, Yewen and Amarasinghe, Dr.Saman and Carbin, Michael},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html},
  urldate = {2022-12-17},
  abstract = {Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit fine-grained data level parallelism. To exploit this parallelism, compilers employ auto-vectorization techniques to automatically convert scalar code into vector code. Larsen \& Amarasinghe (2000) first introduced superword level parallelism (SLP) based vectorization, which is one form of vectorization popularly used by compilers. Current compilers employ hand-crafted heuristics and typically only follow one SLP vectorization strategy which can be suboptimal. Recently, Mendis \& Amarasinghe (2018) formulated the instruction packing problem of SLP vectorization by leveraging an integer linear programming (ILP) solver, achieving superior runtime performance. In this work, we explore whether it is feasible to imitate optimal decisions made by their ILP solution by fitting a graph neural network policy. We show that the learnt policy produces a vectorization scheme which is better than industry standard compiler heuristics both in terms of static measures and runtime performance. More specifically, the learnt agent produces a vectorization scheme which has a 22.6\% higher average reduction in cost compared to LLVM compiler when measured using its own cost model and achieves a geometric mean runtime speedup of 1.015× on the NAS benchmark suite when compared to LLVM’s SLP vectorizer.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/WF5ACIML/Mendis и др. - 2019 - Compiler Auto-Vectorization with Imitation Learnin.pdf}
}

@inproceedings{mendisIthemalAccuratePortable2019,
  title = {Ithemal: {{Accurate}}, {{Portable}} and {{Fast Basic Block Throughput Estimation}} Using {{Deep Neural Networks}}},
  shorttitle = {Ithemal},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Mendis, Charith and Renda, Alex and Amarasinghe, Dr Saman and Carbin, Michael},
  date = {2019-05-24},
  pages = {4505--4515},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/mendis19a.html},
  urldate = {2023-03-06},
  abstract = {Predicting the number of clock cycles a processor takes to execute a block of assembly instructions in steady state (the throughput) is important for both compiler designers and performance engineers. Building an analytical model to do so is especially complicated in modern x86-64 Complex Instruction Set Computer (CISC) machines with sophisticated processor microarchitectures in that it is tedious, error prone, and must be performed from scratch for each processor generation. In this paper we present Ithemal, the first tool which learns to predict the throughput of a set of instructions. Ithemal uses a hierarchical LSTM–based approach to predict throughput based on the opcodes and operands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in compiler backends and static machine code analyzers. In particular, our model has less than half the error of state-of-the-art analytical models (LLVM’s llvm-mca and Intel’s IACA). Ithemal is also able to predict these throughput values just as fast as the aforementioned tools, and is easily ported across a variety of processor microarchitectures with minimal developer effort.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/7AAURHWI/Mendis и др. - 2019 - Ithemal Accurate, Portable and Fast Basic Block T.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/AF9BBU5W/Mendis и др. - 2019 - Ithemal Accurate, Portable and Fast Basic Block T.pdf}
}

@article{mironovBibliotekaDlyaRazrabotki2023,
  title = {Библиотека для разработки компиляторов},
  author = {Миронов, Сергей Владимирович and Батраева, Инна Александровна and Дунаев, Павел Дмитриевич},
  date = {2023-02-04},
  journaltitle = {Труды Института системного программирования РАН},
  volume = {34},
  number = {5},
  pages = {77--88},
  issn = {2220-6426},
  doi = {10.15514/ISPRAS-2022-34(5)-5},
  url = {https://ispranproceedings.elpub.ru/jour/article/view/1577},
  urldate = {2023-03-01},
  abstract = {Научный рецензируемый журнал, список ВАК},
  issue = {5},
  langid = {russian},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/Z6KZJKT9/Миронов и др. - 2023 - Библиотека для разработки компиляторов.pdf}
}

@article{mishraUsingMachineLearning,
  title = {Using {{Machine Learning}} for {{OpenMP GPU Offloading}} in {{LLVM}}},
  author = {Mishra, Alok and Malik, Abid M and Chapman, Barbara},
  abstract = {OpenMP 5.0 provides features to exploit the compute power within the node of today’s leadership class facilities. Among these features, the GPU offloading directives are key to take advantage of heterogeneity on modern machines. However, these features place the domain scientists with portability challenges, especially for optimizing data movement between a host and a device. Tools that facilitate the usage of such features are becoming important to the scientific community. An important tool for porting legacy codes to newer machines will be compilers that can predict the feasibility of transferring kernels on GPUs and inserts required OpenMP GPU offload features automatically at compile time. In this work, we are exploring a novel approach for the automated handling of OpenMP GPU offloading using machine learning techniques. We aim to develop an end-to-end application framework, from legacy code to GPU offloading, that integrates machine learning techniques into the LLVM compiler.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/WNALAF5U/Mishra и др. - Using Machine Learning for OpenMP GPU Offloading i.pdf}
}

@article{misonizhnikAvtomaticheskoeTestirovanieLLVMprogramm2022,
  title = {Автоматическое тестирование LLVM-программ со сложными входными структурами данных},
  author = {Мисонижник, Александр Владимирович and Бабушкин, Алексей Александрович and Морозов, Сергей Антонович and Костюков, Юрий Олегович and Мордвинов, Дмитрий Александрович and Кознов, Дмитрий Владимирович},
  date = {2022-12-20},
  journaltitle = {Труды Института системного программирования РАН},
  volume = {34},
  number = {4},
  pages = {49--62},
  issn = {2220-6426},
  url = {https://ispranproceedings.elpub.ru/jour/article/view/1554},
  urldate = {2023-03-01},
  abstract = {Научный рецензируемый журнал, список ВАК},
  issue = {4},
  langid = {russian},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/FLRM64BY/Мисонижник и др. - 2022 - Автоматическое тестирование LLVM-программ со сложн.pdf}
}

@article{miyazakiRVCorePOptimizedRISCV2020,
  title = {{{RVCoreP}} : {{An}} Optimized {{RISC-V}} Soft Processor of Five-Stage Pipelining},
  shorttitle = {{{RVCoreP}}},
  author = {Miyazaki, Hiromu and Kanamori, Takuto and Islam, Md Ashraful and Kise, Kenji},
  date = {2020-12-01},
  journaltitle = {IEICE Transactions on Information and Systems},
  shortjournal = {IEICE Trans. Inf. \& Syst.},
  volume = {E103.D},
  number = {12},
  eprint = {2002.03568},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2494--2503},
  issn = {0916-8532, 1745-1361},
  doi = {10.1587/transinf.2020PAP0015},
  url = {http://arxiv.org/abs/2002.03568},
  urldate = {2023-03-02},
  abstract = {RISC-V is a RISC based open and loyalty free instruction set architecture which has been developed since 2010, and can be used for cost-effective soft processors on FPGAs. The basic 32-bit integer instruction set in RISC-V is defined as RV32I, which is sufficient to support the operating system environment and suits for embedded systems. In this paper, we propose an optimized RV32I soft processor named RVCoreP adopting five-stage pipelining. The processor applies three effective optimization methods to improve the operating frequency. These methods are instruction fetch unit optimization including pipelined branch prediction mechanism, ALU optimization, and data alignment and sign-extension optimization for data memory output. We implement RVCoreP in Verilog HDL and verify the behavior using Verilog simulation and an actual Xilinx Atrix-7 FPGA board. We evaluate IPC (instructions per cycle), operating frequency, hardware resource utilization, and processor performance. From the evaluation results, we show that RVCoreP achieves 30.0\% performance improvement compared with VexRiscv, which is a high-performance and open source RV32I processor selected from some related works.},
  keywords = {C.1.1,Computer Science - Hardware Architecture},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/999D88EQ/Miyazaki и др. - 2020 - RVCoreP  An optimized RISC-V soft processor of fi.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/EKVF6YY2/2002.html}
}

@inproceedings{mollPartialControlflowLinearization2018,
  title = {Partial Control-Flow Linearization},
  booktitle = {Proceedings of the 39th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Moll, Simon and Hack, Sebastian},
  date = {2018-06-11},
  pages = {543--556},
  publisher = {{ACM}},
  location = {{Philadelphia PA USA}},
  doi = {10.1145/3192366.3192413},
  url = {https://dl.acm.org/doi/10.1145/3192366.3192413},
  urldate = {2023-05-01},
  abstract = {If-conversion is a fundamental technique for vectorization. It accounts for the fact that in a SIMD program, several targets of a branch might be executed because of divergence. Especially for irregular data-parallel workloads, it is crucial to avoid if-converting non-divergent branches to increase SIMD utilization. In this paper, we present partial linearization, a simple and efficient if-conversion algorithm that overcomes several limitations of existing if-conversion techniques. In contrast to prior work, it has provable guarantees on which non-divergent branches are retained and will never duplicate code or insert additional branches. We show how our algorithm can be used in a classic loop vectorizer as well as to implement data-parallel languages such as ISPC or OpenCL. Furthermore, we implement prior vectorizer optimizations on top of partial linearization in a more general way. We evaluate the implementation of our algorithm in LLVM on a range of irregular data analytics kernels, a neutronics simulation benchmark and NAB, a molecular dynamics benchmark from SPEC2017 on AVX2, AVX512, and ARM Advanced SIMD machines and report speedups of up to 146\% over ICC, GCC and Clang O3.},
  eventtitle = {{{PLDI}} '18: {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  isbn = {978-1-4503-5698-5},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/48LMUSBL/moll2018.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/5LU33XHF/Moll и Hack - 2018 - Partial control-flow linearization.pdf}
}

@article{monniauxFormallyVerifiedLoopInvariant2022,
  title = {Formally {{Verified Loop-Invariant Code Motion}} and {{Assorted Optimizations}}},
  author = {Monniaux, David and Six, Cyril},
  date = {2022},
  journaltitle = {ACM Transactions on Embedded Computing Systems (TECS)},
  publisher = {{ACM}},
  doi = {10.1145/3529507},
  url = {https://hal.archives-ouvertes.fr/hal-03628646},
  urldate = {2022-12-17},
  abstract = {We present an approach for implementing a formally certified loop-invariant code motion optimization by composing an unrolling pass and a formally certified yet efficient global subexpression elimination. This approach is lightweight: each pass comes with a simple and independent proof of correctness. Experiments show the approach significantly narrows the performance gap between the CompCert certified compiler and state-of-the-art optimizing compilers. Our static analysis employs an efficient yet verified hashed set structure, resulting in fast compilation.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/YRSM2HXP/Monniaux и Six - 2022 - Formally Verified Loop-Invariant Code Motion and A.pdf}
}

@inproceedings{muhligMxTasksHowMake2021,
  title = {{{MxTasks}}: {{How}} to {{Make Efficient Synchronization}} and {{Prefetching Easy}}},
  shorttitle = {{{MxTasks}}},
  booktitle = {Proceedings of the 2021 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Mühlig, Jan and Teubner, Jens},
  date = {2021-06-09},
  pages = {1331--1344},
  publisher = {{ACM}},
  location = {{Virtual Event China}},
  doi = {10.1145/3448016.3457268},
  url = {https://dl.acm.org/doi/10.1145/3448016.3457268},
  urldate = {2022-12-17},
  abstract = {The hardware environment has changed rapidly in recent years: Many cores, multiple sockets, and large amounts of main memory have become a commodity. To benefit from these highly parallel systems, the software has to be adapted. Sophisticated latch-free data structures and algorithms are often meant to address the situation. But they are cumbersome to develop and may still not provide the desired scalability. As a remedy, we present MxTasking, a task-based framework that assists the design of latch-free and parallel data structures. MxTasking eases the information exchange between applications and the operating system, resulting in novel opportunities to manage resources in a truly hardware- and application-conscious way.},
  eventtitle = {{{SIGMOD}}/{{PODS}} '21: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-8343-1},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/XQAKAXCD/Mühlig и Teubner - 2021 - MxTasks How to Make Efficient Synchronization and.pdf}
}

@inproceedings{mukunokiReproducibleBLASRoutines2020,
  title = {Reproducible {{BLAS Routines}} with~{{Tunable Accuracy Using Ozaki Scheme}} for {{Many-Core Architectures}}},
  booktitle = {Parallel {{Processing}} and {{Applied Mathematics}}},
  author = {Mukunoki, Daichi and Ogita, Takeshi and Ozaki, Katsuhisa},
  editor = {Wyrzykowski, Roman and Deelman, Ewa and Dongarra, Jack and Karczewski, Konrad},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {516--527},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-43229-4_44},
  abstract = {Generally, floating-point computations comprise rounding errors; the result may be inaccurate and not identical (non-reproducible). Particularly, heterogeneous computing has many factors that affect reproducibility. The loss of accuracy and reproducibility could be a crucial issue in debugging complex codes and the reliability of computations. In this paper, we propose high-performance implementations of reproducible basic linear algebra subprograms (BLAS) routines with tunable accuracy for many-core architectures. Our approach is based on an accurate matrix-multiplication method, Ozaki scheme, which can be constructed on level-3 BLAS that performs standard floating-point operations. We demonstrate the performance of three routines: inner product (DOT), matrix-vector multiplication (GEMV), and matrix-multiplication (GEMM) on NVIDIA’s Volta GPU by comparing these with the standard routines provided by the vendor. Furthermore, we demonstrate the reproducibility between CPU and GPU and its accuracy.},
  isbn = {978-3-030-43229-4},
  langid = {english},
  keywords = {Accurate,BLAS,Reproducible},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/VEXJSUB9/10.1007@978-3-030-43229-4.pdf.pdf}
}

@article{NEONProgrammerGuide2013,
  title = {{{NEON Programmer}}’s {{Guide}}},
  date = {2013},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/G668DINQ/2013 - NEON Programmer’s Guide.pdf}
}

@inproceedings{nuzmanAutovectorizationInterleavedData2006,
  title = {Auto-Vectorization of Interleaved Data for {{SIMD}}},
  booktitle = {Proceedings of the 2006 {{ACM SIGPLAN}} Conference on {{Programming}} Language Design and Implementation  - {{PLDI}} '06},
  author = {Nuzman, Dorit and Rosen, Ira and Zaks, Ayal},
  date = {2006},
  pages = {132},
  publisher = {{ACM Press}},
  location = {{Ottawa, Ontario, Canada}},
  doi = {10.1145/1133981.1133997},
  url = {http://portal.acm.org/citation.cfm?doid=1133981.1133997},
  urldate = {2022-12-18},
  abstract = {Most implementations of the Single Instruction Multiple Data (SIMD) model available today require that data elements be packed in vector registers. Operations on disjoint vector elements are not supported directly, and require explicit data reorganization manipulations. Computations on non-contiguous and especially interleaved data appear in important applications, which can greatly benefit from SIMD instructions once the data is reorganized properly. Vectorizing such computations efficiently is therefore an ambitious challenge for both programmers and vectorizing compilers. In this paper we demonstrate an automatic compilation scheme that supports effective vectorization in the presence of interleaved data with strides that are power of 2, facilitating data reorganization. We demonstrate how our vectorization scheme applies to SIMD architectures that are dominant today, and present experimental results on a wide range of key kernels, showing speedups up to 3.7 for interleaving level (stride) as high as 8.},
  eventtitle = {The 2006 {{ACM SIGPLAN}} Conference},
  isbn = {978-1-59593-320-1},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/V25JTYL4/Nuzman и др. - 2006 - Auto-vectorization of interleaved data for SIMD.pdf}
}

@article{nuzmanCompilerVectorizationTechniques2022,
  title = {Compiler {{Vectorization Techniques}} for a {{Disjoint SIMD Architecture}}},
  author = {Nuzman, Dorit and Biberstein, Marina and Zaks, Ayal},
  date = {2022-12-18},
  abstract = {This paper presents compiler technology that targets a novel low-power Digital Signal Processor (DSP) architecture. The architecture is characterized by the exploitation of data and instruction level parallelism,and uses a large register file with dynamically composed vectors for data manipulation. We describe how an optimizing compiler can make use of the vector register file with its flexible addressing to efficiently support a range of data access patterns that are present in the digital processing application domain. We describe new challenges presented by this novel DSP architecture,as well as new opportunities for aggressive yet low-overhead opti- mizations that it introduces. Experiments show that an op- timizing compiler can target such an architecture efficiently to achieve performance that is comparable to the optimal hand-generated code for key benchmarks. The resulting compiler technology represents an advance of the state-of- the-art in the area of DSP compilation.}
}

@inproceedings{nuzmanOuterloopVectorizationRevisited2008,
  title = {Outer-Loop Vectorization - Revisited for Short {{SIMD}} Architectures},
  author = {Nuzman, Dorit and Zaks, Ayal},
  date = {2008-01-01},
  pages = {2--11},
  doi = {10.1145/1454115.1454119},
  abstract = {Vectorization has been an important method of using data-level parallelism to accelerate scientific workloads on vector machines such as Cray for the past three decades. In the last decade it has also proven useful for accelerating multi-media and embedded applications on short SIMD architectures such as MMX, SSE and AltiVec. Most of the focus has been directed at innermost loops, effectively executing their iterations concurrently as much as possible. Outer loop vectorization refers to vectorizing a level of a loop nest other than the innermost, which can be beneficial if the outer loop exhibits greater data-level parallelism and locality than the innermost loop. Outer loop vectorization has traditionally been performed by interchanging an outer-loop with the innermost loop, followed by vectorizing it at the innermost position. A more direct unroll-and-jam approach can be used to vectorize an outer-loop without involving loop interchange, which can be especially suitable for short SIMD architectures. In this paper we revisit the method of outer loop vectorization, paying special attention to properties of modern short SIMD architectures. We show that even though current optimizing compilers for such targets do not apply outer-loop vectorization in general, it can provide significant performance improvements over innermost loop vectorization. Our implementation of direct outer-loop vectorization, available in GCC 4.3, achieves speedup factors of 3.13 and 2.77 on average across a set of benchmarks, compared to 1.53 and 1.39 achieved by innermost loop vectorization, when running on a Cell BE SPU and PowerPC970 processors respectively. Moreover, outer-loop vectorization provides new reuse opportunities that can be vital for such short SIMD architectures, including efficient handling of alignment. We present an optimization tapping such opportunities, capable of further boosting the performance obtained by outer-loop vectorization to achieve average speedup factors of 5.26 and 3.64.},
  eventtitle = {Parallel {{Architectures}} and {{Compilation Techniques}} - {{Conference Proceedings}}, {{PACT}}}
}

@inproceedings{nuzmanVaporSIMDAutovectorize2011,
  title = {Vapor {{SIMD}}: {{Auto-vectorize}} Once, Run Everywhere},
  shorttitle = {Vapor {{SIMD}}},
  author = {Nuzman, Dorit and Dyshel, Sergei and Rohou, Erven and Rosen, Ira and Williams, Kevin and Yuste, David and Cohen, Albert and Zaks, Ayal},
  date = {2011-04-01},
  pages = {151--160},
  doi = {10.1109/CGO.2011.5764683},
  abstract = {Just-in-Time (JIT) compiler technology offers portability while facilitating target- and context-specific specialization. Single-Instruction-Multiple-Data (SIMD) hardware is ubiquitous and markedly diverse, but can be difficult for JIT compilers to efficiently target due to resource and budget constraints. We present our design for a synergistic auto-vectorizing compilation scheme. The scheme is composed of an aggressive, generic offline stage coupled with a lightweight, target-specific online stage. Our method leverages the optimized intermediate results provided by the first stage across disparate SIMD architectures from different vendors, having distinct characteristics ranging from different vector sizes, memory alignment and access constraints, to special computational idioms. We demonstrate the effectiveness of our design using a set of kernels that exercise innermost loop, outer loop, as well as straight-line code vectorization, all automatically extracted by the common offline compilation stage. This results in performance comparable to that provided by specialized monolithic offline compilers. Our framework is implemented using open-source tools and standards, thereby promoting interoperability and extendibility.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/9ULKH5QQ/Nuzman и др. - 2011 - Vapor SIMD Auto-vectorize once, run everywhere.pdf}
}

@patent{nuzmanVectorizationProgramCode2014,
  type = {patent},
  title = {Vectorization of Program Code},
  author = {Nuzman, Dorit and Rosen, Ira and Zaks, Ayal},
  date = {2014-04-29},
  abstract = {A method for vectorization of a block of code is provided. The method comprises receiving a first block of code as input; and converting the first block of code into at least a second block of code and a third block of code. The first block of code accesses a first set of memory addresses that are potentially misaligned. The second block of code performs conditional leaping address incrementation to selectively access a first subset of the first set of memory addresses. The third block of code accesses a second subset of the first set of memory addresses starting from an aligned memory address, simultaneously accessing multiple memory addresses at a time. No memory address belongs to both the first subset and the second subset of memory addresses.}
}

@inproceedings{nuzmanVectorizingSIMdDDSP2003,
  title = {Vectorizing for a {{SIMdD DSP}} Architecture},
  author = {Nuzman, Dorit and Biberstein, Marina and Ben-David, Shay and Zaks, Ayal},
  date = {2003-01-01},
  pages = {2--11},
  doi = {10.1145/951710.951714},
  abstract = {The Single Instruction Multiple Data (SIMD) model for finegrained parallelism was recently extended to support SIMD operations on disjoint vector elements. In this paper we demonstrate how SIMdD (SIMD on disjoint data) supports e\#ective vectorization of digital signal processing (DSP) benchmarks, by facilitating data reorganization and reuse. In particular we show that this model can be adopted by a compiler to achieve nearoptimal performance for important classes of kernels.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/SQH4SDWB/Nuzman и др. - 2003 - Vectorizing for a SIMdD DSP architecture.pdf}
}

@online{olabiCompilerFrameworkOptimizing2022,
  title = {A {{Compiler Framework}} for {{Optimizing Dynamic Parallelism}} on {{GPUs}}},
  author = {Olabi, Mhd Ghaith and Luna, Juan Gómez and Mutlu, Onur and Hwu, Wen-mei and Hajj, Izzat El},
  date = {2022-01-08},
  eprint = {2201.02789},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.02789},
  urldate = {2022-12-17},
  abstract = {Dynamic parallelism on GPUs allows GPU threads to dynamically launch other GPU threads. It is useful in applications with nested parallelism, particularly where the amount of nested parallelism is irregular and cannot be predicted beforehand. However, prior works have shown that dynamic parallelism may impose a high performance penalty when a large number of small grids are launched. The large number of launches results in high launch latency due to congestion, and the small grid sizes result in hardware underutilization. To address this issue, we propose a compiler framework for optimizing the use of dynamic parallelism in applications with nested parallelism. The framework features three key optimizations: thresholding, coarsening, and aggregation. Thresholding involves launching a grid dynamically only if the number of child threads exceeds some threshold, and serializing the child threads in the parent thread otherwise. Coarsening involves executing the work of multiple thread blocks by a single coarsened block to amortize the common work across them. Aggregation involves combining multiple child grids into a single aggregated grid. Our evaluation shows that our compiler framework improves the performance of applications with nested parallelism by a geometric mean of 43.0x over applications that use dynamic parallelism, 8.7x over applications that do not use dynamic parallelism, and 3.6x over applications that use dynamic parallelism with aggregation alone as proposed in prior work.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Hardware Architecture},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/S4Z2CRD3/Olabi и др. - 2022 - A Compiler Framework for Optimizing Dynamic Parall.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/KKM35I56/2201.html}
}

@article{paasoVectorizationJacobiMethod,
  title = {Vectorization of the {{Jacobi}} Method with Single- and Multiple Right-Hand Sides},
  author = {Paaso, Patrik and Klofkorn, Robert},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/FUTKY54M/Paaso и Klofkorn - Vectorization of the Jacobi method with single- an.pdf}
}

@inproceedings{palomaresEvaluatingOutofOrderEngine2016,
  title = {Evaluating {{Out-of-Order Engine Limitations Using Uop Flow Simulation}}},
  booktitle = {Tools for {{High Performance Computing}} 2015},
  author = {Palomares, Vincent and Wong, David C. and Kuck, David J. and Jalby, William},
  editor = {Knüpfer, Andreas and Hilbrich, Tobias and Niethammer, Christoph and Gracia, José and Nagel, Wolfgang E. and Resch, Michael M.},
  date = {2016},
  pages = {161--181},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-39589-0_13},
  abstract = {Out-of-order mechanisms in recent microarchitectures do a very good job at hiding latencies and improving performance. However, they come with limitations not easily modeled statically, and hard to quantify exactly even dynamically. This paper will present Uop Flow Simulation (UFS), a loop performance prediction technique accounting for such restrictions by combining static analysis and cycle-driven simulation. UFS simulates the behavior of the execution pipeline when executing a loop. It handles instruction latencies, dependencies, out-of-order resource consumption and other low-level details while completely ignoring semantics. We will use a UFS prototype to validate our approach on Sandy Bridge using loops from real-world HPC applications, showing it is both accurate and very fast (reaching simulation speeds of hundreds of thousands of cycles per second).},
  isbn = {978-3-319-39589-0},
  langid = {english},
  keywords = {Buffer Size,Reservation Station,Resource Allocation Scheme,Simulated Cycle,Simulation Speed}
}

@inproceedings{parkFlowStreamProcessing2010,
  title = {Flow: {{A Stream Processing System Simulator}}},
  shorttitle = {Flow},
  author = {Park, Alfred and Li, Cheng-Hong and Nair, Ravi and Ohba, Nobuyuki and Shvadron, Uzi and Zaks, Ayal and Schenfeld, Eugen},
  date = {2010-06-19},
  pages = {1--9},
  doi = {10.1109/PADS.2010.5471658},
  abstract = {Stream processing is an important emerging computational model for performing complex operations on and across multi-source, high volume, unpredictable dataflows. We present Flow, a platform for parallel and distributed stream processing system simulation that provides a flexible modeling environment for analyzing stream processing applications. The Flow stream processing system simulator is a high performance, scalable simulator that automatically parallelizes chunks of the model space and incurs near zero synchronization overhead for stream application graphs that exhibit feed-forward behavior. We show promising multi-threaded and multi-process event rates exceeding 80 million events per second on a cluster with 256 processor cores.},
  eventtitle = {Proceedings - {{Workshop}} on {{Principles}} of {{Advanced}} and {{Distributed Simulation}}, {{PADS}}}
}

@inproceedings{patelMARSSFullSystem2011,
  title = {{{MARSS}}: A Full System Simulator for Multicore X86 {{CPUs}}},
  shorttitle = {{{MARSS}}},
  booktitle = {Proceedings of the 48th {{Design Automation Conference}}},
  author = {Patel, Avadh and Afram, Furat and Chen, Shunfei and Ghose, Kanad},
  date = {2011-06-05},
  series = {{{DAC}} '11},
  pages = {1050--1055},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2024724.2024954},
  url = {https://doi.org/10.1145/2024724.2024954},
  urldate = {2023-03-06},
  abstract = {We present MARSS, an open source, fast, full system simulation tool built on QEMU to support cycle-accurate simulation of superscalar homogeneous and heterogeneous multicore x86 processors. MARSS includes detailed models of coherent caches, interconnections, chipsets, memory and IO devices. MARSS simulates the execution of all software components in the system, including unmodified binaries of applications, OS and libraries.},
  isbn = {978-1-4503-0636-2},
  keywords = {emulator,full-system simulator,heterogeneous multi-core systems,multi-core x86 CPU simulator},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/JPACJDC6/patel2011.pdf.pdf}
}

@article{petrogalliArmSVEFundamentals2019,
  title = {Arm {{SVE Fundamentals}}},
  author = {Petrogalli, Francesco},
  date = {2019},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/423JTQRW/Petrogalli - 2019 - Arm SVE Fundamentals.pdf}
}

@online{phanConvolutionalNeuralNetworks2018,
  title = {Convolutional {{Neural Networks}} over {{Control Flow Graphs}} for {{Software Defect Prediction}}},
  author = {Phan, Anh Viet and Nguyen, Minh Le and Bui, Lam Thu},
  date = {2018-02-14},
  eprint = {1802.04986},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.04986},
  urldate = {2023-05-09},
  abstract = {Existing defects in software components is unavoidable and leads to not only a waste of time and money but also many serious consequences. To build predictive models, previous studies focus on manually extracting features or using tree representations of programs, and exploiting different machine learning algorithms. However, the performance of the models is not high since the existing features and tree structures often fail to capture the semantics of programs. To explore deeply programs' semantics, this paper proposes to leverage precise graphs representing program execution flows, and deep neural networks for automatically learning defect features. Firstly, control flow graphs are constructed from the assembly instructions obtained by compiling source code; we thereafter apply multi-view multi-layer directed graph-based convolutional neural networks (DGCNNs) to learn semantic features. The experiments on four real-world datasets show that our method significantly outperforms the baselines including several other deep learning approaches.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Software Engineering},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ZPDXNF5R/Phan и др. - 2018 - Convolutional Neural Networks over Control Flow Gr.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/864YQUU9/1802.html}
}

@article{pohlCorrelatingCostPerformance,
  title = {Correlating {{Cost}} with {{Performance}} in {{LLVM}}},
  author = {Pohl, Angela and Cosenza, Biagio},
  abstract = {A common technique to exploit data level parallelism is code vectorization. When performed by a compiler, it needs to find a valid vectorization and assess its benefit. In LLVM, this analysis is based on a cost calculation, which will approve the transformation if the cost of the vectorized code is lower than the original scalar code. However, the calculated cost does not correlate to the actual speedup gain. We therefore propose a pluggable cost model that correctly accounts for vectorization overheads and further features of the target hardware platform. Using such a platform specific model, the compiler can assess a code transformation’s impact on application performance, make safe choices whether to transform or not, and compare different optimization options.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/IAMNKCQF/Pohl и Cosenza - Correlating Cost with Performance in LLVM.pdf}
}

@inproceedings{pohlCostModellingVectorization2018,
  title = {Cost {{Modelling}} for {{Vectorization}} on {{ARM}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Pohl, Angela and Cosenza, Biagio and Juurlink, Ben},
  date = {2018-09},
  pages = {644--645},
  issn = {2168-9253},
  doi = {10.1109/CLUSTER.2018.00084},
  abstract = {When applying a code transformation to optimize for performance, compilers need to assess its profitability beforehand. For this purpose, they utilize cost models, which compare the cost, an abstract measure of the code, before and after the transformation. If the cost is lower after the transformation, it will be applied. Exact cost modelling is therefore critical to avoid slowdowns or missed opportunities for speedups. In this work, we analyze the accuracy of LLVM's loop-level vectorization (LLV) cost model, and show the benefit of modelling speedup instead of instruction costs for higher vectorization rates and smaller execution times. The presented approach is portable to other compilers and hardwares as well.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  keywords = {Analytical models,ARM,Compiler,Conferences,Correlation,Cost Modelling,Hardware,Kernel,Linear regression,Performance Prediction,Predictive models,Vectorization},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/KSHY3XSR/8514928.html}
}

@inproceedings{pohlPortableCostModeling2019,
  title = {Portable {{Cost Modeling}} for {{Auto-Vectorizers}}},
  booktitle = {2019 {{IEEE}} 27th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  author = {Pohl, Angela and Cosenza, Biagio and Juurlink, Ben},
  date = {2019-10},
  pages = {359--369},
  publisher = {{IEEE}},
  location = {{Rennes, FR}},
  doi = {10.1109/MASCOTS.2019.00046},
  url = {https://ieeexplore.ieee.org/document/8843321/},
  urldate = {2022-12-18},
  abstract = {Compiler optimization passes employ cost models to determine if a code transformation will yield performance improvements. When this assessment is inaccurate, compilers apply transformations that are not beneficial, or refrain from applying ones that would have improved the code. We analyze the accuracy of the cost models used in LLVM’s and GCC’s vectorization passes for two different instruction set architectures. In general, speedup is over-estimated, resulting in mispredictions and a weak to medium correlation between predicted and actual performance gain. We therefore propose a novel cost model that is based on a code’s intermediate representation with refined memory access pattern features. Using linear regression techniques, this platform independent model is fitted to an AVX2 and a NEON hardware. Results show that the fitted model significantly improves the correlation between predicted and measured speedup (AVX2: +52\% for training data, +13\% for validation data), as well as the number of mispredictions (NEON: -15 for training data, -12 for validation data) for more than 80 code patterns.},
  eventtitle = {2019 {{IEEE}} 27th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  isbn = {978-1-72814-950-9},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/YG4TB2MI/Pohl и др. - 2019 - Portable Cost Modeling for Auto-Vectorizers.pdf}
}

@inproceedings{pohlPortableCostModeling2019a,
  title = {Portable {{Cost Modeling}} for {{Auto-Vectorizers}}},
  booktitle = {2019 {{IEEE}} 27th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  author = {Pohl, Angela and Cosenza, Biagio and Juurlink, Ben},
  date = {2019-10},
  pages = {359--369},
  publisher = {{IEEE}},
  location = {{Rennes, FR}},
  doi = {10.1109/MASCOTS.2019.00046},
  url = {https://ieeexplore.ieee.org/document/8843321/},
  urldate = {2023-08-12},
  abstract = {Compiler optimization passes employ cost models to determine if a code transformation will yield performance improvements. When this assessment is inaccurate, compilers apply transformations that are not beneficial, or refrain from applying ones that would have improved the code. We analyze the accuracy of the cost models used in LLVM’s and GCC’s vectorization passes for two different instruction set architectures. In general, speedup is over-estimated, resulting in mispredictions and a weak to medium correlation between predicted and actual performance gain. We therefore propose a novel cost model that is based on a code’s intermediate representation with refined memory access pattern features. Using linear regression techniques, this platform independent model is fitted to an AVX2 and a NEON hardware. Results show that the fitted model significantly improves the correlation between predicted and measured speedup (AVX2: +52\% for training data, +13\% for validation data), as well as the number of mispredictions (NEON: -15 for training data, -12 for validation data) for more than 80 code patterns.},
  eventtitle = {2019 {{IEEE}} 27th {{International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}} ({{MASCOTS}})},
  isbn = {978-1-72814-950-9},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/YHII6BBU/Pohl и др. - 2019 - Portable Cost Modeling for Auto-Vectorizers.pdf}
}

@article{pohlVectorizationCostModeling2020,
  title = {Vectorization Cost Modeling for {{NEON}}, {{AVX}} and {{SVE}}},
  author = {Pohl, Angela and Cosenza, Biagio and Juurlink, Ben},
  date = {2020-07},
  journaltitle = {Performance Evaluation},
  shortjournal = {Performance Evaluation},
  volume = {140--141},
  pages = {102106},
  issn = {01665316},
  doi = {10.1016/j.peva.2020.102106},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166531620300262},
  urldate = {2022-12-18},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/RE8RT4NZ/pohl2020.pdf.pdf}
}

@incollection{porpodasPostSLPCrossRegionVectorization2021,
  title = {{{PostSLP}}: {{Cross-Region Vectorization}} of {{Fully}} or {{Partially Vectorized Code}}},
  shorttitle = {{{PostSLP}}},
  booktitle = {Languages and {{Compilers}} for {{Parallel Computing}}},
  author = {Porpodas, Vasileios and Ratnalikar, Pushkar},
  editor = {Pande, Santosh and Sarkar, Vivek},
  date = {2021},
  volume = {11998},
  pages = {15--31},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-72789-5_2},
  url = {http://link.springer.com/10.1007/978-3-030-72789-5_2},
  urldate = {2022-12-17},
  isbn = {978-3-030-72788-8 978-3-030-72789-5},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/K7D5K2BL/Porpodas и Ratnalikar - 2021 - PostSLP Cross-Region Vectorization of Fully or Pa.pdf}
}

@inproceedings{porpodasPSLPPaddedSLP2015,
  title = {{{PSLP}}: {{Padded SLP}} Automatic Vectorization},
  shorttitle = {{{PSLP}}},
  booktitle = {2015 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  author = {Porpodas, Vasileios and Magni, Alberto and Jones, Timothy M.},
  date = {2015-02},
  pages = {190--201},
  publisher = {{IEEE}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1109/CGO.2015.7054199},
  url = {http://ieeexplore.ieee.org/document/7054199/},
  urldate = {2022-12-17},
  abstract = {The need to increase performance and power efficiency in modern processors has led to a wide adoption of SIMD vector units. All major vendors support vector instructions and the trend is pushing them to become wider and more powerful. However, writing code that makes efficient use of these units is hard and leads to platform-specific implementations. Compiler-based automatic vectorization is one solution for this problem. In particular the Superword-Level Parallelism (SLP) vectorization algorithm is the primary way to automatically generate vector code starting from straight-line scalar code. SLP is implemented in all major compilers, including GCC and LLVM.},
  eventtitle = {2015 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  isbn = {978-1-4799-8161-8},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/WTX7Y4QT/Porpodas и др. - 2015 - PSLP Padded SLP automatic vectorization.pdf}
}

@book{porpodasSuperNodeSLPOptimized2019,
  title = {Super-{{Node SLP}}: {{Optimized Vectorization}} for {{Code Sequences Containing Operators}} and {{Their Inverse Elements}}},
  shorttitle = {Super-{{Node SLP}}},
  author = {Porpodas, Vasileios and Rocha, Rodrigo and Brevnov, Evgueni and Góes, Luís and Mattson, Tim},
  date = {2019-02-16},
  doi = {10.1109/CGO.2019.8661192},
  abstract = {SLP Auto-vectorization converts straight-line code into vector code. It scans input code for groups of instructions that can be combined into vectors and replaces them with their corresponding vector instructions. This work introduces Super-Node SLP (SN-SLP), a new SLP-style algorithm, optimized for expressions that include a commu-tative operator (such as addition) and its corresponding inverse element (subtraction). SN-SLP uses the algebraic properties of commutative operators and their inverse elements to enable additional transformations that extend auto-vectorization to cases difficult for state-of-the-art auto-vectorizing compilers. We implemented SN-SLP in LLVM. Our evaluation on a real system demonstrates considerable performance improvements of benchmark code with no significant change in compilation time.}
}

@inproceedings{porpodasVWSLPAutovectorizationAdaptive2018,
  title = {{{VW-SLP}}: Auto-Vectorization with Adaptive Vector Width},
  shorttitle = {{{VW-SLP}}},
  booktitle = {Proceedings of the 27th {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}}},
  author = {Porpodas, Vasileios and Rocha, Rodrigo C. O. and Góes, Luís F. W.},
  date = {2018-11},
  pages = {1--15},
  publisher = {{ACM}},
  location = {{Limassol Cyprus}},
  doi = {10.1145/3243176.3243189},
  url = {https://dl.acm.org/doi/10.1145/3243176.3243189},
  urldate = {2022-12-17},
  eventtitle = {{{PACT}} '18: {{International}} Conference on {{Parallel Architectures}} and {{Compilation Techniques}}},
  isbn = {978-1-4503-5986-3},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/KXJPBMTF/Porpodas и др. - 2018 - VW-SLP auto-vectorization with adaptive vector wi.pdf}
}

@article{ProprietaryNotice,
  title = {Proprietary {{Notice}}},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/2PMPM9ZC/Proprietary Notice.pdf}
}

@inproceedings{rawatRegisterOptimizationsStencils2018,
  title = {Register Optimizations for Stencils on {{GPUs}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Rawat, Prashant Singh and Rastello, Fabrice and Sukumaran-Rajam, Aravind and Pouchet, Louis-Noël and Rountev, Atanas and Sadayappan, P.},
  date = {2018-02-10},
  pages = {168--182},
  publisher = {{ACM}},
  location = {{Vienna Austria}},
  doi = {10.1145/3178487.3178500},
  url = {https://dl.acm.org/doi/10.1145/3178487.3178500},
  urldate = {2022-12-17},
  abstract = {The recent advent of compute-intensive GPU architecture has allowed application developers to explore high-order 3D stencils for better computational accuracy. A common optimization strategy for such stencils is to expose sufficient data reuse by means such as loop unrolling, with the expectation of register-level reuse. However, the resulting code is often highly constrained by register pressure. While current state-of-the-art register allocators are satisfactory for most applications, they are unable to effectively manage register pressure for such complex high-order stencils, resulting in sub-optimal code with a large number of register spills. In this paper, we develop a statement reordering framework that models stencil computations as a DAG of trees with shared leaves, and adapts an optimal scheduling algorithm for minimizing register usage for expression trees. The effectiveness of the approach is demonstrated through experimental results on a range of stencils extracted from application codes.},
  eventtitle = {{{PPoPP}} '18: 23nd {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  isbn = {978-1-4503-4982-6},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/25GKXVMU/Rawat и др. - 2018 - Register optimizations for stencils on GPUs.pdf}
}

@inproceedings{rendaDiffTuneOptimizingCPU2020,
  title = {{{DiffTune}}: {{Optimizing CPU Simulator Parameters}} with {{Learned Differentiable Surrogates}}},
  shorttitle = {{{DiffTune}}},
  booktitle = {2020 53rd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  author = {Renda, Alex and Chen, Yishen and Mendis, Charith and Carbin, Michael},
  date = {2020-10},
  pages = {442--455},
  doi = {10.1109/MICRO50266.2020.00045},
  abstract = {CPU simulators are useful tools for modeling CPU execution behavior. However, they suffer from inaccuracies due to the cost and complexity of setting their fine-grained parameters, such as the latencies of individual instructions. This complexity arises from the expertise required to design benchmarks and measurement frameworks that can precisely measure the values of parameters at such fine granularity. In some cases, these parameters do not necessarily have a physical realization and are therefore fundamentally approximate, or even unmeasurable. In this paper we present DiffTune, a system for learning the parameters of x86 basic block CPU simulators from coarse-grained end-to-end measurements. Given a simulator, DiffTune learns its parameters by first replacing the original simulator with a differentiable surrogate, another function that approximates the original function; by making the surrogate differentiable, DiffTune is then able to apply gradient-based optimization techniques even when the original function is non-differentiable, such as is the case with CPU simulators. With this differentiable surrogate, DiffTune then applies gradient-based optimization to produce values of the simulator's parameters that minimize the simulator's error on a dataset of ground truth end-to-end performance measurements. Finally, the learned parameters are plugged back into the original simulator. DiffTune is able to automatically learn the entire set of microarchitecture-specific parameters within the Intel x86 simulation model of llvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling model. DiffTune's learned parameters lead llvm-mca to an average error that not only matches but lowers that of its original, expert-provided parameter values.},
  eventtitle = {2020 53rd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}} ({{MICRO}})},
  keywords = {Complexity theory,Machine Learning,Microarchitecture,Optimization,Performance Modeling,Software measurement,Task analysis,Tools,Tuning},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/9H4886EU/Renda и др. - 2020 - DiffTune Optimizing CPU Simulator Parameters with.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/E3CYJ5L6/renda2020.pdf.pdf}
}

@inproceedings{renSeeDeadUops2021,
  title = {I {{See Dead}} Μops: {{Leaking Secrets}} via {{Intel}}/{{AMD Micro-Op Caches}}},
  shorttitle = {I {{See Dead}} Μops},
  booktitle = {2021 {{ACM}}/{{IEEE}} 48th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Ren, Xida and Moody, Logan and Taram, Mohammadkazem and Jordan, Matthew and Tullsen, Dean M. and Venkat, Ashish},
  date = {2021-06},
  pages = {361--374},
  issn = {2575-713X},
  doi = {10.1109/ISCA52012.2021.00036},
  abstract = {Modern Intel, AMD, and ARM processors translate complex instructions into simpler internal micro-ops that are then cached in a dedicated on-chip structure called the micro-op cache. This work presents an in-depth characterization study of the micro-op cache, reverse-engineering many undocumented features, and further describes attacks that exploit the micro-op cache as a timing channel to transmit secret information. In particular, this paper describes three attacks – (1) a same thread cross-domain attack that leaks secrets across the user-kernel boundary, (2) a cross-SMT thread attack that transmits secrets across two SMT threads via the micro-op cache, and (3) transient execution attacks that have the ability to leak an unauthorized secret accessed along a misspeculated path, even before the transient instruction is dispatched to execution, breaking several existing invisible speculation and fencing-based solutions that mitigate Spectre.},
  eventtitle = {2021 {{ACM}}/{{IEEE}} 48th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  keywords = {Computer architecture,Microarchitecture,Program processors,System-on-chip,Timing,Transient analysis},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/3KDSXP3L/ren2021.pdf.pdf}
}

@online{ritterPMEvoPortableInference2020,
  title = {{{PMEvo}}: {{Portable Inference}} of {{Port Mappings}} for {{Out-of-Order Processors}} by {{Evolutionary Optimization}}},
  shorttitle = {{{PMEvo}}},
  author = {Ritter, Fabian and Hack, Sebastian},
  date = {2020-04-21},
  eprint = {2004.10044},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.10044},
  urldate = {2023-03-06},
  abstract = {Achieving peak performance in a computer system requires optimizations in every layer of the system, be it hardware or software. A detailed understanding of the underlying hardware, and especially the processor, is crucial to optimize software. One key criterion for the performance of a processor is its ability to exploit instruction-level parallelism. This ability is determined by the port mapping of the processor, which describes the execution units of the processor for each instruction. Processor manufacturers usually do not share the port mappings of their microarchitectures. While approaches to automatically infer port mappings from experiments exist, they are based on processor-specific hardware performance counters that are not available on every platform. We present PMEvo, a framework to automatically infer port mappings solely based on the measurement of the execution time of short instruction sequences. PMEvo uses an evolutionary algorithm that evaluates the fitness of candidate mappings with an analytical throughput model formulated as a linear program. Our prototype implementation infers a port mapping for Intel's Skylake architecture that predicts measured instruction throughput with an accuracy that is competitive to existing work. Furthermore, it finds port mappings for AMD's Zen+ architecture and the ARM Cortex-A72 architecture, which are out of scope of existing techniques.},
  pubstate = {preprint},
  keywords = {Computer Science - Hardware Architecture,Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/G6FG7F7V/Ritter и Hack - 2020 - PMEvo Portable Inference of Port Mappings for Out.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/P9SXH74V/2004.html}
}

@inproceedings{rohlLIKWIDMonitoringStack2017,
  title = {{{LIKWID Monitoring Stack}}: {{A}} Flexible Framework Enabling Job Specific Performance Monitoring for the Masses},
  shorttitle = {{{LIKWID Monitoring Stack}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Cluster Computing}} ({{CLUSTER}})},
  author = {Röhl, Thomas and Eitzinger, Jan and Hager, Georg and Wellein, Gerhard},
  date = {2017-09},
  eprint = {1708.01476},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {781--784},
  doi = {10.1109/CLUSTER.2017.115},
  url = {http://arxiv.org/abs/1708.01476},
  urldate = {2022-12-31},
  abstract = {System monitoring is an established tool to measure the utilization and health of HPC systems. Usually system monitoring infrastructures make no connection to job information and do not utilize hardware performance monitoring (HPM) data. To increase the efficient use of HPC systems automatic and continuous performance monitoring of jobs is an essential component. It can help to identify pathological cases, provides instant performance feedback to the users, offers initial data to judge on the optimization potential of applications and helps to build a statistical foundation about application specific system usage. The LIKWID monitoring stack is a modular framework build on top of the LIKWID tools library. It aims on enabling job specific performance monitoring using HPM data, system metrics and application-level data for small to medium sized commodity clusters. Moreover, it is designed to integrate in existing monitoring infrastructures to speed up the change from pure system monitoring to job-aware monitoring.},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/QQ5SF2WT/rohl2017.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/SM3BVSIH/Röhl и др. - 2017 - LIKWID Monitoring Stack A flexible framework enab.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/CDUN8A86/1708.html}
}

@incollection{rohlValidationHardwareEvents2016,
  title = {Validation of Hardware Events for Successful Performance Pattern Identification in {{High Performance Computing}}},
  author = {Röhl, Thomas and Eitzinger, Jan and Hager, Georg and Wellein, Gerhard},
  date = {2016},
  eprint = {1710.04094},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {17--28},
  doi = {10.1007/978-3-319-39589-0_2},
  url = {http://arxiv.org/abs/1710.04094},
  urldate = {2022-12-31},
  abstract = {Hardware performance monitoring (HPM) is a crucial ingredient of performance analysis tools. While there are interfaces like LIKWID, PAPI or the kernel interface perf\textbackslash\_event which provide HPM access with some additional features, many higher level tools combine event counts with results retrieved from other sources like function call traces to derive (semi-)automatic performance advice. However, although HPM is available for x86 systems since the early 90s, only a small subset of the HPM features is used in practice. Performance patterns provide a more comprehensive approach, enabling the identification of various performance-limiting effects. Patterns address issues like bandwidth saturation, load imbalance, non-local data access in ccNUMA systems, or false sharing of cache lines. This work defines HPM event sets that are best suited to identify a selection of performance patterns on the Intel Haswell processor. We validate the chosen event sets for accuracy in order to arrive at a reliable pattern detection mechanism and point out shortcomings that cannot be easily circumvented due to bugs or limitations in the hardware.},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Performance},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/AX9B43XQ/Röhl и др. - 2016 - Validation of hardware events for successful perfo.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/QPK72AC5/1710.html}
}

@article{rootOptimizingVectorInstruction,
  title = {Optimizing {{Vector Instruction Selection}} for {{Digital Signal Processing}}},
  author = {Root, Alexander James},
  abstract = {Digital signal processing applications benefit from fast implementations of vectorized inner kernels. Existing compilers rely on brittle pattern-matching or search-based methods with poor scalability for vector instruction selection – techniques which are limited by a reliance on the syntax of the input code. These techniques struggle to utilize the efficient fused instructions that exist on modern hardware.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/4R4CSTTN/Root - Optimizing Vector Instruction Selection for Digita.pdf}
}

@article{saavedraMeasuringCacheTLB1995,
  title = {Measuring Cache and {{TLB}} Performance and Their Effect on Benchmark Runtimes},
  author = {Saavedra, R.H. and Smith, A.J.},
  year = {Oct./1995},
  journaltitle = {IEEE Transactions on Computers},
  shortjournal = {IEEE Trans. Comput.},
  volume = {44},
  number = {10},
  pages = {1223--1235},
  issn = {00189340},
  doi = {10.1109/12.467697},
  url = {http://ieeexplore.ieee.org/document/467697/},
  urldate = {2022-12-18},
  abstract = {In previous research, we have developed and presented a model for measuring machines and analyzing programs, and for accurately predicting the running time of any analyzed program on any measured machine. That work is extended here by: (a) developing a high level program to measure the design and performance of the cache and TLB for any machine; (b) using those measurements, along with published miss ratio data, to improve the accuracy of our run time predictions; (c) using our analysis tools and measurements to study and compare the design of several machines, with particular reference to their cache and TLB performance. As part of this work, we describe the design and performance of the cache and TLB for ten machines. The work presented in this paper extends a powerful technique for the evaluation and analysis of both computer systems and their workloads; this methodology is valuable both to computer users and computer system designers.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/9XB3LWZK/Saavedra и Smith - 1995 - Measuring cache and TLB performance and their effe.pdf}
}

@article{shamparovOptimizaciiRaspolozheniyaDannyh2021,
  title = {Оптимизации Расположения Данных Для Компилятора {{LCC}} Для Архитектуры {{Эльбрус}}},
  author = {Шампаров, Виктор Евгеньевич and Нейман-Заде, Мурад Искендер-оглы},
  date = {2021-07-30},
  journaltitle = {Труды Института системного программирования РАН},
  volume = {33},
  number = {3},
  pages = {51--60},
  issn = {2220-6426},
  url = {https://ispranproceedings.elpub.ru/jour/article/view/1402},
  urldate = {2023-03-01},
  abstract = {Научный рецензируемый журнал, список ВАК},
  issue = {3},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/PTV4WK8P/Шампаров и Нейман-Заде - 2021 - Оптимизации расположения данных для компилятора LC.pdf}
}

@online{shrivastavaSLAPSplitLatency2021,
  title = {{{SLAP}}: {{A Split Latency Adaptive VLIW}} Pipeline Architecture Which Enables on-the-Fly Variable {{SIMD}} Vector-Length},
  shorttitle = {{{SLAP}}},
  author = {Shrivastava, Ashish and Gatherer, Alan and Sun, Tong and Wokhlu, Sushma and Chandra, Alex},
  date = {2021-02-25},
  eprint = {2102.13301},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.13301},
  url = {http://arxiv.org/abs/2102.13301},
  urldate = {2023-03-02},
  abstract = {Over the last decade the relative latency of access to shared memory by multicore increased as wire resistance dominated latency and low wire density layout pushed multiport memories farther away from their ports. Various techniques were deployed to improve average memory access latencies, such as speculative pre-fetching and branch-prediction, often leading to high variance in execution time which is unacceptable in real time systems. Smart DMAs can be used to directly copy data into a layer1 SRAM, but with overhead. The VLIW architecture, the de facto signal processing engine, suffers badly from a breakdown in lockstep execution of scalar and vector instructions. We describe the Split Latency Adaptive Pipeline (SLAP) VLIW architecture, a cache performance improvement technology that requires zero change to object code, while removing smart DMAs and their overhead. SLAP builds on the Decoupled Access and Execute concept by 1) breaking lockstep execution of functional units, 2) enabling variable vector length for variable data level parallelism, and 3) adding a novel triangular load mechanism. We discuss the SLAP architecture and demonstrate the performance benefits on real traces from a wireless baseband system (where even the most compute intensive functions suffer from an Amdahls law limitation due to a mixture of scalar and vector processing).},
  pubstate = {preprint},
  keywords = {Computer Science - Hardware Architecture},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ZH9LGVXX/Shrivastava и др. - 2021 - SLAP A Split Latency Adaptive VLIW pipeline archi.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/PBQ2T4CN/2102.html}
}

@article{skvorcovRazrabotkaKompilyatoraDlya2021,
  title = {Разработка компилятора для стековой процессорной архитектуры TF16 на основе LLVM},
  author = {Скворцов, Леонид Владленович and Баев, Роман Вячеславович and Долгорукова, Ксения Юрьевна and Шарыгин, Евгений Юрьевич},
  date = {2021-12-19},
  journaltitle = {Труды Института системного программирования РАН},
  volume = {33},
  number = {5},
  pages = {137--154},
  issn = {2220-6426},
  url = {https://ispranproceedings.elpub.ru/jour/article/view/1457},
  urldate = {2023-03-01},
  abstract = {Научный рецензируемый журнал, список ВАК},
  issue = {5},
  langid = {russian},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/4B9EXB8L/Скворцов и др. - 2021 - Разработка компилятора для стековой процессорной а.pdf}
}

@inproceedings{sokulskiSAPIVeSimpleAVX2022,
  title = {{{SAPIVe}}: {{Simple AVX}} to {{PIM Vectorizer}}},
  shorttitle = {{{SAPIVe}}},
  booktitle = {2022 {{XII Brazilian Symposium}} on {{Computing Systems Engineering}} ({{SBESC}})},
  author = {Sokulski, Rodrigo M. and Santos, Paulo C. and family=Santos, given=Sairo R., prefix=dos, useprefix=true and Alves, Marco A. Z.},
  date = {2022-11},
  pages = {1--8},
  issn = {2324-7894},
  doi = {10.1109/SBESC56799.2022.9964539},
  abstract = {Larger vector extensions are one of the commonly used techniques to meet the growing demands from computational systems. These extensions, capable of operating over multiple data elements with a single instruction, exert a lot of pressure on the memory hierarchy, increasing the impact of growing problems such as Memory-Wall and von Neumann bottleneck. An alternative to work around these problems would be adding processing elements close to the memory, known as Processing-In-Memory (PIM). As with processor vector extensions, the most efficient PIM techniques use in-memory vector processing units. There are several ways to convert a code into in-memory vector processing, such as binary hardware translation, which may not depend on programmers or adapted software and can be carried out transparently to its users. However, in the context of in-memory processing, this conversion technique presents some challenges related to the PIM instructions format and the structure of the loops present in each application. Thus, this article proposes and evaluates Simple AVX to PIM Vectorizer (SAPIVe), a hardware binary translation mechanism from processor vector instructions into in-memory vector instructions, which, in addition to processing more data, also performs loads, operations, and stores at once. Our results show that our mechanism can accelerate kernels up to 5 times with possible performance losses prevented using loop predictors.},
  eventtitle = {2022 {{XII Brazilian Symposium}} on {{Computing Systems Engineering}} ({{SBESC}})},
  keywords = {binary,Context,Degradation,hardware,Hardware,In-memory computing,Manuals,processing-in-memory,Software,Systems engineering and theory,translator,vectorizer}
}

@inproceedings{sommerSPNCOpenSourceMLIRBased2022,
  title = {{{SPNC}}: {{An Open-Source MLIR-Based Compiler}} for {{Fast Sum-Product Network Inference}} on {{CPUs}} and {{GPUs}}},
  shorttitle = {{{SPNC}}},
  booktitle = {2022 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  author = {Sommer, Lukas and Axenie, Cristian and Koch, Andreas},
  date = {2022-04-02},
  pages = {1--11},
  publisher = {{IEEE}},
  location = {{Seoul, Korea, Republic of}},
  doi = {10.1109/CGO53902.2022.9741277},
  url = {https://ieeexplore.ieee.org/document/9741277/},
  urldate = {2022-12-17},
  abstract = {Sum-Product Networks (SPNs) are an alternative to the widely used Neural Networks (NNs) for machine learning. SPNs can not only reason about (un)certainty by qualifying their output with a probability, they also allow fast (tractable) inference by having run-times that are just linear w.r.t. the network size. We present SPNC, the first tool flow for generating fast native code for SPN inference on both CPUs and GPUs, including the use of vectorized/SIMD execution. To this end, we add two SPN-specific dialects to the MLIR framework and discuss their lowering towards the execution targets.},
  eventtitle = {2022 {{IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}} ({{CGO}})},
  isbn = {978-1-66540-584-3},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/Z83GI9XE/Sommer и др. - 2022 - SPNC An Open-Source MLIR-Based Compiler for Fast .pdf}
}

@inproceedings{stripfCompilerBackEndReconfigurable2012,
  title = {A {{Compiler Back-End}} for {{Reconfigurable}}, {{Mixed-ISA Processors}} with {{Clustered Register Files}}},
  booktitle = {2012 {{IEEE}} 26th {{International Parallel}} and {{Distributed Processing Symposium Workshops}} \& {{PhD Forum}}},
  author = {Stripf, Timo and Koenig, Ralf and Rieder, Patrick and Becker, Juergen},
  date = {2012-05},
  pages = {462--469},
  doi = {10.1109/IPDPSW.2012.60},
  abstract = {Reconfigurable tile-based architectures can dynamically interconnect several tiles in order to establish processor instances with varying resource, performance, and energy characteristics at run time. These flexible processor instances offer a new degree of freedom for adapting to changing applications' requirements while optimizing resource and energy consumption. Our solution for dynamic interconnection of tiles requires a flexible Run-Time Scalable Issue-Width (RSIW) Instruction Set Architecture (ISA) that changes dependent on the configuration. In order to enable high-level programmability of our architecture in C/C++ a novel compiler back-end is needed. In this paper we address this necessity by presenting a novel LLVM compiler back-end targeting the reconfigurable RSIW ISA and supporting mixed-ISA software development. RSIW is comparable to clustered-VLIW ISAs since it expresses parallel operations within the ISA and explicitly uses clustered register files. Therefore, we extended our architecture description language based RISC LLVM back-end by representations of parallel operations as well as compilation passes for clustering and scheduling of parallel operations as well as mixed-ISA code generation. Based on the novel back-end we compare the performance characteristics of several applications compiled for and simulated on different configurations. Additionally, we demonstrate resource-aware reconfiguration by a mixed-ISA application scenario.},
  eventtitle = {2012 {{IEEE}} 26th {{International Parallel}} and {{Distributed Processing Symposium Workshops}} \& {{PhD Forum}}},
  keywords = {clustered VLIW,Clustering algorithms,Computer architecture,dynamic reconfigurable architecture,Hazards,low level virtual machine,Mixed-ISA Compiler,Reduced instruction set computing,Registers,Resource management},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/PTN7U9ZQ/stripf2012.pdf.pdf}
}

@article{SVENeonCoding2020,
  title = {{{SVE}} and {{Neon}} Coding Compared},
  date = {2020},
  number = {01},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/3RH7PU4X/2020 - SVE and Neon coding compared.pdf}
}

@article{SVEProgrammingExamples,
  title = {{{SVE Programming Examples}}},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/GCC78UEW/SVE Programming Examples.pdf}
}

@book{szczesnyPerformanceAnalysisLTE2009,
  title = {Performance {{Analysis}} of {{LTE Protocol Processing}} on an {{ARM}} Based {{Mobile Platform}}},
  author = {Szczesny, David and Showk, Anas and Hessel, Sebastian and Bilgic, Attila and Hildebrand, Uwe and Frascolla, Valerio},
  date = {2009-11-07},
  pages = {063},
  doi = {10.1109/SOCC.2009.5335678},
  abstract = {In this paper we present detailed profiling results and identify the time critical algorithms of the Long Term Evolution (LTE) layer 2 (L2) protocol processing on an ARM based mobile hardware platform. Furthermore, we investigate the applicability of a single ARM processor combined with a traditional hardware acceleration concept for the significantly increased computational demands in LTE and future mobile devices. A virtual prototyping approach is adopted in order to simulate a state-of-the-art mobile phone platform which is based on an ARM1176 core. Moreover a physical layer and base station emulator is implemented that allows for protocol investigations on transport block level at different transmission conditions. By simulating LTE data rates of 100 Mbit/s and beyond, we measure the execution times in a protocol stack model which is compliant to 3GPP Rel.8 specifications and comprises the most processing intensive downlink (DL) part of the LTE L2 data plane. We show that the computing power of a single embedded processor at reasonable clock frequencies is not enough to cope with the L2 requirements of next generation mobile devices. Thereby, Robust Header Compression (ROHC) processing is identified as the major time critical software algorithm, demanding half of the entire L2 DL execution time. Finally, we illustrate that a conventional hardware acceleration approach for the encryption algorithms fails to offer the performance required by LTE and future mobile phones.},
  pagetotal = {056}
}

@inproceedings{talaashrafiAutomaticOpenMPAwareUtilization2022,
  title = {Towards {{Automatic OpenMP-Aware Utilization}} of~{{Fast GPU Memory}}},
  booktitle = {{{OpenMP}} in a {{Modern World}}: {{From Multi-device Support}} to {{Meta Programming}}},
  author = {Talaashrafi, Delaram and Maza, Marc Moreno and Doerfert, Johannes},
  editor = {Klemm, Michael and family=Supinski, given=Bronis R., prefix=de, useprefix=true and Klinkenberg, Jannis and Neth, Brandon},
  date = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {67--80},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-15922-0_5},
  abstract = {OpenMP has supported target offloading since version 4.0, and LLVM/Clang supports its compilation and optimization. There have been several optimizing transformations in LLVM aiming to improve the performance of the offloaded region, especially for targeting GPUs. Although using the memory efficiently is essential for high performance on a GPU, there has not been much work done to automatically optimize memory transactions inside the target region at compile time.},
  isbn = {978-3-031-15922-0},
  langid = {english},
  keywords = {compiler optimization,GPU,LLVM/Clang,OpenMP,shared memory,target offloading}
}

@inproceedings{talatiProdigyImprovingMemory2021,
  title = {Prodigy: {{Improving}} the {{Memory Latency}} of {{Data-Indirect Irregular Workloads Using Hardware-Software Co-Design}}},
  shorttitle = {Prodigy},
  booktitle = {2021 {{IEEE International Symposium}} on {{High-Performance Computer Architecture}} ({{HPCA}})},
  author = {Talati, Nishil and May, Kyle and Behroozi, Armand and Yang, Yichen and Kaszyk, Kuba and Vasiladiotis, Christos and Verma, Tarunesh and Li, Lu and Nguyen, Brandon and Sun, Jiawen and Morton, John Magnus and Ahmadi, Agreen and Austin, Todd and O'Boyle, Michael F. P. and Mahlke, Scott and Mudge, Trevor and Dreslinski, Ronald},
  date = {2021-04-22},
  pages = {654--667},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/HPCA51647.2021.00061},
  url = {https://www.research.ed.ac.uk/en/publications/prodigy-improving-the-memory-latency-of-data-indirect-irregular-w},
  urldate = {2022-12-17},
  eventtitle = {The 27th {{IEEE International Symposium}} on {{High-Performance Computer Architecture}}},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/5TH6BGUQ/Talati и др. - 2021 - Prodigy Improving the Memory Latency of Data-Indi.pdf}
}

@online{tarsaImprovingBranchPrediction2019,
  title = {Improving {{Branch Prediction By Modeling Global History}} with {{Convolutional Neural Networks}}},
  author = {Tarsa, Stephen J. and Lin, Chit-Kwan and Keskin, Gokce and Chinya, Gautham and Wang, Hong},
  date = {2019-06-20},
  eprint = {1906.09889},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1906.09889},
  url = {http://arxiv.org/abs/1906.09889},
  urldate = {2023-03-02},
  abstract = {CPU branch prediction has hit a wall--existing techniques achieve near-perfect accuracy on 99\% of static branches, and yet the mispredictions that remain hide major performance gains. In a companion report, we show that a primary source of mispredictions is a handful of systematically hard-to-predict branches (H2Ps), e.g. just 10 static instructions per SimPoint phase in SPECint 2017. The lost opportunity posed by these mispredictions is significant to the CPU: 14.0\% in instructions-per-cycle (IPC) on Intel SkyLake and 37.4\% IPC when the pipeline is scaled four-fold, on par with gains from process technology. However, up to 80\% of this upside is unreachable by the best known branch predictors, even when afforded exponentially more resources. New approaches are needed, and machine learning (ML) provides a palette of powerful predictors. A growing body of work has shown that ML models are deployable within the microarchitecture to optimize hardware at runtime, and are one way to customize CPUs post-silicon by training to customer applications. We develop this scenario for branch prediction using convolutional neural networks (CNNs) to boost accuracy for H2Ps. Step-by-step, we (1) map CNNs to the global history data used by existing branch predictors; (2) show how CNNs improve H2P prediction in SPEC 2017; (3) adapt 2-bit CNN inference to the constraints of current branch prediction units; and (4) establish that CNN helper predictors are reusable across application executions on different inputs, enabling us to amortize offline training and deploy ML pattern matching to improve IPC.},
  pubstate = {preprint},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing},Computer Science - Machine Learning},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/CMS8U6RR/Tarsa и др. - 2019 - Improving Branch Prediction By Modeling Global His.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/BS8NGLAB/1906.html}
}

@article{tewaryCompilerassistedEnergyReduction2022,
  title = {Compiler-Assisted Energy Reduction of Java Real-Time Programs},
  author = {Tewary, Manish and Salcic, Zoran and Biglari-Abhari, Morteza and Malik, Avinash},
  date = {2022-03},
  journaltitle = {Microprocessors and Microsystems},
  shortjournal = {Microprocessors and Microsystems},
  volume = {89},
  pages = {104436},
  issn = {01419331},
  doi = {10.1016/j.micpro.2022.104436},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0141933122000126},
  urldate = {2022-12-18},
  langid = {english}
}

@online{thavappiragasamPortabilityGPUacceleratedMolecular2022,
  title = {Portability for {{GPU-accelerated}} Molecular Docking Applications for Cloud and {{HPC}}: Can Portable Compiler Directives Provide Performance across All Platforms?},
  shorttitle = {Portability for {{GPU-accelerated}} Molecular Docking Applications for Cloud and {{HPC}}},
  author = {Thavappiragasam, Mathialakan and Elwasif, Wael and Sedova, Ada},
  date = {2022-03-03},
  eprint = {2203.02096},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2203.02096},
  urldate = {2022-12-17},
  abstract = {High-throughput structure-based screening of drug-like molecules has become a common tool in biomedical research. Recently, acceleration with graphics processing units (GPUs) has provided a large performance boost for molecular docking programs. Both cloud and high-performance computing (HPC) resources have been used for large screens with molecular docking programs; while NVIDIA GPUs have dominated cloud and HPC resources, new vendors such as AMD and Intel are now entering the field, creating the problem of software portability across different GPUs. Ideally, software productivity could be maximized with portable programming models that are able to maintain high performance across architectures. While in many cases compiler directives have been used as an easy way to offload parallel regions of a CPU-based program to a GPU accelerator, they may also be an attractive programming model for providing portability across different GPU vendors, in which case the porting process may proceed in the reverse direction: from low-level, architecture-specific code to higher-level directive-based abstractions. MiniMDock is a new mini-application (miniapp) designed to capture the essential computational kernels found in molecular docking calculations, such as are used in pharmaceutical drug discovery efforts, in order to test different solutions for porting across GPU architectures. Here we extend MiniMDock to GPU offloading with OpenMP directives, and compare to performance of kernels using CUDA, and HIP on both NVIDIA and AMD GPUs, as well as across different compilers, exploring performance bottlenecks. We document this reverse-porting process, from highly optimized device code to a higher-level version using directives, compare code structure, and describe barriers that were overcome in this effort.},
  pubstate = {preprint},
  keywords = {68M20,C.4,{Computer Science - Distributed, Parallel, and Cluster Computing},Quantitative Biology - Biomolecules},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/745JDLEK/Thavappiragasam и др. - 2022 - Portability for GPU-accelerated molecular docking .pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/ZWBM2P3A/2203.html}
}

@inproceedings{theodoridisFindingMissedOptimizations2022,
  title = {Finding Missed Optimizations through the Lens of Dead Code Elimination},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {Theodoridis, Theodoros and Rigger, Manuel and Su, Zhendong},
  date = {2022-02-28},
  pages = {697--709},
  publisher = {{ACM}},
  location = {{Lausanne Switzerland}},
  doi = {10.1145/3503222.3507764},
  url = {https://dl.acm.org/doi/10.1145/3503222.3507764},
  urldate = {2022-12-17},
  abstract = {Compilers are foundational software development tools and incorporate increasingly sophisticated optimizations. Due to their complexity, it is difficult to systematically identify opportunities for improving them. Indeed, the automatic discovery of missed optimizations has been an important and significant challenge. The few existing approaches either cannot accurately pinpoint missed optimizations or target only specific analyses. This paper tackles this challenge by introducing a novel, effective approach that — in a simple and general manner — automatically identifies a wide range of missed optimizations. Our core insight is to leverage dead code elimination (DCE) to both analyze how well compilers optimize code and identify missed optimizations: (1) insert “optimization markers” in the basic blocks of a given program, (2) compute the program’s live/dead basic blocks using the “optimization markers”, and (3) identify missed optimizations from how well compilers eliminate dead blocks. We essentially exploit that, since DCE heavily depends on the rest of the optimization pipeline, through the lens of DCE, one can systematically quantify how well compilers optimize code. We conduct an extensive analysis of GCC and LLVM using our approach, which (1) provides quantitative and qualitative insights regarding their optimization capabilities, and (2) uncovers a diverse set of missed optimizations. Our results also lead to 84 bug reports for GCC and LLVM, of which 62 have already been confirmed or fixed, demonstrating our work’s strong practical utility. We expect that the simplicity and generality of our approach will make it widely applicable for understanding compiler performance and finding missed optimizations. This work opens and initiates this promising direction.},
  eventtitle = {{{ASPLOS}} '22: 27th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  isbn = {978-1-4503-9205-1},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/INI3PYKU/Theodoridis и др. - 2022 - Finding missed optimizations through the lens of d.pdf}
}

@inproceedings{tilletTritonIntermediateLanguage2019,
  title = {Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations},
  shorttitle = {Triton},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN International Workshop}} on {{Machine Learning}} and {{Programming Languages}}  - {{MAPL}} 2019},
  author = {Tillet, Philippe and Kung, H. T. and Cox, David},
  date = {2019},
  pages = {10--19},
  publisher = {{ACM Press}},
  location = {{Phoenix, AZ, USA}},
  doi = {10.1145/3315508.3329973},
  url = {http://dl.acm.org/citation.cfm?doid=3315508.3329973},
  urldate = {2022-12-17},
  abstract = {The validation and deployment of novel research ideas in the field of Deep Learning is often limited by the availability of efficient compute kernels for certain basic primitives. In particular, operations that cannot leverage existing vendor libraries (e.g., cuBLAS, cuDNN) are at risk of facing poor device utilization unless custom implementations are written by experts – usually at the expense of portability. For this reason, the development of new programming abstractions for specifying custom Deep Learning workloads at a minimal performance cost has become crucial.},
  eventtitle = {The 3rd {{ACM SIGPLAN International Workshop}}},
  isbn = {978-1-4503-6719-6},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/S8TNKB72/Tillet и др. - 2019 - Triton an intermediate language and compiler for .pdf}
}

@inproceedings{trifunovicPolyhedralModelGuidedLoopNest2009,
  title = {Polyhedral-{{Model Guided Loop-Nest Auto-Vectorization}}},
  author = {Trifunovic, Konrad and Nuzman, Dorit and Cohen, Albert and Zaks, Ayal and Rosen, Ira},
  date = {2009-09-01},
  pages = {327--337},
  doi = {10.1109/PACT.2009.18},
  abstract = {Optimizing compilers apply numerous interdependent optimizations, leading to the notoriously difficult phase-ordering problem - that of deciding which transformations to apply and in which order. Fortunately, new infrastructures such as the polyhedral compilation framework host a variety of transformations, facilitating the efficient exploration and configuration of multiple transformation sequences. Many powerful optimizations, however, remain external to the polyhedral framework, including vectorization. The low-level, target-specific aspects of vectorization for fine-grain SIMD has so far excluded it from being part of the polyhedral framework. In this paper we examine the interactions between loop transformations of the polyhedral framework and subsequent vectorization. We model the performance impact of the different loop transformations and vectorization strategies, and then show how this cost model can be integrated seamlessly into the polyhedral representation. This predictive modelling facilitates efficient exploration and educated decision making to best apply various polyhedral loop transformations while considering the subsequent effects of different vectorization schemes. Our work demonstrates the feasibility and benefit of tuning the polyhedral model in the context of vectorization. Experimental results confirm that our model has accurate predictions, providing speedups of over 2.0times on average over traditional innermost-loop vectorization on PowerPC970 and Cell-SPU SIMD platforms.},
  eventtitle = {Parallel {{Architectures}} and {{Compilation Techniques}} - {{Conference Proceedings}}, {{PACT}}},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/6QH5C42V/Trifunovic и др. - 2009 - Polyhedral-Model Guided Loop-Nest Auto-Vectorizati.pdf}
}

@article{universityofportharcourtNewTrendsCompiler2017,
  title = {The {{New Trends}} in {{Compiler Analysis}} and {{Optimizations}}},
  author = {{University of Port Harcourt} and P, Enyindah and Uko, Okon E},
  date = {2017-04-25},
  journaltitle = {International Journal of Computer Trends and Technology},
  shortjournal = {IJCTT},
  volume = {46},
  number = {2},
  pages = {95--99},
  issn = {22312803},
  doi = {10.14445/22312803/IJCTT-V46P119},
  url = {http://www.ijcttjournal.org/archives/ijctt-v46p119},
  urldate = {2022-12-18},
  abstract = {Compiler construction primarily comprises of some standard phases such as lexical analysis, syntax analysis, semantic analysis, intermediate code generation, code optimization and target code generation but due to the improvement in computer architectural designs, there is a need to improve on the code size, instruction execution speed, etc.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/ZBTNA79V/University of Port Harcourt и др. - 2017 - The New Trends in Compiler Analysis and Optimizati.pdf}
}

@inproceedings{vanhattumVectorizationDigitalSignal2021,
  title = {Vectorization for Digital Signal Processors via Equality Saturation},
  booktitle = {Proceedings of the 26th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author = {VanHattum, Alexa and Nigam, Rachit and Lee, Vincent T. and Bornholt, James and Sampson, Adrian},
  date = {2021-04-19},
  pages = {874--886},
  publisher = {{ACM}},
  location = {{Virtual USA}},
  doi = {10.1145/3445814.3446707},
  url = {https://dl.acm.org/doi/10.1145/3445814.3446707},
  urldate = {2022-12-17},
  abstract = {Applications targeting digital signal processors (DSPs) benefit from fast implementations of small linear algebra kernels. While existing auto-vectorizing compilers are effective at extracting performance from large kernels, they struggle to invent the complex data movements necessary to optimize small kernels. To get the best performance, DSP engineers must hand-write and tune specialized small kernels for a wide spectrum of applications and architectures. We present Diospyros, a search-based compiler that automatically finds efficient vectorizations and data layouts for small linear algebra kernels. Diospyros combines symbolic evaluation and equality saturation to vectorize computations with irregular structure. We show that a collection of Diospyros-compiled kernels outperform implementations from existing DSP libraries by 3.1× on average, that Diospyros can generate kernels that are competitive with expert-tuned code, and that optimizing these small kernels offers end-to-end speedup for a DSP application.},
  eventtitle = {{{ASPLOS}} '21: 26th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  isbn = {978-1-4503-8317-2},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/WTC5UMSZ/VanHattum и др. - 2021 - Vectorization for digital signal processors via eq.pdf}
}

@article{venkatakeerthyIR2VECLLVM2020,
  title = {{{IR2V}} {{{\textsc{EC}}}}: {{LLVM IR Based Scalable Program Embeddings}}},
  shorttitle = {{{IR2V}}},
  author = {VenkataKeerthy, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
  date = {2020-12-31},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  volume = {17},
  number = {4},
  pages = {1--27},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/3418463},
  url = {https://dl.acm.org/doi/10.1145/3418463},
  urldate = {2022-12-18},
  abstract = {We propose IR2V               EC               , a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a               seed embedding vocabulary               . Using this infrastructure, we propose two incremental encodings:               Symbolic               and               Flow-Aware               .               Symbolic               encodings are obtained from the               seed embedding vocabulary               , and               Flow-Aware               encodings are obtained by augmenting the               Symbolic               encodings with the flow information.                                         We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2V               EC               outperform the existing methods in both the tasks, even while using               simple               machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are               more scalable               ,               is non-data-hungry               , and               has better Out-Of-Vocabulary (OOV) characteristics               .},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/WSZ9EFSJ/VenkataKeerthy и др. - 2020 - IR2V ECsp.pdf}
}

@article{venkatakeerthyIR2VECLLVM2020a,
  title = {{{IR2V}} {{{\textsc{EC}}}}: {{LLVM IR Based Scalable Program Embeddings}}},
  shorttitle = {{{IR2V}}},
  author = {VenkataKeerthy, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
  date = {2020-12-31},
  journaltitle = {ACM Transactions on Architecture and Code Optimization},
  shortjournal = {ACM Trans. Archit. Code Optim.},
  volume = {17},
  number = {4},
  pages = {1--27},
  issn = {1544-3566, 1544-3973},
  doi = {10.1145/3418463},
  url = {https://dl.acm.org/doi/10.1145/3418463},
  urldate = {2023-05-09},
  abstract = {We propose IR2V               EC               , a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a               seed embedding vocabulary               . Using this infrastructure, we propose two incremental encodings:               Symbolic               and               Flow-Aware               .               Symbolic               encodings are obtained from the               seed embedding vocabulary               , and               Flow-Aware               encodings are obtained by augmenting the               Symbolic               encodings with the flow information.                                         We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2V               EC               outperform the existing methods in both the tasks, even while using               simple               machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are               more scalable               ,               is non-data-hungry               , and               has better Out-Of-Vocabulary (OOV) characteristics               .},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/232576WH/VenkataKeerthy и др. - 2020 - IR2V ECsp.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/WNWLQYTX/venkatakeerthy2020.pdf.pdf}
}

@inproceedings{vermaNeuralArchitectureawareExploration2022,
  title = {Towards Neural Architecture-Aware Exploration of Compiler Optimizations in a Deep Learning \{graph\} Compiler},
  booktitle = {Proceedings of the 19th {{ACM International Conference}} on {{Computing Frontiers}}},
  author = {Verma, Gaurav and Finviya, Swetang and Malik, Abid M. and Emani, Murali and Chapman, Barbara},
  date = {2022-05-17},
  pages = {244--250},
  publisher = {{ACM}},
  location = {{Turin Italy}},
  doi = {10.1145/3528416.3530251},
  url = {https://dl.acm.org/doi/10.1145/3528416.3530251},
  urldate = {2022-12-18},
  abstract = {Deep Neural Networks (DNN) form the basis for many existing and emerging applications. Many DL compilers analyze the computation graphs and apply various optimizations at different stages. These high-level optimizations are applied using compiler passes before feeding the resultant computation graph for low-level and hardware-specific optimizations. With advancements in DNN architectures and backend hardware, the search space of compiler optimizations has grown manifolds. Also, the inclusion of passes without the knowledge of the computation graph leads to increased execution time with a slight influence on the intermediate representation. This paper presents preliminary results 1) summarizing the relevance of pass selection and ordering in a DL compiler, 2) neural architecture-aware selection of optimization passes, and 3) pruning search space for the phase selection problem in a DL compiler. We use TVM as a compiler to demonstrate the experimental results on Nvidia A100 and GeForce RTX 2080 GPUs, establishing the relevance of neural architecture-aware selection of optimization passes for DNNs DL compilers. Experimental evaluation with seven models categorized into four architecturally different classes demonstrated performance gains for most neural networks. For ResNets, the average throughput increased by 24\% and 32\% for TensorFlow and PyTorch frameworks, respectively. Additionally, we observed an average 15\% decrease in the compilation time for ResNets, 45\% for MobileNet, and 54\% for SSD-based models without impacting the throughput. BERT models showed a dramatic improvement with a 92\% reduction in the compile time.},
  eventtitle = {{{CF}} '22: 19th {{ACM International Conference}} on {{Computing Frontiers}}},
  isbn = {978-1-4503-9338-6},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/USABFV9D/Verma и др. - 2022 - Towards neural architecture-aware exploration of c.pdf}
}

@article{volkovUnderstandingLatencyHiding,
  title = {Understanding {{Latency Hiding}} on {{GPUs}}},
  author = {Volkov, Vasily},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/CG7UJ56N/Volkov - Understanding Latency Hiding on GPUs.pdf}
}

@inproceedings{wanAcceleratingLevelBLAS2021,
  title = {Accelerating {{Level}} 2 {{BLAS Based}} on {{ARM SVE}}},
  booktitle = {2021 4th {{International Conference}} on {{Advanced Electronic Materials}}, {{Computers}} and {{Software Engineering}} ({{AEMCSE}})},
  author = {Wan, Xiuwen and Gu, Naijie and Su, Junjie},
  date = {2021-03},
  pages = {1018--1022},
  doi = {10.1109/AEMCSE51986.2021.00208},
  abstract = {Scalable Vector Extension (SVE) is a special vector instruction set architecture recently released by ARM, which is a vector extension for A64 instruction set of ARMv8-A architecture. SVE introduces many significant features that not only take full advantage of the use of wide vector, but also enable efficient vectorization functions which can realize high performance. This paper described how to apply the main features of SVE to accelerate the Level 2 BLAS routines. We carried out the simulation on ARM Instruction Emulator (ARMIE) and Gem5 simulator. The results demonstrated that our SVE implementation reduced the number of executed instructions by 90\% and achieved over 17x speedup compared with the NEON implementation.},
  eventtitle = {2021 4th {{International Conference}} on {{Advanced Electronic Materials}}, {{Computers}} and {{Software Engineering}} ({{AEMCSE}})},
  keywords = {BLAS,Computational modeling,Computer architecture,Computers,gemv,ger,Instruction sets,Neon,Software engineering,SVE,vectorization,VLA},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/VDXCG55H/9513183.html}
}

@inproceedings{wangAUGEMAutomaticallyGenerate2013,
  title = {{{AUGEM}}: {{Automatically}} Generate High Performance {{Dense Linear Algebra}} Kernels on X86 {{CPUs}}},
  shorttitle = {{{AUGEM}}},
  booktitle = {{{SC}} '13: {{Proceedings}} of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  author = {Wang, Qian and Zhang, Xianyi and Zhang, Yunquan and Yi, Qing},
  date = {2013-11},
  pages = {1--12},
  issn = {2167-4337},
  doi = {10.1145/2503210.2503219},
  abstract = {Basic Liner algebra subprograms (BLAS) is a fundamental library in scientific computing. In this paper, we present a template-based optimization framework, AUGEM, which can automatically generate fully optimized assembly code for several dense linear algebra (DLA) kernels, such as GEMM, GEMV, AXPY and DOT, on varying multi-core CPUs without requiring any manual interference from developers. In particular, based on domain-specific knowledge about algorithms of the DLA kernels, we use a collection of parameterized code templates to formulate a number of commonly occurring instruction sequences within the optimized low-level C code of these DLA kernels. Then, our framework uses a specialized low-level C optimizer to identify instruction sequences that match the pre-defined code templates and thereby translates them into extremely efficient SSE/AVX instructions. The DLA kernels generated by our templatebased approach surpass the implementations of Intel MKL and AMD ACML BLAS libraries, on both Intel Sandy Bridge and AMD Piledriver processors.},
  eventtitle = {{{SC}} '13: {{Proceedings}} of the {{International Conference}} on {{High Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}}},
  keywords = {Abstracts,Arrays,auto-tuning,code generation,DLA code optimization,Generators,Kernel,Registers,Resource management,Seals},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/6FK3NLZ2/wang2013.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/9XX8NMN4/6877458.html}
}

@inproceedings{wangTransformerBasedDirectHidden2021,
  title = {Transformer-{{Based Direct Hidden Markov Model}} for {{Machine Translation}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}: {{Student Research Workshop}}},
  author = {Wang, Weiyue and Yang, Zijian and Gao, Yingbo and Ney, Hermann},
  date = {2021-08},
  pages = {23--32},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-srw.3},
  url = {https://aclanthology.org/2021.acl-srw.3},
  urldate = {2023-03-06},
  abstract = {The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a model with first-order dependency, which performs similarly but is significantly slower in training and decoding.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/3A9JU27A/Wang и др. - 2021 - Transformer-Based Direct Hidden Markov Model for M.pdf}
}

@inproceedings{wangTransformerBasedDirectHidden2021a,
  title = {Transformer-{{Based Direct Hidden Markov Model}} for {{Machine Translation}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}: {{Student Research Workshop}}},
  author = {Wang, Weiyue and Yang, Zijian and Gao, Yingbo and Ney, Hermann},
  date = {2021},
  pages = {23--32},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-srw.3},
  url = {https://aclanthology.org/2021.acl-srw.3},
  urldate = {2023-03-06},
  abstract = {The neural hidden Markov model has been proposed as an alternative to attention mechanism in machine translation with recurrent neural networks. However, since the introduction of the transformer models, its performance has been surpassed. This work proposes to introduce the concept of the hidden Markov model to the transformer architecture, which outperforms the transformer baseline. Interestingly, we find that the zero-order model already provides promising performance, giving it an edge compared to a model with first-order dependency, which performs similarly but is significantly slower in training and decoding.},
  eventtitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}: {{Student Research Workshop}}},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/NNU9NMLY/Wang и др. - 2021 - Transformer-Based Direct Hidden Markov Model for M.pdf}
}

@article{williamsRooflineInsightfulVisual2009,
  title = {Roofline: An Insightful Visual Performance Model for Multicore Architectures},
  shorttitle = {Roofline},
  author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
  date = {2009-04-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {52},
  number = {4},
  pages = {65--76},
  issn = {0001-0782},
  doi = {10.1145/1498765.1498785},
  url = {https://doi.org/10.1145/1498765.1498785},
  urldate = {2023-03-06},
  abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/E8KCGQDA/williams2009.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/I7J9LPXS/Williams и др. - 2009 - Roofline an insightful visual performance model f.pdf}
}

@article{wonEvolvingStructureHidden2006,
  title = {Evolving the Structure of Hidden {{Markov}} Models},
  author = {Won, Kyoung-Jae and Prugel-Bennett, A. and Krogh, A.},
  date = {2006-02},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {10},
  number = {1},
  pages = {39--49},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2005.851271},
  abstract = {A genetic algorithm (GA) is proposed for finding the structure of hidden Markov Models (HMMs) used for biological sequence analysis. The GA is designed to preserve biologically meaningful building blocks. The search through the space of HMM structures is combined with optimization of the emission and transition probabilities using the classic Baum-Welch algorithm. The system is tested on the problem of finding the promoter and coding region of C. jejuni. The resulting HMM has a superior discrimination ability to a handcrafted model that has been published in the literature.},
  eventtitle = {{{IEEE Transactions}} on {{Evolutionary Computation}}},
  keywords = {Algorithm design and analysis,Approximation error,Bioinformatics,Biological sequence analysis,Biological system modeling,Estimation error,genetic algorithm (GA),Genetic algorithms,hidden Markov model (HMM),Hidden Markov models,hybrid algorithm,machine learning,Machine learning,Machine learning algorithms,System testing},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/NUHZQUD6/kyoung-jaewon2006.pdf.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/LR484FX9/1583626.html}
}

@inproceedings{wongDemystifyingGPUMicroarchitecture2010,
  title = {Demystifying {{GPU}} Microarchitecture through Microbenchmarking},
  booktitle = {2010 {{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} \& {{Software}} ({{ISPASS}})},
  author = {Wong, Henry and Papadopoulou, Misel-Myrto and Sadooghi-Alvandi, Maryam and Moshovos, Andreas},
  date = {2010-03},
  pages = {235--246},
  publisher = {{IEEE}},
  location = {{White Plains, NY}},
  doi = {10.1109/ISPASS.2010.5452013},
  url = {http://ieeexplore.ieee.org/document/5452013/},
  urldate = {2022-12-18},
  abstract = {Graphics processors (GPU) offer the promise of more than an order of magnitude speedup over conventional processors for certain non-graphics computations. Because the GPU is often presented as a C-like abstraction (e.g., Nvidia’s CUDA), little is known about the characteristics of the GPU’s architecture beyond what the manufacturer has documented. This work develops a microbechmark suite and measures the CUDA-visible architectural characteristics of the Nvidia GT200 (GTX280) GPU. Various undisclosed characteristics of the processing elements and the memory hierarchies are measured. This analysis exposes undocumented features that impact program performance and correctness. These measurements can be useful for improving performance optimization, analysis, and modeling on this architecture and offer additional insight on the decisions made in developing this GPU.},
  eventtitle = {Amp; {{Software}} ({{ISPASS}} 2010)},
  isbn = {978-1-4244-6023-6},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/PP9VQDNN/Wong и др. - 2010 - Demystifying GPU microarchitecture through microbe.pdf}
}

@article{yotovSearchReallyNecessary2005,
  title = {Is {{Search Really Necessary}} to {{Generate High-Performance BLAS}}?},
  author = {Yotov, K. and Li, Xiaoming and Ren, Gang and Garzaran, M.J.S. and Padua, D. and Pingali, K. and Stodghill, P.},
  date = {2005-02},
  journaltitle = {Proceedings of the IEEE},
  volume = {93},
  number = {2},
  pages = {358--386},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2004.840444},
  abstract = {A key step in program optimization is the estimation of optimal values for parameters such as tile sizes and loop unrolling factors. Traditional compilers use simple analytical models to compute these values. In contrast, library generators like ATLAS use global search over the space of parameter values by generating programs with many different combinations of parameter values, and running them on the actual hardware to determine which values give the best performance. It is widely believed that traditional model-driven optimization cannot compete with search-based empirical optimization because tractable analytical models cannot capture all the complexities of modern high-performance architectures, but few quantitative comparisons have been done to date. To make such a comparison, we replaced the global search engine in ATLAS with a model-driven optimization engine and measured the relative performance of the code produced by the two systems on a variety of architectures. Since both systems use the same code generator, any differences in the performance of the code produced by the two systems can come only from differences in optimization parameter values. Our experiments show that model-driven optimization can be surprisingly effective and can generate code with performance comparable to that of code generated by ATLAS using global search.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Analytical models,Basic Linear Algebra Subprograms (BLAS),compilers,Computer science,empirical optimization,Hardware,high-performance computing,Libraries,library generators,Linear algebra,model-driven optimization,Optimizing compilers,program optimization,Program processors,Programming profession,Search engines},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/P886EGVT/yotov2005.pdf.pdf}
}

@inproceedings{zabokrtskyHiddenMarkovTree2009,
  title = {Hidden {{Markov Tree Model}} in {{Dependency-based Machine Translation}}},
  booktitle = {Proceedings of the {{ACL-IJCNLP}} 2009 {{Conference Short Papers}}},
  author = {Žabokrtský, Zdeněk and Popel, Martin},
  date = {2009-08},
  pages = {145--148},
  publisher = {{Association for Computational Linguistics}},
  location = {{Suntec, Singapore}},
  url = {https://aclanthology.org/P09-2037},
  urldate = {2023-03-06},
  eventtitle = {{{ACL-IJCNLP}} 2009},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/AXUUX6RV/Žabokrtský и Popel - 2009 - Hidden Markov Tree Model in Dependency-based Machi.pdf}
}

@book{zhaiAcceleratingFaultTolerantBLAS2022,
  title = {Accelerating {{Fault-Tolerant BLAS}} on X86 {{CPUs}}},
  author = {Zhai, Yujia and Giem, Elisabeth and Zhao, Kai and Liu, Jinyang and Huang, Jiajun and Wong, Bryan and Shelton, Christian},
  date = {2022-07-06},
  doi = {10.13140/RG.2.2.19052.64649},
  abstract = {Basic Linear Algebra Subprograms (BLAS) serves as a foundational library for scientific computing and machine learning. In this paper, we present a brand-new BLAS implementation, FT-BLAS, that provides performance comparable to or faster than the state-of-the-art BLAS libraries, while being capable of tolerating soft errors on-the-fly. At the algorithmic level, we propose a hybrid strategy to incorporate the fault tolerant functionality. For memory-bound Level-1 and Level-2 BLAS routines, we duplicate computing instructions and re-use data at the register level to avoid memory overhead when validating the runtime correctness. Here we novelly propose to utilize mask registers on AVX512-enabled processors and SIMD registers on AVX2-enabled processors to store intermediate comparison results. For compute-bound Level-3 BLAS routines, we fuse the memory-intensive operations such as checksum encoding and verification into the GEMM assembly kernels to optimize the memory footprint. We also design cache-friendly parallel algorithms for our fault-tolerant library. Through a series of architectural-aware optimizations, we manage to maintain the fault-tolerant overhead at a negligible order ({$<$}3\%). Experimental results obtained on widely-used processors such as Intel Skylake, Intel Cascade Lake, and AMD Zen2 demonstrate that FT-BLAS offers high reliability and high performance-faster than Intel MKL, OpenBLAS, and BLIS by up to 3.50\%, 22.14\%, and 21.70\%, respectively, for both serial and parallel routines spanning all three levels of BLAS we benchmarked, even under hundreds of errors injected per minute.},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/T6LX2B5S/Zhai и др. - 2022 - Accelerating Fault-Tolerant BLAS on x86 CPUs.pdf}
}

@online{zhangSTBPUReasonablySecure2022,
  title = {{{STBPU}}: {{A Reasonably Secure Branch Prediction Unit}}},
  shorttitle = {{{STBPU}}},
  author = {Zhang, Tao and Lesch, Timothy and Koltermann, Kenneth and Evtyushkin, Dmitry},
  date = {2022-04-20},
  eprint = {2108.02156},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.02156},
  url = {http://arxiv.org/abs/2108.02156},
  urldate = {2023-03-02},
  abstract = {Modern processors have suffered a deluge of threats exploiting branch instruction collisions inside the branch prediction unit (BPU), from eavesdropping on secret-related branch operations to triggering malicious speculative executions. Protecting branch predictors tends to be challenging from both security and performance perspectives. For example, partitioning or flushing BPU can stop certain collision-based exploits but only to a limited extent. Meanwhile, such mitigations negatively affect branch prediction accuracy and further CPU performance. This paper proposes Secret Token Branch Prediction Unit (STBPU), a secure BPU design to defend against collision-based transient execution attacks and BPU side channels while incurring minimal performance overhead. STBPU resolves the challenges above by customizing data representation inside BPU for each software entity requiring isolation. In addition, to prevent an attacker from using brute force techniques to trigger malicious branch instruction collisions, STBPU actively monitors the prediction-related events and preemptively changes BPU data representation.},
  pubstate = {preprint},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Hardware Architecture},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/7GDRV8SC/Zhang и др. - 2022 - STBPU A Reasonably Secure Branch Prediction Unit.pdf;/home/alex/SynologyDrive/PhD/Zotero/storage/MJ6569NK/2108.html}
}

@inproceedings{zhangUnderstandingGPUMicroarchitecture2017,
  title = {Understanding the {{GPU Microarchitecture}} to {{Achieve Bare-Metal Performance Tuning}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  author = {Zhang, Xiuxia and Tan, Guangming and Xue, Shuangbai and Li, Jiajia and Zhou, Keren and Chen, Mingyu},
  date = {2017-01-26},
  pages = {31--43},
  publisher = {{ACM}},
  location = {{Austin Texas USA}},
  doi = {10.1145/3018743.3018755},
  url = {https://dl.acm.org/doi/10.1145/3018743.3018755},
  urldate = {2022-12-18},
  abstract = {In this paper, we present a methodology to understand GPU microarchitectural features and improve performance for compute-intensive kernels. The methodology relies on a reverse engineering approach to crack the GPU ISA encodings in order to build a GPU assembler. An assembly microbenchmark suite correlates microarchitectural features with their performance factors to uncover instruction-level and memory hierarchy preferences. We use SGEMM as a running example to show the ways to achieve bare-metal performance tuning. The performance boost is achieved by tuning FFMA throughput by activating dual-issue, eliminating register bank conflicts, adding non-FFMA instructions with little penalty, and choosing proper width of global/shared load instructions. On NVIDIA Kepler K20m, we develop a faster SGEMM with 3.1Tflop/s performance and 88\% efficiency; the performance is 15\% higher than cuBLAS7.0. Applying these optimizations to convolution, the implementation gains 39\%-62\% performance improvement compared with cuDNN4.0. The toolchain is an attempt to automatically crack different GPU ISA encodings and build an assembler adaptively for the purpose of performance enhancements to applications on GPUs.},
  eventtitle = {{{PPoPP}} '17: 22nd {{ACM SIGPLAN Symposium}} on {{Principles}} and {{Practice}} of {{Parallel Programming}}},
  isbn = {978-1-4503-4493-7},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/YGBXNPUQ/Zhang и др. - 2017 - Understanding the GPU Microarchitecture to Achieve.pdf}
}

@article{zhufengOptimizationBasedLLVM2021,
  title = {Optimization Based on {{LLVM}} Global Instruction Selection},
  author = {Zhufeng, Huang and Jiandong, Shang},
  date = {2021-04-01},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {1856},
  number = {1},
  pages = {012004},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1856/1/012004},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/1856/1/012004},
  urldate = {2022-12-18},
  abstract = {Instruction selection is a key component of code generation. High-quality instruction selection has a great impact on the size and quality of the generated code. The existing instruction selection technology is mostly limited to a single statement or a single basic block, and the global instruction selection based on LLVM degrades the entire function in the form of SSA. Global instruction selection optimization based on LLVM is implemented on Shenwei platform, including global instruction merge optimization based on cost model, register bank selection optimization and instruction locality optimization. Through the test of SPEC CPU2006, Experimental results show that the average speed-up ratio before and after the optimization of global instruction selection based on LLVM is 1.08, and the maximum speedup ratio is 1.36. In addition, when the quality of the generated code is equivalent, the global instruction selection is compared with the default instruction selection, the LLC compilation speed is increased by an average of 20\%, and the entire compilation cycle is increased by an average of 6\%-8\%.},
  langid = {english},
  file = {/home/alex/SynologyDrive/PhD/Zotero/storage/FS9EIIUZ/Zhufeng и Jiandong - 2021 - Optimization based on LLVM global instruction sele.pdf}
}
